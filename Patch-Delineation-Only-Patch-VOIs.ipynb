{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to look at the vessels in the delineated patches of the hearts\n",
    "Ruslan/Tim delineated the patch region in the hearts.\n",
    "Let's repeat what we did with the `Vessels.ipynb` notebook, but only for the delineated patch regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import seaborn\n",
    "import pandas\n",
    "import platform\n",
    "import os\n",
    "import glob\n",
    "import numpy\n",
    "from tqdm import notebook\n",
    "from itkwidgets import view  # 3d viewer\n",
    "import imageio\n",
    "import skimage\n",
    "import skimage.morphology\n",
    "import scipy.stats\n",
    "import dask\n",
    "import dask.array as da\n",
    "import dask_image.imread\n",
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "from numcodecs import Blosc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('You can see what DASK is doing at \"http://localhost:%s/status\"' % client.scheduler_info()['services']['dashboard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings in the notebook\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure defaults\n",
    "plt.rc('image', cmap='gray', interpolation='nearest')  # Display all images in b&w and with 'nearest' interpolation\n",
    "plt.rcParams['figure.figsize'] = (14, 7)  # Size up figures a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup scale bar defaults\n",
    "plt.rcParams['scalebar.location'] = 'lower right'\n",
    "plt.rcParams['scalebar.frameon'] = False\n",
    "plt.rcParams['scalebar.color'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all plots identically\n",
    "lines = 3\n",
    "# And then do something like\n",
    "# plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_git_hash():\n",
    "    \"\"\"\n",
    "    Get the current git hash from the repository.\n",
    "    Based on http://stackoverflow.com/a/949391/323100 and\n",
    "    http://stackoverflow.com/a/18283905/323100\n",
    "    \"\"\"\n",
    "    from subprocess import Popen, PIPE\n",
    "    import os\n",
    "    gitprocess = Popen(['git', '--git-dir', os.path.join(os.getcwd(), '.git'),\n",
    "                        'rev-parse', '--short', '--verify', 'HEAD'],\n",
    "                       stdout=PIPE)\n",
    "    (output, _) = gitprocess.communicate()\n",
    "    return output.strip().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform.system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are we working with?\n",
    "the_current_git_hash = get_git_hash()\n",
    "print('We are working with version %s of the analyis notebook.'\n",
    "      % the_current_git_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the output folder\n",
    "# Including the git hash, so we (potentially) have different versions of all the images we generate\n",
    "OutputDir = os.path.join('Output', the_current_git_hash)\n",
    "os.makedirs(OutputDir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different locations if running either on Linux or Windows\n",
    "if 'anaklin25' in platform.node():\n",
    "    FastSSD = True\n",
    "else:\n",
    "    FastSSD = False\n",
    "# to speed things up significantly\n",
    "if 'Linux' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "    else:\n",
    "        BasePath = os.path.join(os.sep, 'home', 'habi', '1272')\n",
    "elif 'Darwin' in platform.system():\n",
    "    BasePath = os.path.join('/Volumes/2TBSSD/')\n",
    "else:\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        if 'anaklin' in platform.node():\n",
    "            BasePath = os.path.join('S:\\\\')\n",
    "        else:\n",
    "            BasePath = os.path.join('D:\\\\Results')\n",
    "Root = os.path.join(BasePath, 'HeartsÂ Melly')\n",
    "print('We are loading all the data from %s' % Root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "print(tempfile.gettempdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Linux' in platform.system():\n",
    "    tmp = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "elif 'Darwin' in platform.system():\n",
    "    tmp = os.path.join('/Volumes/2TBSSD/')\n",
    "else:\n",
    "    tmp = os.path.join('F:\\\\')\n",
    "dask.config.set({'temporary_directory': os.path.join(tmp, 'dask_tmp')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixelsize(logfile):\n",
    "    \"\"\"Get the pixel size from the scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Image Pixel' in line and 'Scaled' not in line:\n",
    "                pixelsize = float(line.split('=')[1])\n",
    "    return(pixelsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make us a dataframe for saving all that we need\n",
    "Data = pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get *all* log files\n",
    "Data['LogFile'] = [f for f in sorted(glob.glob(os.path.join(Root, '**', '*.log'), recursive=True))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all folders\n",
    "Data['Folder'] = [os.path.dirname(f) for f in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of all non-rec logfiles\n",
    "for c, row in Data.iterrows():\n",
    "    if 'rec' not in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "# Reset dataframe to something that we would get if we only would have loaded the 'rec' files\n",
    "Data = Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all folders we don't need\n",
    "for c, row in Data.iterrows():\n",
    "    if 'Rat' not in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'Test' in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "# Reset dataframe to something that we would get if we only would have loaded the 'rec' files\n",
    "Data = Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some data from folders\n",
    "Data['Sample'] = [l[len(Root)+1:].split(os.sep)[0] for l in Data['LogFile']]\n",
    "Data['Animal'] = [int(s.replace('Rat', '').replace('b', '')) for s in Data['Sample']]\n",
    "Data['Scan'] = [l[len(Root)+1:].split(os.sep)[1] for l in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Ludovics mail\n",
    "# R60/61 : W1, VP (VEGF + PDGF = both growth factors) \n",
    "# R62/63 : W1, F    (=fibrin only = empty patch)\n",
    "# R64/65 : W1, Tachosil only (negative control)\n",
    "\n",
    "# R66/68 : W4, VP (VEGF + PDGF = both growth factors) \n",
    "# R67/69 : W4, F    (=fibrin only = empty patch)\n",
    "# R70/71 : W4, Tachosil only (negative control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in table from Ludovic with animal details (Mail from 18.2.2021)\n",
    "Animals = pandas.read_excel('Animals.xlsx',\n",
    "                            header=None,\n",
    "                            names=('Animal', 'Sex', '', 'Experiment', 'Timepoint'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pandas.merge(Data, Animals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclusion from Tims visual inspection\n",
    "# R63\n",
    "# R65\n",
    "# R66\n",
    "# R70\n",
    "#exclude = [63, 65, 66, 70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop samples which should be excluded\n",
    "Data = Data[Data[\"Scan\"] == 'cu_10um']\n",
    "Data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get voxelsize from logfiles\n",
    "Data['Voxelsize'] = [get_pixelsize(log) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect what's in the VOI folders\n",
    "for sample in Data.Sample.unique():\n",
    "    for folder in ['_patch', '_myocard']:\n",
    "        # print(os.path.join(Root, sample, 'cu_10um', '*' + folder))\n",
    "        if not len(glob.glob(os.path.join(Root, sample, 'cu_10um', '*' + folder, '*.png'))):\n",
    "            print('%s contains *no* PNG files' % glob.glob(os.path.join(Root, sample, 'cu_10um', '*' + folder))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect VOI files\n",
    "for sample in Data.Sample.unique():\n",
    "    for folder in ['_patch', '_myocard']:\n",
    "        if len(glob.glob(os.path.join(Root, sample, 'cu_10um', '*' + folder, '*.roi'))) != 1:\n",
    "            print('%s does *not* contain a .roi files' % glob.glob(os.path.join(Root, sample, 'cu_10um', '*' + folder))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of VOI files\n",
    "Data['VOIFilesPatch'] = [sorted(glob.glob(os.path.join(Root, sample, scan, 'voi_patch', '*.png')))\n",
    "                         for (sample, scan)\n",
    "                         in zip(Data['Sample'], Data['Scan'])]\n",
    "Data['VOIFilesMyocard'] = [sorted(glob.glob(os.path.join(Root, sample, scan, 'voi_myocard', '*.png')))\n",
    "                         for (sample, scan)\n",
    "                         in zip(Data['Sample'], Data['Scan'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete empties\n",
    "# https://stackoverflow.com/a/13851602/323100\n",
    "Data = Data[Data[\"VOIFilesPatch\"].map(len) > 0]\n",
    "Data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all patch VOI slices into a DASK array and save them to disk\n",
    "# Partially based on http://stackoverflow.com/a/39195332/323100\n",
    "# and on /LungMetastasis/HighResolutionScanAnalysis.ipynb\n",
    "Data['OutputNameVOIPatch'] = [os.path.join(Root, sample, scan, sample + '_voi_patch.zarr')\n",
    "                              for sample, scan\n",
    "                              in zip(Data['Sample'], Data['Scan'])]\n",
    "for c, row in notebook.tqdm(Data.iterrows(), total=len(Data)):\n",
    "    if not os.path.exists(row['OutputNameVOIPatch']):\n",
    "        if len(row['VOIFilesPatch']):\n",
    "            print('%2s/%s: Reading %s VOI slices from %s and saving to %s' % (c + 1,\n",
    "                                                                              len(Data),\n",
    "                                                                              len(row['VOIFilesPatch']),\n",
    "                                                                              os.path.join(row['Sample'],\n",
    "                                                                                           row['Scan'],\n",
    "                                                                                           'voi_patch'),\n",
    "                                                                              row['OutputNameVOIPatch'][len(Root):]))\n",
    "            Reconstructions = dask_image.imread.imread(os.path.join(os.path.commonpath(row['VOIFilesPatch']), '*.png'))\n",
    "            Reconstructions.rechunk(100).to_zarr(row['OutputNameVOIPatch'],\n",
    "                                    overwrite=True,\n",
    "                                    compressor=Blosc(cname='zstd',\n",
    "                                                    clevel=3,\n",
    "                                                    shuffle=Blosc.BITSHUFFLE))\n",
    "        else:\n",
    "            print('%2s/%s: NO VOI slices found for patch of %s' % (c + 1,\n",
    "                                                                   len(Data),\n",
    "                                                                   row['Sample']))\n",
    "            Data.at[c, 'OutputNameVOIPatch'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all myocard VOI slices into a DASK array and save them to disk\n",
    "# Partially based on http://stackoverflow.com/a/39195332/323100\n",
    "# and on /LungMetastasis/HighResolutionScanAnalysis.ipynb\n",
    "Data['OutputNameVOIMyocard'] = [os.path.join(Root, sample, scan, sample + '_voi_myocard.zarr')\n",
    "                                for sample, scan\n",
    "                                in zip(Data['Sample'], Data['Scan'])]\n",
    "for c, row in notebook.tqdm(Data.iterrows(), total=len(Data)):\n",
    "    if not os.path.exists(row['OutputNameVOIMyocard']):\n",
    "        if len(row['VOIFilesMyocard']):\n",
    "            print('%2s/%s: Reading %s VOI slices from %s and saving to %s' % (c + 1,\n",
    "                                                                              len(Data),\n",
    "                                                                              len(row['VOIFilesMyocard']),\n",
    "                                                                              os.path.join(row['Sample'],\n",
    "                                                                                           row['Scan'],\n",
    "                                                                                           'voi_myocard'),\n",
    "                                                                              row['OutputNameVOIMyocard'][len(Root):]))\n",
    "            Reconstructions = dask_image.imread.imread(os.path.join(os.path.commonpath(row['VOIFilesMyocard']), '*.png'))\n",
    "            Reconstructions.rechunk(100).to_zarr(row['OutputNameVOIMyocard'],\n",
    "                                                 overwrite=True,\n",
    "                                                 compressor=Blosc(cname='zstd',\n",
    "                                                                  clevel=3,\n",
    "                                                                  shuffle=Blosc.BITSHUFFLE))\n",
    "        else:\n",
    "            print('%2s/%s: NO VOI slices found for myocard of %s' % (c + 1,\n",
    "                                                                     len(Data),\n",
    "                                                                     row['Sample']))\n",
    "            Data.at[c, 'OutputNameVOIMyocard'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the reconstructions as zarr arrays\n",
    "Patch = [dask.array.from_zarr(file) if file else numpy.nan for file in Data['OutputNameVOIPatch']]\n",
    "Myocard = [dask.array.from_zarr(file) if file else numpy.nan for file in Data['OutputNameVOIMyocard']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The three cardinal directions\n",
    "directions = ['Axial', 'Sagittal', 'Coronal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read or calculate the directional MIPs, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['MIP_Patch' + direction] = ''\n",
    "for c, row in notebook.tqdm(Data.iterrows(), desc='MIPs Patch', total=len(Data)):\n",
    "    for d, direction in notebook.tqdm(enumerate(directions),\n",
    "                                      desc=row['Sample'],\n",
    "                                      total=len(directions),\n",
    "                                      leave=False):\n",
    "        outfilepath = os.path.join(Root,\n",
    "                                   row['Sample'],\n",
    "                                   row['Scan'],\n",
    "                                   '%s.MIP.%s.Patch.png' % (row['Sample'], direction))  \n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c, 'MIP_Patch' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            try:\n",
    "                # Generate MIP\n",
    "                Data.at[c, 'MIP_Patch' + direction] = Patch[c].max(axis=d).compute()\n",
    "                # Save it out\n",
    "                imageio.imwrite(outfilepath, Data.at[c, 'MIP_Patch' + direction].astype('uint8'))\n",
    "            except AttributeError:\n",
    "                # No MIP to calculate\n",
    "                Data.at[c, 'MIP_Patch' + direction] = numpy.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read or calculate the directional MIPs, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['MIP_Myocard' + direction] = ''\n",
    "for c, row in notebook.tqdm(Data.iterrows(), desc='MIPs Patch', total=len(Data)):\n",
    "    for d, direction in notebook.tqdm(enumerate(directions),\n",
    "                                      desc=row['Sample'],\n",
    "                                      total=len(directions),\n",
    "                                      leave=False):\n",
    "        outfilepath = os.path.join(Root, row['Sample'], row['Scan'],\n",
    "                                   '%s.MIP.%s.Myocard.png' % (row['Sample'], direction))       \n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c, 'MIP_Myocard' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            try:\n",
    "                # Generate MIP\n",
    "                Data.at[c, 'MIP_Myocard' + direction] = Patch[c].max(axis=d).compute()\n",
    "                # Save it out\n",
    "                imageio.imwrite(outfilepath, Data.at[c, 'MIP_Myocard' + direction].astype('uint8'))\n",
    "            except AttributeError:\n",
    "                # No MIP to calculate\n",
    "                Data.at[c, 'MIP_Myocard' + direction] = numpy.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show MIP slices\n",
    "for voi in ('Patch', 'Myocard'):\n",
    "    for c, row in Data.iterrows():\n",
    "        outfilepath = os.path.join(Root,\n",
    "                                   row['Sample'],\n",
    "                                   row['Scan'],\n",
    "                                   row['Sample'] + '.MIPs.' + voi + '.png')\n",
    "        for d, direction in enumerate(directions):\n",
    "            plt.subplot(1, 3, d + 1)\n",
    "            plt.imshow(row['MIP_' + voi + direction])\n",
    "            plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "            plt.title('%s: %s' % (os.path.join(row['Sample'], voi), \n",
    "                                  direction + ' MIP'))\n",
    "\n",
    "            plt.axis('off')\n",
    "        if not os.path.exists(outfilepath):\n",
    "            plt.savefig(outfilepath, bbox_inches='tight')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference between the delineations from Tim\n",
    "Data['OutputNameVOISubMyocard'] = [os.path.join(Root, sample, scan, sample + '_voi_submyocard.zarr')\n",
    "                                   for sample, scan\n",
    "                                   in zip(Data['Sample'], Data['Scan'])]\n",
    "for c, row in notebook.tqdm(Data.iterrows(), total=len(Data)):\n",
    "    if not os.path.exists(row['OutputNameVOISubMyocard']):\n",
    "        print('%2s/%s: Calculating submyocard by subtracting patch'\n",
    "              ' from myocard and saving to %s' % (c + 1,\n",
    "                                                  len(Data),\n",
    "                                                  row['OutputNameVOISubMyocard'][len(Root):]))\n",
    "        SubMyocard = Myocard[c] - Patch[c]\n",
    "        SubMyocard.rechunk(100).to_zarr(row['OutputNameVOISubMyocard'],\n",
    "                               overwrite=True,\n",
    "                               compressor=Blosc(cname='zstd',\n",
    "                                                clevel=3,\n",
    "                                                shuffle=Blosc.BITSHUFFLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "SubMyocard = [dask.array.from_zarr(file) for file in Data['OutputNameVOISubMyocard']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdfasdfasdf=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare histograms\n",
    "HistogramPatch = [dask.array.histogram(ptch, bins=255, range=[0, 255]) for ptch in Patch]\n",
    "HistogramMyocard = [dask.array.histogram(myc, bins=255, range=[0, 255]) for myc in Myocard]\n",
    "HistogramSubMyocard = [dask.array.histogram(myc, bins=255, range=[0, 255]) for myc in SubMyocard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(131)\n",
    "for c,h in enumerate(HistogramPatch):\n",
    "    plt.semilogy(h[0], label=Data.Animal[c])\n",
    "plt.xlim([0,255])\n",
    "plt.legend()\n",
    "plt.title('Histograms patch')\n",
    "plt.subplot(132)\n",
    "for c,h in enumerate(HistogramMyocard):\n",
    "    plt.semilogy(h[0], label=Data.Animal[c])\n",
    "plt.xlim([0,255])\n",
    "plt.legend()\n",
    "plt.title('Histograms myocard')\n",
    "plt.subplot(133)\n",
    "for c,h in enumerate(HistogramSubMyocard):\n",
    "    plt.semilogy(h[0], label=Data.Animal[c])\n",
    "plt.xlim([0,255])\n",
    "plt.legend()\n",
    "plt.title('Histograms submyocard')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out submyocard slices\n",
    "for c, row in notebook.tqdm(Data.iterrows(),\n",
    "                            desc='Saving out myocard slices',\n",
    "                            total=len(Data)):\n",
    "    os.makedirs(os.path.join(Root, row.Sample, row.Scan, 'voi_submyocard'),\n",
    "                exist_ok=True)\n",
    "    for d, rec in notebook.tqdm(enumerate(SubMyocard[c]),\n",
    "                                total=len(SubMyocard[c]),\n",
    "                                desc=row.Sample,\n",
    "                                leave=False):\n",
    "        filename = os.path.join(Root,\n",
    "                                row.Sample,\n",
    "                                row.Scan,\n",
    "                                'voi_submyocard',\n",
    "                                os.path.basename(row.VOIFilesMyocard[d]))\n",
    "        if not os.path.exists(filename):\n",
    "            imageio.imsave(filename, rec.astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Data:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mean` gray value needs to be calculated and 'calibrated' to the total volume of the ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask the outside of the ROI that Tim drew\n",
    "MaskedPatch = [da.ma.masked_equal(v, 0) for v in Patch]\n",
    "MaskedMyocard = [da.ma.masked_equal(v, 0) for v in Myocard]\n",
    "MaskedSubMyocard = [da.ma.masked_equal(v, 0) for v in Patch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How large are the VOIs from Tim?\n",
    "# We select/mask everything non-zero and fill this whith one.\n",
    "RegionPatch = [da.ma.filled(da.ma.masked_not_equal(v, 0), 1) for v in Patch]\n",
    "# By summing it, we get the volume\n",
    "Data['VolumePatch'] = [rp.sum().compute() for rp in RegionPatch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c,e in enumerate(Data.Experiment.unique()):\n",
    "    print(c,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot volume of VOIs for comparison\n",
    "import random\n",
    "seaborn.swarmplot(data=Data,\n",
    "                  x='Timepoint', y='VolumePatch', hue='Experiment',\n",
    "                  s=10, linewidth=1.5)\n",
    "for i, tp in enumerate(Data['Timepoint'].unique()):\n",
    "    for j, row in Data[Data['Timepoint'] == tp].iterrows():\n",
    "        plt.annotate(row['Sample'],\n",
    "                     xy=(i + 0.025 * j, row['VolumePatch']))\n",
    "plt.ylim(ymin=0)   \n",
    "plt.title('Volume of the individual VOIs')\n",
    "plt.savefig(os.path.join(OutputDir, 'Volume.VOIs.png'),\n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.Timepoint.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.lmplot(data=Data,\n",
    "               x='Timepoint', y='VolumePatch', hue='Experiment', x_jitter=1)\n",
    "plt.gca().set_xticks(Data.Timepoint.unique())\n",
    "plt.savefig(os.path.join(OutputDir, 'Volume.VOIs.Regression.png'),\n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.catplot(data=Data, x='Experiment', y='VolumePatch', hue='Timepoint')\n",
    "plt.savefig(os.path.join(OutputDir, 'Volume.VOIs.Experiment.png'),\n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.lmplot(data=Data,\n",
    "               x='Timepoint', y='VolumePatch', hue='Experiment', col='Experiment')\n",
    "plt.gca().set_xticks(Data.Timepoint.unique())\n",
    "plt.savefig(os.path.join(OutputDir, 'Volume.VOIs.Separated.png'),\n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutputDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Sample', 'VolumePatch']].to_excel(os.path.join(OutputDir, 'Volume.Patches.xls'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.groupby(by=['Experiment'])['VolumePatch'].describe()[['count',\n",
    "                                                           'mean',\n",
    "                                                           'std',\n",
    "                                                           'min',\n",
    "                                                           'max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save mean of reconstruction gray values, which we can use for getting an overview of the image data\n",
    "Data['GrayValueMeanPatch'] = [p.mean().compute() for p in Patch]\n",
    "Data['GrayValueMeanMyocard'] = [m.mean().compute() for m in Myocard]\n",
    "Data['GrayValueMeanSubMyocard'] = [sm.mean().compute() for sm in SubMyocard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot grayvalue mean of datasets for comparison\n",
    "plt.subplot(131)\n",
    "seaborn.swarmplot(data=Data,\n",
    "                  x='Timepoint', y='GrayValueMeanPatch',\n",
    "                  hue='Experiment', s=10, linewidth=1.5, dodge=True)\n",
    "plt.title('Average grayvalue in the patch')\n",
    "# plt.ylim([0,5])\n",
    "plt.subplot(132)\n",
    "seaborn.swarmplot(data=Data,\n",
    "                  x='Timepoint', y='GrayValueMeanMyocard',\n",
    "                  hue='Experiment', s=10, linewidth=1.5, dodge=True)\n",
    "plt.title('in the myocard')\n",
    "# plt.ylim([0,5])\n",
    "plt.subplot(133)\n",
    "seaborn.swarmplot(data=Data,\n",
    "                  x='Timepoint', y='GrayValueMeanSubMyocard',\n",
    "                  hue='Experiment', s=10, linewidth=1.5, dodge=True)\n",
    "plt.title('in the sub-yocard')\n",
    "# plt.ylim([0,5])\n",
    "plt.savefig(os.path.join(OutputDir, 'Grayvalues.Mean.VOIs.png'),\n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Sample',\n",
    "      'GrayValueMeanPatch',\n",
    "      'GrayValueMeanMyocard',\n",
    "      'GrayValueMeanSubMyocard']].to_excel(os.path.join(OutputDir, 'Grayvalues.Mean.VOIs.xls'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.groupby(by=['Experiment'])['GrayValueMeanPatch'].describe()[['count',\n",
    "                                                                  'mean',\n",
    "                                                                  'std',\n",
    "                                                                  'min',\n",
    "                                                                  'max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.groupby(by=['Experiment'])['GrayValueMeanMyocard'].describe()[['count',\n",
    "                                                                    'mean',\n",
    "                                                                    'std',\n",
    "                                                                    'min',\n",
    "                                                                    'max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.groupby(by=['Experiment'])['GrayValueMeanSubMyocard'].describe()[['count',\n",
    "                                                                       'mean',\n",
    "                                                                       'std',\n",
    "                                                                       'min',\n",
    "                                                                       'max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['GrayValueMeanNormalizedToVOIVolume'] = [dask.array.divide(gvm,\n",
    "                                                                vv) for gvm, vv in zip(Data['GrayValueMeanPatch'],\n",
    "                                                                                       Data['VolumePatch'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot volume-normalized mean of datasets for comparison\n",
    "#seaborn.boxplot(data=Data, x='Experiment', y='GrayValueMeanNormalizedToVOIVolume', hue='Timepoint')\n",
    "seaborn.swarmplot(data=Data,\n",
    "                  x='Timepoint', y='GrayValueMeanNormalizedToVOIVolume',\n",
    "                  hue='Experiment', linewidth=1.5, s=10)\n",
    "for i, tp in enumerate(Data['Timepoint'].unique()):\n",
    "    for j, row in Data[Data['Timepoint'] == tp].iterrows():\n",
    "        plt.annotate(row['Sample'],\n",
    "                     xy=(i+0.025*j, row['GrayValueMeanNormalizedToVOIVolume']))\n",
    "plt.ylim(ymin=0, ymax=1.1*Data.GrayValueMeanNormalizedToVOIVolume.max())\n",
    "plt.title('Average grayvalue in the volumes of interest normalized to size of volume of interest')\n",
    "plt.savefig(os.path.join(OutputDir, 'Grayvalues.Mean.NormalizedVOI.png'),\n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save STD of reconstruction gray values, which we can use for getting an overview of the image data\n",
    "Data['GrayValueSTD'] = [m.std().compute() for m in MaskedPatch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot STD of datasets for comparison\n",
    "seaborn.catplot(data=Data, kind='box', x='Experiment', y='GrayValueSTD')\n",
    "seaborn.swarmplot(data=Data, x='Experiment', y='GrayValueSTD', linewidth=1.5, s=10, color='gray')\n",
    "plt.ylim(ymin=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_threshold(img, verbose=False):\n",
    "#     '''\n",
    "#     Calculate (Otsu) threshold of a stack, with some nice output if desired\n",
    "#     '''\n",
    "#     if len(numpy.shape(img)) != 3:\n",
    "#         print('Only works with a 3D stack')\n",
    "#         return()\n",
    "#     if verbose:\n",
    "#         print('The stack we use has a size of %s x %s x %s px' % numpy.shape(img))\n",
    "#     threshold = skimage.filters.threshold_otsu(dask.array.ravel(img.compute()))\n",
    "#     if verbose:\n",
    "#         seaborn.distplot(img.ravel())\n",
    "#         plt.axvline(threshold, label='Otsu@%s' % threshold, c=seaborn.color_palette()[1])\n",
    "#         plt.axvline(numpy.mean(img), label='Image mean@%0.2f' % img.mean(), c=seaborn.color_palette()[2])\n",
    "#         plt.legend()\n",
    "#         plt.semilogy()\n",
    "#         plt.xlim([0,255])\n",
    "#         plt.show()\n",
    "#     return(threshold.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://stackoverflow.com/a/38086839\n",
    "# h,bins=dask.array.histogram(VOIs[0], bins=range(0,255,4))\n",
    "# plt.semilogy(h)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thresholds\n",
    "preset = True\n",
    "if preset:\n",
    "    # Set them (from previous calculations)\n",
    "    Data['Threshold'] = [13, 63, 15, 4, 5, 5, 5, 8, 25, 7, 40, 16]\n",
    "#    Data['Threshold'] = [44, 46, 41, 16, 12, 15, 63, 16, 15, 18, 13, 13]\n",
    "#     Data['Threshold'] = [41, 45, 41, 12, 13, 19, 13]\n",
    "else:\n",
    "    # Calculate Threshold\n",
    "    Data['Threshold'] = [skimage.filters.threshold_otsu(\n",
    "        dask.array.ravel(\n",
    "            dask.array.ma.masked_less(\n",
    "                rec, 1).compute())) for rec in Patch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(Data.Threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean threshold of all samples\n",
    "Data['ThresholdMean'] = int(Data['Threshold'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['ThresholdMean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the thresholds\n",
    "seaborn.swarmplot(data=Data, x='Experiment', y='Threshold', hue='Timepoint', dodge=True, linewidth=1.5, s=10)\n",
    "for c,row in Data.iterrows():\n",
    "    if 'VP' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (0, row.Threshold))\n",
    "    elif 'F' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (1, row.Threshold))\n",
    "    elif 'Tacho' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (2, row.Threshold))\n",
    "plt.axhline(Data['ThresholdMean'].mean(), label='Mean threshold @ %s' % Data['ThresholdMean'].mean())\n",
    "plt.ylim(ymin=0)\n",
    "plt.legend()\n",
    "plt.title('Otsu thresholds of individual VOIs')\n",
    "plt.savefig(os.path.join(OutputDir, 'Thresholds.png'),\n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Sample', 'Threshold']].to_excel(os.path.join(OutputDir, 'Thresholds.xls'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.groupby(by=['Experiment'])['Threshold'].describe()[['count',\n",
    "                                                         'mean',\n",
    "                                                         'std',\n",
    "                                                         'min',\n",
    "                                                         'max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold the reconstructions individually                                                                                                                           Data.Sample)\n",
    "Data['OutputNameThresholded'] = [f.replace('.zarr',\n",
    "                                           '_thresholded_%s.zarr' % str(t).zfill(3)) for f, t in zip(Data['OutputNameVOIPatch'],\n",
    "                                                                                                     Data['Threshold'])]\n",
    "for c, row in Data.iterrows():\n",
    "    if os.path.exists(row['OutputNameThresholded']):  \n",
    "        print('%2s/%s: Already saved to %s' % (c + 1,\n",
    "                                               len(Data),\n",
    "                                               row['OutputNameThresholded'][len(Root):]))\n",
    "    else:\n",
    "        print('%2s/%s: Thresholding and saving to %s' % (c + 1,\n",
    "                                                         len(Data),\n",
    "                                                         row['OutputNameThresholded'][len(Root):]))\n",
    "        Thresholded = Patch[c] > row['Threshold']\n",
    "        Thresholded.rechunk(100).to_zarr(row['OutputNameThresholded'],\n",
    "                           overwrite=True,\n",
    "                           compressor=Blosc(cname='zstd',\n",
    "                                            clevel=3,\n",
    "                                            shuffle=Blosc.BITSHUFFLE))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold the reconstructions with the mean threshold                                                                                                                           Data.Sample)\n",
    "Data['OutputNameThresholdedMean'] = [f.replace('.zarr',\n",
    "                                           '_thresholded_%s.zarr' % str(t).zfill(3)) for f, t in zip(Data['OutputNameVOIPatch'],\n",
    "                                                                                                     Data['ThresholdMean'])]\n",
    "for c, row in Data.iterrows():\n",
    "    if os.path.exists(row['OutputNameThresholdedMean']):  \n",
    "        print('%2s/%s: Already saved to %s' % (c + 1,\n",
    "                                               len(Data),\n",
    "                                               row['OutputNameThresholdedMean'][len(Root):]))\n",
    "    else:\n",
    "        print('%2s/%s: Thresholding and saving to %s' % (c + 1,\n",
    "                                                         len(Data),\n",
    "                                                         row['OutputNameThresholded'][len(Root):]))\n",
    "        Thresholded = Patch[c] > row['ThresholdMean']\n",
    "        Thresholded.rechunk(100).to_zarr(row['OutputNameThresholdedMean'],\n",
    "                           overwrite=True,\n",
    "                           compressor=Blosc(cname='zstd', clevel=3, shuffle=Blosc.BITSHUFFLE))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DASK arrays of the thresholded samples\n",
    "individualThreshold = False\n",
    "if individualThreshold:\n",
    "    Thresholded = [dask.array.from_zarr(file) for file in Data['OutputNameThresholded']]\n",
    "    print('Loading individually thresholded stacks')\n",
    "else:\n",
    "    Thresholded = [dask.array.from_zarr(file) for file in Data['OutputNameThresholdedMean']]\n",
    "    print('Loading all stacks with a threshold of %s' % Data.ThresholdMean.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read or calculate the directional MIPs of the thresholded datasets,\n",
    "# put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Thresholded_MIP_' + direction] = ''\n",
    "for c, row in notebook.tqdm(Data.iterrows(), desc='MIPs', total=len(Data)):\n",
    "    for d, direction in notebook.tqdm(enumerate(directions),\n",
    "                                      desc=row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        outfilepath = os.path.join(Root, row['Sample'], row['Scan'], '%s.Thresholded.MIP.%s.png' % (row['Sample'], direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c,'Thresholded_MIP_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate MIP\n",
    "            Data.at[c,'Thresholded_MIP_' + direction] = Thresholded[c].max(axis=d).compute()\n",
    "            # Save it out\n",
    "            imageio.imwrite(outfilepath, Data.at[c,'Thresholded_MIP_' + direction].astype('uint8'))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show thresholded MIP slices\n",
    "for c, row in Data.iterrows():\n",
    "    for d, direction in enumerate(directions):\n",
    "        plt.subplot(1, 3, d + 1)\n",
    "        plt.imshow(row['Thresholded_MIP_' + direction])\n",
    "        plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "#         plt.title('%s/%s: %s, %s' % (c + 1,\n",
    "#                                      len(Data),\n",
    "#                                      row['Sample'],\n",
    "#                                      direction + ' MIP'))\n",
    "        plt.title('%s, %s' % (row['Sample'], direction + ' MIP'))\n",
    "        plt.axis('off')\n",
    "    plt.savefig(os.path.join(Root, row['Sample'], row['Scan'], row['Sample'] + '.Thresholded.MIPs.png'),\n",
    "                bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the images, so we can see if they contain approximately the same *thresholded* volume\n",
    "Data['ThresholdedVolume'] = [th.sum().compute() for th in Thresholded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data[['Sample', 'ThresholdedVolume']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the thresholded volumes\n",
    "seaborn.swarmplot(data=Data, x='Experiment', y='ThresholdedVolume', hue='Timepoint', dodge=True, linewidth=1.5, s=10)\n",
    "for c,row in Data.iterrows():\n",
    "    if 'VP' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (0, row.ThresholdedVolume))\n",
    "    elif 'F' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (1, row.ThresholdedVolume))\n",
    "    elif 'Tacho' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (2, row.ThresholdedVolume))\n",
    "plt.ylim(ymin=0)\n",
    "if individualThreshold:\n",
    "    plt.title('Volume of the individually thresholded images, corresponding to the total thresholded volume')\n",
    "    plt.savefig(os.path.join(OutputDir, 'Volumes.Thresholded.ThresholdedIndividually.png'),\n",
    "                bbox_inches='tight')    \n",
    "else:\n",
    "    plt.title('Volume of the images thresholded all equally, corresponding to the total thresholded volume')    \n",
    "    plt.savefig(os.path.join(OutputDir, 'Volumes.Thresholded.ThresholdedEqually.png'),\n",
    "                bbox_inches='tight')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutputDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Sample', 'Experiment', 'Threshold', 'ThresholdedVolume',\n",
    "      'GrayValueMeanPatch', 'GrayValueMeanMyocard', 'GrayValueMeanSubMyocard']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characterization of vessel diameter\n",
    "- Fill the vessels/ventricle (with something like `skimage.fill.small.holes`)\n",
    "    This doesn't seem to be working in the 3D case (maybe because of small holes) but we just loop through every slice and do it for each and everyone of it. This is bad code, but works :)\n",
    "- Remove all the big stuff with `tophat`\n",
    "- Calculate the distance-transformation or skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ourfiller(image, verbose=False):\n",
    "    '''\n",
    "    Since we cannot seem to get remove_small_holes to work in 3D, we simply brute-force it on every slice.\n",
    "    Thanks to `tqdm_notebook` we also get a progress bar...\n",
    "    And afterwards generate an output array.\n",
    "    '''\n",
    "    filled = [skimage.morphology.remove_small_holes(s, area_threshold=1e4) for\n",
    "              s in tqdm.notebook.tqdm(image, leave=False)]\n",
    "    if verbose:\n",
    "        plt.subplot(131)\n",
    "        plt.imshow(image[len(filled)//2,:,:])\n",
    "        plt.title('Original')\n",
    "        plt.subplot(132)\n",
    "        plt.imshow(filled[len(filled)//2,:,:])       \n",
    "        plt.title('Filled (output)')\n",
    "        plt.subplot(133)        \n",
    "        plt.imshow(image[len(filled)//2,:,:], alpha=0.5)       \n",
    "        plt.imshow(filled[len(filled)//2,:,:], cmap='viridis', alpha=0.5)       \n",
    "        plt.title('Overlay')        \n",
    "        plt.show()    \n",
    "    return(filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pad_edges(image, howmanypixels=25):\n",
    "#     '''There *has* to be a better way to set the edges to one, but I havent found one'''\n",
    "#     closed = image.copy()\n",
    "#     closed[:howmanypixels,:,:] = True\n",
    "#     closed[:,:howmanypixels,:] = True\n",
    "#     closed[:,:,:howmanypixels] = True\n",
    "#     closed[-howmanypixels:,:,:] = True\n",
    "#     closed[:,-howmanypixels:,:] = True\n",
    "#     closed[:,:,-howmanypixels:] = True\n",
    "#     return(closed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fill_hollow_bones(image, verbose=False):\n",
    "#     '''\n",
    "#     We flood-fill the image from one edge.\n",
    "#     Then we add the inversion of this to the original image and thus filled all the long bones.\n",
    "#     '''\n",
    "#     dilated = skimage.morphology.binary_dilation(\n",
    "#         skimage.morphology.binary_dilation(\n",
    "#             skimage.morphology.binary_dilation(image)))\n",
    "#     closed = pad_edges(dilated)\n",
    "#     # Flood fill from one edge and invert the (boolean) result\n",
    "#     flooded = ~skimage.morphology.flood_fill(closed.astype('int'),\n",
    "#                                              seed_point=(30,30,30),\n",
    "#                                              new_value=1).astype('bool')\n",
    "#     # Add the inverted result to the original image, filling the long bones\n",
    "#     filled = numpy.add(image, skimage.morphology.binary_dilation(\n",
    "#         skimage.morphology.binary_dilation(\n",
    "#             skimage.morphology.binary_dilation(flooded))))\n",
    "#     if verbose:\n",
    "#         plt.subplot(131)\n",
    "#         plt.imshow(image[len(filled)//2,:,:])\n",
    "#         plt.title('original')\n",
    "#         plt.subplot(132)\n",
    "#         plt.imshow(flooded[len(filled)//2,:,:])       \n",
    "#         plt.title('flooded')\n",
    "#         plt.subplot(133)        \n",
    "#         plt.imshow(image[len(filled)//2,:,:], alpha=0.5)       \n",
    "#         plt.imshow(filled[len(filled)//2,:,:], cmap='viridis', alpha=0.5)       \n",
    "#         plt.title('filled (output)')        \n",
    "#         plt.show()\n",
    "#     return(filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the flood-filled image\n",
    "# Since this takes a while, we don't do it in a Pythonic way\n",
    "# e.g. (Flooded = [ourfiller(t, verbose=True) for t in Tresholded])\n",
    "# but in a loop with saving in between.\n",
    "Data['OutputNameFlooded'] = [f.replace('.zarr', '_flooded.zarr') for f in Data['OutputNameThresholded']]\n",
    "for c, row in Data.iterrows():\n",
    "    if os.path.exists(row['OutputNameFlooded']):  \n",
    "        print('%2s/%s: Already saved to %s' % (c + 1,\n",
    "                                               len(Data),\n",
    "                                               row['OutputNameFlooded'][len(Root):]))\n",
    "    else:\n",
    "        print('%2s/%s: %s: Filling holes' % (c + 1,\n",
    "                                             len(Data),\n",
    "                                             row['Sample']))\n",
    "        Flooded = ourfiller(Thresholded[c].compute())\n",
    "        Flooded = da.stack(Flooded[:])\n",
    "        print('%11s: Saving to %s' % (row['Sample'],\n",
    "                                     row['OutputNameFlooded'][len(Root):]))\n",
    "        Flooded.rechunk(100).to_zarr(row['OutputNameFlooded'],\n",
    "                        overwrite=True,\n",
    "                        compressor=Blosc(cname='zstd', clevel=3, shuffle=Blosc.BITSHUFFLE))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DASK arrays with the filled samples\n",
    "Flooded = [dask.array.from_zarr(file) for file in Data['OutputNameFlooded']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DASK\n",
    "# Read or calculate the middle slices of the flooded images,\n",
    "# put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Flooded_Mid_' + direction] = [None] * len(VOIs)\n",
    "for c, row in tqdm.notebook.tqdm(Data.iterrows(),\n",
    "                                 desc='Middle flooded images',\n",
    "                                 total=len(Data)):\n",
    "    for d, direction in tqdm.notebook.tqdm(enumerate(directions),\n",
    "                                           desc=row['Sample'],\n",
    "                                           leave=False,\n",
    "                                           total=len(directions)):\n",
    "        outfilepath = os.path.join(Root, row['Sample'], row['Scan'],\n",
    "                                   '%s.Thresholded%03d.Flooded.Middle.%s.png' % (row['Sample'],\n",
    "                                                                                 row['Threshold'],\n",
    "                                                                                 direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c,'Flooded_Mid_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate requested axial view\n",
    "            if 'Axial' in direction:\n",
    "                Data.at[c,'Flooded_Mid_' + direction] = Flooded[c][Data['Size'][c][0]//2]\n",
    "            if 'Sagittal' in direction:\n",
    "                Data.at[c,'Flooded_Mid_' + direction] = Flooded[c][:,Data['Size'][c][1]//2,:]\n",
    "            if 'Coronal' in direction:\n",
    "                Data.at[c,'Flooded_Mid_' + direction] = Flooded[c][:,:,Data['Size'][c][2]//2]\n",
    "            # Save the calculated 'direction' view out\n",
    "            # Dask only calculates/reads the images here at this point...\n",
    "            imageio.imwrite(outfilepath,\n",
    "                            (Data.at[c,'Flooded_Mid_' + direction].astype('uint8')*255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show middle flood-filled images\n",
    "for c, row in Data.iterrows():\n",
    "    for d, direction in enumerate(directions):\n",
    "        plt.subplot(1, 3, d + 1)\n",
    "        plt.imshow(row['Flooded_Mid_' + direction])\n",
    "        plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "#         plt.title('%s/%s: %s, %s' % (c + 1,\n",
    "#                                      len(Data),\n",
    "#                                      row['Sample'],\n",
    "#                                      direction + ' MIP'))\n",
    "        plt.title('%s: %s' % (row['Sample'],\n",
    "                              direction + ' MIP'))\n",
    "        \n",
    "        plt.axis('off')\n",
    "    plt.savefig(os.path.join(Root, row['Sample'], row['Scan'], row['Sample'] + '.Flooded.MiddleSlices.png'),\n",
    "                bbox_inches='tight')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show middle flood-filled images with overlay\n",
    "for c, row in Data.iterrows():\n",
    "    for d, direction in enumerate(directions):\n",
    "        plt.subplot(1, 3, d + 1)\n",
    "        plt.imshow(row['Flooded_Mid_' + direction])\n",
    "        plt.imshow(dask.array.ma.masked_less(row['Thresholded_Mid_' + direction], 1), alpha=0.5, cmap='viridis')        \n",
    "        plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "#         plt.title('%s/%s: %s, %s' % (c + 1,\n",
    "#                                      len(Data),\n",
    "#                                      row['Sample'],\n",
    "#                                      direction + ' MIP'))\n",
    "        plt.title('%s: %s' % (row['Sample'],\n",
    "                              direction + ' MIP'))\n",
    "        \n",
    "        plt.axis('off')\n",
    "    plt.savefig(os.path.join(Root, row['Sample'], row['Scan'], row['Sample'] + '.Flooded.Overlay.MiddleSlices.png'),\n",
    "                bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(Data['Thresholded_Mid_Axial'][0])\n",
    "# plt.imshow(dask.array.ma.masked_less(Data['Flooded_Mid_Axial'][0], 1), alpha=0.5, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show middle axis flooded images\n",
    "for d,direction in enumerate(directions):\n",
    "    for c,row in Data.iterrows():\n",
    "        plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)\n",
    "        plt.imshow(row['Flooded_Mid_' + direction])\n",
    "        plt.imshow(dask.array.ma.masked_less(row['Thresholded_Mid_' + direction], 1), alpha=0.5, cmap='viridis')\n",
    "        plt.title('Middle %s slice of tophat of\\n%s together with original' % (direction, row['Sample']))\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][c], 'um'))    \n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.subplot(121)\n",
    "# plt.imshow(Data['Thresholded_Mid_' + direction][1])\n",
    "# plt.subplot(122)\n",
    "# plt.imshow(Data['Flooded_Mid_' + direction][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flooded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reconstructions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the white tophat\n",
    "# https://scikit-image.org/docs/dev/api/skimage.morphology.html#skimage.morphology.white_tophat\n",
    "# e.g. the bright spots of the image that are smaller than the structuring element.\n",
    "# We use a ball-shaped (sphere) structuring element\n",
    "# Again, since this takes a *long* while, we don't do it nice and pythonic,\n",
    "# but in a loop with intermediate saving\n",
    "# e.g. not (Tophat = [skimage.morphology.white_tophat(f, selem=skimage.morphology.ball(7)) for f in Flooded])\n",
    "# but\n",
    "tophatselem = 5\n",
    "Data['OutputNameTophat'] = [f.replace('.zarr', '_tophat_%s.zarr' % tophatselem) for f in Data['OutputNameFlooded']]\n",
    "Tophat = [numpy.nan for file in Data['OutputNameTophat']]\n",
    "for c, row in Data.iterrows():\n",
    "    if os.path.exists(row['OutputNameTophat']):\n",
    "        print('%2s/%s: Already saved to %s' % (c + 1,\n",
    "                                               len(Data),\n",
    "                                               row['OutputNameTophat'][len(Root):]))\n",
    "    else:\n",
    "        print('%2s/%s: %s: Calculating white thophat with a \"selem\" of %s' % (c + 1,\n",
    "                                                                              len(Data),\n",
    "                                                                              row['Sample'],\n",
    "                                                                              tophatselem))\n",
    "        Tophat = skimage.morphology.white_tophat(Flooded[c].compute(),\n",
    "                                                 selem=skimage.morphology.ball(tophatselem))\n",
    "        Tophat = da.stack(Tophat[:])        \n",
    "        print('%11s: Saving to %s' % (row['Sample'],\n",
    "                                      row['OutputNameTophat'][len(Root):]))\n",
    "        Tophat.rechunk(100).to_zarr(row['OutputNameTophat'],\n",
    "                       overwrite=True,\n",
    "                       compressor=Blosc(cname='zstd', clevel=3, shuffle=Blosc.BITSHUFFLE))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img = Thresholded[1][1250:-1350,100:-100,100:-100].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D\n",
    "#tophat = skimage.morphology.white_tophat(a, selem=skimage.morphology.disk(33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D\n",
    "#plt.subplot(131)\n",
    "#plt.imshow(a)\n",
    "#plt.subplot(132)\n",
    "#plt.imshow(tophat)\n",
    "#plt.subplot(133)\n",
    "#plt.imshow(numpy.bitwise_xor(a,\n",
    "#                             tophat), alpha=0.5)\n",
    "#plt.imshow(tophat, cmap='viridis', alpha=0.5)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 3D topat\n",
    "#tophat = skimage.morphology.white_tophat(img, selem=skimage.morphology.ball(11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whichslice = numpy.shape(img)[0]//2\n",
    "#plt.subplot(131)\n",
    "#plt.imshow(img[whichslice])\n",
    "#plt.subplot(132)\n",
    "#plt.imshow(tophat[whichslice])\n",
    "#plt.subplot(133)\n",
    "#plt.imshow(numpy.bitwise_xor(img[whichslice],\n",
    "#                             tophat[whichslice]), alpha=0.5)\n",
    "#plt.imshow(tophat[whichslice], cmap='viridis', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DASK arrays with the tophat-filtered samples (e.g. only containing the smaller vessels)\n",
    "Tophat = [dask.array.from_zarr(file) for file in Data['OutputNameTophat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DASK\n",
    "# Read or calculate the middle slices of the Tophat images,\n",
    "# put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Tophat_Mid_' + direction] = [None] * len(VOIs)\n",
    "for c, row in tqdm.notebook.tqdm(Data.iterrows(), desc='Middle tophat images', total=len(Data)):\n",
    "    for d, direction in tqdm.notebook.tqdm(enumerate(directions),\n",
    "                                      desc=row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        outfilepath = os.path.join(Root, row['Sample'], row['Scan'],\n",
    "                                   '%s.Thresholded%03d.Tophat.Middle.%s.png' % (row['Sample'],\n",
    "                                                                                row['Threshold'],\n",
    "                                                                                 direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c,'Tophat_Mid_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate requested axial view\n",
    "            if 'Axial' in direction:\n",
    "                Data.at[c,'Tophat_Mid_' + direction] = Tophat[c][Data['Size'][c][0]//2]\n",
    "            if 'Sagittal' in direction:\n",
    "                Data.at[c,'Tophat_Mid_' + direction] = Tophat[c][:,Data['Size'][c][1]//2,:]\n",
    "            if 'Coronal' in direction:\n",
    "                Data.at[c,'Tophat_Mid_' + direction] = Tophat[c][:,:,Data['Size'][c][2]//2]\n",
    "            # Save the calculated 'direction' view out\n",
    "            # Dask only calculates/reads the images here at this point...\n",
    "            imageio.imwrite(outfilepath, (Data.at[c,'Tophat_Mid_' + direction].astype('uint8')*255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show middle flood-filled images with overlay\n",
    "for c, row in Data.iterrows():\n",
    "    for d, direction in enumerate(directions):\n",
    "        plt.subplot(1, 3, d + 1)\n",
    "        plt.imshow(row['Tophat_Mid_' + direction])\n",
    "        plt.imshow(dask.array.ma.masked_less(row['Thresholded_Mid_' + direction], 1), alpha=0.5, cmap='viridis')        \n",
    "        plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "#         plt.title('%s/%s: %s, %s' % (c + 1,\n",
    "#                                      len(Data),\n",
    "#                                      row['Sample'],\n",
    "#                                      direction + ' MIP'))\n",
    "        plt.title('%s: %s' % (row['Sample'],\n",
    "                              direction + ' MIP'))\n",
    "        \n",
    "        plt.axis('off')\n",
    "    plt.savefig(os.path.join(Root, row['Sample'], row['Scan'], row['Sample'] + '.Tophat.Overlay.MiddleSlices.png'),\n",
    "                bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show middle slices of tophat data\n",
    "for d,direction in enumerate(directions):\n",
    "    for c,row in Data.iterrows():\n",
    "        plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)\n",
    "        plt.imshow(row['Thresholded_Mid_' + direction])\n",
    "        plt.imshow(dask.array.ma.masked_less(row['Tophat_Mid_' + direction], 1), alpha=0.5, cmap='viridis')\n",
    "        plt.title('Middle %s slice of tophat of\\n%s together with original' % (direction, row['Sample']))\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][c], 'um'))    \n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show overlay\n",
    "# for c, direction in enumerate(directions):\n",
    "#     print(c, direction)    \n",
    "#     if c:\n",
    "#         plt.imshow(numpy.rot90(center, axes=(0,c))[len(flooded)//2,:,:], alpha=0.33)\n",
    "#         plt.imshow(numpy.rot90(flooded, axes=(0,c))[len(flooded)//2,:,:], alpha=0.33, cmap='magma')\n",
    "#         plt.imshow(numpy.rot90(tophat, axes=(0,c))[len(flooded)//2,:,:], alpha=0.33, cmap='viridis')\n",
    "#     else:\n",
    "#         plt.imshow(center[len(flooded)//2,:,:], alpha=0.33)\n",
    "#         plt.imshow(flooded[len(flooded)//2,:,:], alpha=0.33, cmap='magma')\n",
    "#         plt.imshow(tophat[len(flooded)//2,:,:], alpha=0.33, cmap='viridis')\n",
    "#     plt.gca().add_artist(ScaleBar(Data['Voxelsize'][0], 'um'))\n",
    "#     plt.axis('off')\n",
    "#     plt.savefig('Overlay-%s.png' % direction, bbox_inches='tight')  \n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the skeletonization\n",
    "# By multiplying them later on we get a color-coded medial axis transformation\n",
    "# This conforms to what we would expect from 'skimage.morphology.medial_axis(image, return_distance=True)' which does *not* work for 3D images\n",
    "Data['OutputNameSkeleton'] = [f.replace('.zarr', '_skeleton.zarr') for f in Data['OutputNameTophat']]\n",
    "for c, row in Data.iterrows():\n",
    "    if os.path.exists(row['OutputNameSkeleton']):  \n",
    "        print('%2s/%s: Already saved to %s' % (c + 1,\n",
    "                                               len(Data),\n",
    "                                               row['OutputNameSkeleton'][len(Root):]))\n",
    "    else:\n",
    "        print('%2s/%s: %s: Calculating skeletonization' % (c + 1,\n",
    "                                                           len(Data),\n",
    "                                                           row['Sample']))\n",
    "        Skeleton = skimage.morphology.skeletonize_3d(Tophat[c])\n",
    "        Skeleton = da.stack(Skeleton[:])\n",
    "        print('%11s: Saving to %s' % (row['Sample'],\n",
    "                                      row['OutputNameSkeleton'][len(Root):]))\n",
    "        Skeleton.rechunk(100).to_zarr(row['OutputNameSkeleton'],\n",
    "                         overwrite=True,\n",
    "                         compressor=Blosc(cname='zstd', clevel=3, shuffle=Blosc.BITSHUFFLE))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DASK arrays with the skeletonized images\n",
    "Skeleton = [dask.array.from_zarr(file) for file in Data['OutputNameSkeleton']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read or calculate the middle slices of the Skeletonization images,\n",
    "# put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Skeleton_Mid_' + direction] = [None] * len(VOIs)\n",
    "for c, row in tqdm.notebook.tqdm(Data.iterrows(), desc='Middle skeleton images', total=len(Data)):\n",
    "    for d, direction in tqdm.notebook.tqdm(enumerate(directions),\n",
    "                                      desc=row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        outfilepath = os.path.join(Root, row['Sample'], row['Scan'],\n",
    "                                   '%s.Thresholded%03d.Skeleton.Middle.%s.png' % (row['Sample'],\n",
    "                                                                                  row['Threshold'],\n",
    "                                                                                  direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c,'Skeleton_Mid_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate requested axial view\n",
    "            if 'Axial' in direction:\n",
    "                Data.at[c,'Skeleton_Mid_' + direction] = Skeleton[c][Data['Size'][c][0]//2]\n",
    "            if 'Sagittal' in direction:\n",
    "                Data.at[c,'Skeleton_Mid_' + direction] = Skeleton[c][:,Data['Size'][c][1]//2,:]\n",
    "            if 'Coronal' in direction:\n",
    "                Data.at[c,'Skeleton_Mid_' + direction] = Skeleton[c][:,:,Data['Size'][c][2]//2]\n",
    "            # Save the calculated 'direction' view out\n",
    "            # Dask only calculates/reads the images here at this point...\n",
    "            imageio.imwrite(outfilepath, Data.at[c,'Skeleton_Mid_' + direction].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show middle skeletonized images with overlay\n",
    "for c, row in Data.iterrows():\n",
    "    for d, direction in enumerate(directions):\n",
    "        plt.subplot(1, 3, d + 1)\n",
    "        plt.imshow(row['Mid_' + direction])\n",
    "        plt.imshow(dask.array.ma.masked_less(row['Skeleton_Mid_' + direction], 1), alpha=0.5, cmap='viridis')        \n",
    "        plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "#         plt.title('%s/%s: %s, %s' % (c + 1,\n",
    "#                                      len(Data),\n",
    "#                                      row['Sample'],\n",
    "#                                      direction + ' MIP'))\n",
    "        plt.title('%s: %s' % (row['Sample'],\n",
    "                              direction + ' MIP'))\n",
    "        \n",
    "        plt.axis('off')\n",
    "    plt.savefig(os.path.join(Root, row['Sample'], row['Scan'], row['Sample'] + '.Skeleton.Overlay.MiddleSlices.png'),\n",
    "                bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d,direction in enumerate(directions):\n",
    "    for c,row in Data.iterrows():\n",
    "        plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)\n",
    "        plt.imshow(row['Tophat_Mid_' + direction])\n",
    "        plt.imshow(row['Skeleton_Mid_' + direction], alpha=0.5, cmap='viridis')\n",
    "        plt.title('Middle %s slice of Skeletonization of\\n%s together with tophat' % (direction, row['Sample']))\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][c], 'um'))    \n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a, b = scipy.ndimage.morphology.distance_transform_edt(Tophat[0][900:1000], sampling=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numpy.shape(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(a[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tophat[0][800:-800,800:-800,800:-800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tophat[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Folder',\n",
    "      'Sample',\n",
    "#       'Scan',\n",
    "      'SampleNameLength',\n",
    "      'ScanNameLength',\n",
    "      'Experiment',\n",
    "      'Timepoint',\n",
    "      'LogFile',\n",
    "#       'VOIFolders',\n",
    "      'VOIFolder',\n",
    "      'Voxelsize',\n",
    "#       'VOISlices',\n",
    "      'Number of VOI slices',\n",
    "      'Size',\n",
    "      'VOIVolume',\n",
    "      'GrayValueMean',\n",
    "      'GrayValueMeanNormalizedToVOIVolume',\n",
    "      'Threshold',\n",
    "      'ThresholdMean',\n",
    "      'ThresholdedVolume',\n",
    "      'GrayValueMeanNormalizedToThresholdedVolume']].to_excel(os.path.join(OutputDir, 'Data_' + get_git_hash() + '.xls'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in Data:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the euclidean distance transformation\n",
    "subsampling = None\n",
    "if subsampling:\n",
    "    Data['OutputNameEDT'] = [f.replace('.zarr', '_edt_sampling%s.zarr' % subsampling) for f in Data['OutputNameTophat']]\n",
    "else:\n",
    "    Data['OutputNameEDT'] = [f.replace('.zarr', '_edt.zarr') for f in Data['OutputNameTophat']]    \n",
    "# Calculate EDT\n",
    "for c, row in Data.iterrows():\n",
    "    if os.path.exists(row['OutputNameEDT']):\n",
    "        print('%2s/%s: Already saved to %s' % (c + 1,\n",
    "                                               len(Data),\n",
    "                                               row['OutputNameEDT'][len(Root):]))\n",
    "    else:\n",
    "        print('%2s/%s: %s: Calculating euclidean distance transformation' % (c + 1,\n",
    "                                                                             len(Data),\n",
    "                                                                             row['Sample'].rjust(Data['SampleNameLength'].max())))\n",
    "        EDT = scipy.ndimage.morphology.distance_transform_edt(Tophat[c].astype('bool'),\n",
    "                                                              sampling=subsampling)\n",
    "        EDT = da.stack(EDT[:])\n",
    "        print('%11s: Saving to %s' % (row['Sample'].rjust(Data['SampleNameLength'].max()),\n",
    "                                      row['OutputNameEDT']))\n",
    "        EDT.rechunk(100).to_zarr(row['OutputNameEDT'],\n",
    "                    overwrite=True,\n",
    "                    compressor=Blosc(cname='zstd', clevel=3, shuffle=Blosc.BITSHUFFLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the EDT from the saved zarr files   \n",
    "EDT = [dask.array.from_zarr(file) for file in Data['OutputNameEDT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DASK\n",
    "# Read or calculate the middle slices of the EDT images,\n",
    "# put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['EDT_Mid_' + direction] = [None] * len(VOIs)\n",
    "for c, row in tqdm.notebook.tqdm(Data.iterrows(), desc='Middle EDT images', total=len(Data)):\n",
    "    for d, direction in tqdm.notebook.tqdm(enumerate(directions),\n",
    "                                           desc=row['Sample'],\n",
    "                                           leave=False,\n",
    "                                           total=len(directions)):\n",
    "        outfilepath = os.path.join(Root, row['Sample'], row['Scan'],\n",
    "                                   '%s.Thresholded%03d.EDT.Middle.%s.png' % (row['Sample'],\n",
    "                                                                             row['Threshold'],\n",
    "                                                                             direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c,'EDT_Mid_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate requested axial view\n",
    "            if 'Axial' in direction:\n",
    "                Data.at[c,'EDT_Mid_' + direction] = EDT[c][Data['Size'][c][0]//2]\n",
    "            if 'Sagittal' in direction:\n",
    "                Data.at[c,'EDT_Mid_' + direction] = EDT[c][:,Data['Size'][c][1]//2,:]\n",
    "            if 'Coronal' in direction:\n",
    "                Data.at[c,'EDT_Mid_' + direction] = EDT[c][:,:,Data['Size'][c][2]//2]\n",
    "            # Save the calculated 'direction' view out\n",
    "            # Dask only calculates/reads the images here at this point...\n",
    "            imageio.imwrite(outfilepath,Data.at[c,'EDT_Mid_' + direction].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d,direction in enumerate(directions):\n",
    "    for c,row in Data.iterrows():\n",
    "        plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)\n",
    "        plt.imshow(row['Flooded_Mid_' + direction])\n",
    "        plt.imshow(row['EDT_Mid_' + direction], alpha=0.5, cmap='viridis')\n",
    "        plt.title('Middle %s slice of EDT of\\n%s together with original' % (direction, row['Sample']))\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][c], 'um'))    \n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d,direction in enumerate(directions):\n",
    "    for c,row in Data.iterrows():\n",
    "        plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)\n",
    "        plt.imshow(row['EDT_Mid_' + direction], alpha=0.5, cmap='viridis')\n",
    "        plt.title('Middle %s slice of EDT of\\n%s together with original' % (direction, row['Sample']))\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][c], 'um'))    \n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show middle skeletonized images with overlay\n",
    "for c, row in Data.iterrows():\n",
    "    for d, direction in enumerate(directions):\n",
    "        plt.subplot(1, 3, d + 1)\n",
    "        plt.imshow(row['EDT_Mid_' + direction])\n",
    "        plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "#         plt.title('%s/%s: %s, %s' % (c + 1,\n",
    "#                                      len(Data),\n",
    "#                                      row['Sample'],\n",
    "#                                      direction + ' MIP'))\n",
    "        plt.title('%s: %s' % (row['Sample'],\n",
    "                              direction + ' MIP'))\n",
    "        \n",
    "        plt.axis('off')\n",
    "    plt.savefig(os.path.join(Root, row['Sample'], row['Scan'], row['Sample'] + '.EDT.MiddleSlices.png'),\n",
    "                bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distance on skeleton\n",
    "Data['OutputNameSkelDist'] = [f.replace('.zarr', '_skeletondistance.zarr') for f in Data['OutputNameTophat']]\n",
    "# Calculate edt\n",
    "for c, row in Data.iterrows():\n",
    "    if os.path.exists(row['OutputNameSkelDist']):\n",
    "        print('%2s/%s: Already saved to %s' % (c + 1,\n",
    "                                               len(Data),\n",
    "                                               row['OutputNameSkelDist'][len(Root):]))\n",
    "    else:\n",
    "        print('%2s/%s: %s: Multiplying skeleton and EDT and saving to %s' % (c + 1,\n",
    "                                                                             len(Data),\n",
    "                                                                             row['Sample'].rjust(Data['SampleNameLength'].max()),\n",
    "                                                                             row['OutputNameSkelDist'][len(Root):]))\n",
    "        SkelDist = numpy.multiply(Skeleton[c], EDT[c])\n",
    "        SkelDist.rechunk(100).to_zarr(row['OutputNameSkelDist'],\n",
    "                         overwrite=True,\n",
    "                         compressor=Blosc(cname='zstd', clevel=3, shuffle=Blosc.BITSHUFFLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DASK arrays with the skeleton-distance\n",
    "SkelDist = [dask.array.from_zarr(file) for file in Data['OutputNameSkelDist']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DASK\n",
    "# Read or calculate the middle slices of the SkelDist images,\n",
    "# put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['SkelDist_Mid_' + direction] = [None] * len(VOIs)\n",
    "for c, row in tqdm.notebook.tqdm(Data.iterrows(), desc='Middle SkelDist images', total=len(Data)):\n",
    "    for d, direction in tqdm.notebook.tqdm(enumerate(directions),\n",
    "                                      desc=row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        outfilepath = os.path.join(Root, row['Sample'], row['Scan'],\n",
    "                                   '%s.Thresholded%03d.SkelDist.Middle.%s.png' % (row['Sample'],\n",
    "                                                                                  row['Threshold'],\n",
    "                                                                                  direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c,'SkelDist_Mid_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate requested axial view\n",
    "            if 'Axial' in direction:\n",
    "                Data.at[c,'SkelDist_Mid_' + direction] = SkelDist[c][Data['Size'][c][0]//2]\n",
    "            if 'Sagittal' in direction:\n",
    "                Data.at[c,'SkelDist_Mid_' + direction] = SkelDist[c][:,Data['Size'][c][1]//2,:]\n",
    "            if 'Coronal' in direction:\n",
    "                Data.at[c,'SkelDist_Mid_' + direction] = SkelDist[c][:,:,Data['Size'][c][2]//2]\n",
    "            # Save the calculated 'direction' view out\n",
    "            # Dask only calculates/reads the images here at this point...\n",
    "            imageio.imwrite(outfilepath,Data.at[c,'SkelDist_Mid_' + direction].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d,direction in enumerate(directions):\n",
    "    for c,row in Data.iterrows():\n",
    "        plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)\n",
    "#         plt.imshow(row['Thresholded_Mid_' + direction])\n",
    "#         plt.imshow(dask.array.ma.masked_where(0, row['EDT_Mid_' + direction]), alpha=0.5, cmap='viridis')\n",
    "        plt.imshow(row['SkelDist_Mid_' + direction], alpha=0.5, cmap='viridis')\n",
    "        plt.title('Middle %s slice of SkelDist of\\n%s together with original' % (direction, row['Sample']))\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][c], 'um'))    \n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sl = 999\n",
    "# plt.subplot(131)\n",
    "# plt.imshow(numpy.max(Skeleton[0], axis=0), cmap='viridis')\n",
    "# plt.subplot(132)\n",
    "# plt.imshow(numpy.max(EDT[0], axis=0), cmap='viridis')\n",
    "# plt.subplot(133)\n",
    "# plt.imshow(numpy.max(SkelDist[0], axis=0), cmap='viridis')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read or calculate skeletondistance MIPs, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['MIP_SkelDist_' + direction] = [None] * len(VOIs)\n",
    "for c, row in tqdm.notebook.tqdm(Data.iterrows(), desc='MIPs SkelDist', total=len(Data)):\n",
    "    for d, direction in tqdm.notebook.tqdm(enumerate(directions),\n",
    "                                      desc=row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        outfilepath = os.path.join(Root, row['Sample'], row['Scan'],\n",
    "                                   '%s.Thresholded%03d.MIP.SkelDist.%s.png' % (row['Sample'],\n",
    "                                                                               row['Threshold'],\n",
    "                                                                               direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c,'MIP_SkelDist_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Keep *this* reconstruction in RAM for a bit\n",
    "            img = SkelDist[c].astype('uint8').persist()\n",
    "            # Generate MIP\n",
    "            Data.at[c,'MIP_SkelDist_' + direction] = img.max(axis=d).compute()\n",
    "            # Save it out\n",
    "            imageio.imwrite(outfilepath,\n",
    "                            Data.at[c,'MIP_SkelDist_' + direction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in Data.iterrows():\n",
    "    for j, direction in enumerate(directions):\n",
    "        plt.subplot(1,3,j+1)\n",
    "#         plt.imshow(row['MIP_' + direction], alpha=0.5)\n",
    "#         plt.imshow(dask.array.ma.masked_less(row['MIP_SkelDist_' + direction],1), cmap='viridis')        \n",
    "        plt.imshow(row['MIP_SkelDist_' + direction], cmap='viridis')            \n",
    "        plt.title('%s view' % direction)\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][c], 'um'))                \n",
    "        plt.axis('off')        \n",
    "    plt.suptitle('%02d/%02d: MIP with Skeleton overlay %s' % (i+1, len(Data), row['Sample']))\n",
    "    plt.savefig(os.path.join(Root, row['Sample'], row['Scan'], row['Sample'] + '.SkelDist.MiddleSlices.png'),\n",
    "                bbox_inches='tight')    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SkelDist[0].max().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['SkelDistMean'] = [dask.array.mean(skldst).compute() for skldst in SkelDist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['SkelDistMeanNormalized'] = [dask.array.mean(skldst).compute()/tv for skldst, tv in zip(SkelDist, Data['ThresholdedVolume'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['SkelDistSTD'] = [dask.array.std(skldst).compute() for skldst in SkelDist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean of datasets for comparison\n",
    "seaborn.catplot(data=Data, kind='box', x='Sample', y='SkelDistMean')\n",
    "seaborn.swarmplot(data=Data, x='Sample', y='SkelDistMean', linewidth=1.5, s=10, color='gray');\n",
    "plt.ylabel('Mean Skeleton distance value')\n",
    "plt.ylim(ymin=0)\n",
    "plt.savefig(os.path.join(OutputDir,\n",
    "                         'Skeleton_Average_Distance.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean of datasets for comparison\n",
    "seaborn.catplot(data=Data, kind='box', x='Sample', y='SkelDistMeanNormalized')\n",
    "seaborn.swarmplot(data=Data, x='Sample', y='SkelDistMeanNormalized', linewidth=1.5, s=10, color='gray');\n",
    "plt.ylabel('Mean Skeleton distance value, normalized to thresholded volume')\n",
    "plt.ylim(ymin=0)\n",
    "plt.savefig(os.path.join(OutputDir,\n",
    "                         'Skeleton_Average_Distance_Normalized.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot STD of datasets for comparison\n",
    "seaborn.catplot(data=Data, kind='box', x='Sample', y='SkelDistSTD')\n",
    "seaborn.swarmplot(data=Data, x='Sample', y='SkelDistSTD', linewidth=1.5, s=10, color='gray');\n",
    "plt.ylabel('Skeleton distance STD')\n",
    "plt.ylim(ymin=0)\n",
    "plt.savefig(os.path.join(OutputDir,\n",
    "                         'Skeleton_Average_Distance_STD.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
