{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volume/Density analysis\n",
    "Let's look at the 'vessel density' in the delineated path region and in the region below them\n",
    "Tim delineated the patch region in the hearts and cut out a 'pizza slice' shaped region below the patch.\n",
    "\n",
    "*This* notebook is based on the original `Vessels.ipynb` notebook, but has significantly changed course :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import os\n",
    "import glob\n",
    "import pandas\n",
    "import numpy\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import seaborn\n",
    "import dask\n",
    "import dask_image.imread\n",
    "from dask.distributed import Client\n",
    "from numcodecs import Blosc\n",
    "from tqdm import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x18272ab76a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set dask temporary folder\n",
    "# Do this before creating a client: https://stackoverflow.com/a/62804525/323100\n",
    "if 'Linux' in platform.system():\n",
    "    tmp = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "elif 'Darwin' in platform.system():\n",
    "    import tempfile\n",
    "    tmp = tempfile.gettempdir()\n",
    "else:\n",
    "    if 'anaklin' in platform.node():\n",
    "        tmp = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        tmp = os.path.join('D:\\\\')\n",
    "dask.config.set({'temporary_directory': os.path.join(tmp, 'tmp')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haberthu\\Miniconda3\\lib\\site-packages\\distributed\\node.py:160: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 63814 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can seee what DASK is doing at \"http://localhost:63814/status\"\n"
     ]
    }
   ],
   "source": [
    "# Start dask client and tell where we can see what it does\n",
    "client = Client()\n",
    "print('You can seee what DASK is doing at \"http://localhost:%s/status\"' % client.scheduler_info()['services']['dashboard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings in the notebook\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure defaults\n",
    "plt.rc('image', cmap='gray', interpolation='nearest')  # Display all images in b&w and with 'nearest' interpolation\n",
    "plt.rcParams['figure.figsize'] = (14, 7)  # Size up figures a bit\n",
    "plt.rcParams['figure.dpi'] = 200  # Increase dpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup scale bar defaults\n",
    "plt.rcParams['scalebar.location'] = 'lower right'\n",
    "plt.rcParams['scalebar.frameon'] = False\n",
    "plt.rcParams['scalebar.color'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all plots identically\n",
    "lines = 3\n",
    "# And then do something like\n",
    "# plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_git_hash():\n",
    "    \"\"\"\n",
    "    Get the current git hash from the repository.\n",
    "    Based on http://stackoverflow.com/a/949391/323100 and\n",
    "    http://stackoverflow.com/a/18283905/323100\n",
    "    \"\"\"\n",
    "    from subprocess import Popen, PIPE\n",
    "    import os\n",
    "    gitprocess = Popen(['git', '--git-dir', os.path.join(os.getcwd(), '.git'),\n",
    "                        'rev-parse', '--short', '--verify', 'HEAD'],\n",
    "                       stdout=PIPE)\n",
    "    (output, _) = gitprocess.communicate()\n",
    "    return output.strip().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are working with version e9b8728 of the analyis notebook.\n"
     ]
    }
   ],
   "source": [
    "# What are we working with?\n",
    "the_current_git_hash = get_git_hash()\n",
    "print('We are working with version %s of the analyis notebook.'\n",
    "      % the_current_git_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the output folder\n",
    "# Including the git hash, so we (potentially) have different versions of all the images we generate\n",
    "OutputDir = os.path.join('Output', the_current_git_hash)\n",
    "os.makedirs(OutputDir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are loading all the data from F:\\Hearts Melly\n"
     ]
    }
   ],
   "source": [
    "# Different locations if running either on Linux or Windows\n",
    "if 'anaklin' in platform.node():\n",
    "    FastSSD = True\n",
    "else:\n",
    "    FastSSD = False\n",
    "# to speed things up significantly\n",
    "if 'Linux' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "    else:\n",
    "        BasePath = os.path.join(os.sep, 'home', 'habi', '1272')\n",
    "elif 'Darwin' in platform.system():\n",
    "    BasePath = os.path.join('/Volumes/2TBSSD/')\n",
    "else:\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        if 'anaklin' in platform.node():\n",
    "            BasePath = os.path.join('S:\\\\')\n",
    "        else:\n",
    "            BasePath = os.path.join('D:\\\\', 'Results')\n",
    "Root = os.path.join(BasePath, 'Hearts Melly')\n",
    "print('We are loading all the data from %s' % Root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixelsize(logfile):\n",
    "    \"\"\"Get the pixel size from the scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Image Pixel' in line and 'Scaled' not in line:\n",
    "                pixelsize = float(line.split('=')[1])\n",
    "    return(pixelsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make us a dataframe for saving all that we need\n",
    "Data = pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get *all* log files\n",
    "Data['LogFile'] = [f for f in sorted(glob.glob(os.path.join(Root, '**', '*.log'), recursive=True))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all folders\n",
    "Data['Folder'] = [os.path.dirname(f) for f in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of all non-rec logfiles\n",
    "for c, row in Data.iterrows():\n",
    "    if 'rec' not in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'ctan.log' in row.LogFile:\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'rectmp.log' in row.LogFile:\n",
    "        Data.drop([c], inplace=True)\n",
    "# Reset dataframe to something that we would get if we only would have loaded the 'rec' files\n",
    "Data = Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all folders we don't need\n",
    "for c, row in Data.iterrows():\n",
    "    if 'Rat' not in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'Rat4' in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'Rat5' in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'Test' in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "# Reset dataframe to something that we would get if we only would have loaded the 'rec' files\n",
    "Data = Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LogFile</th>\n",
       "      <th>Folder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F:\\Hearts Melly\\Rat80\\2214_7.5um_Al1mm_wetfoam...</td>\n",
       "      <td>F:\\Hearts Melly\\Rat80\\2214_7.5um_Al1mm_wetfoam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\rec\\Rat...</td>\n",
       "      <td>F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\rec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F:\\Hearts Melly\\Rat66\\2214_7.5um_Al1mm\\rec\\Rat...</td>\n",
       "      <td>F:\\Hearts Melly\\Rat66\\2214_7.5um_Al1mm\\rec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F:\\Hearts Melly\\Rat81\\2214_7.5um_Al1mm_wetfoam...</td>\n",
       "      <td>F:\\Hearts Melly\\Rat81\\2214_7.5um_Al1mm_wetfoam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F:\\Hearts Melly\\Rat70\\2214_7.5um_Al1mm_wetfoam...</td>\n",
       "      <td>F:\\Hearts Melly\\Rat70\\2214_7.5um_Al1mm_wetfoam...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             LogFile  \\\n",
       "5  F:\\Hearts Melly\\Rat80\\2214_7.5um_Al1mm_wetfoam...   \n",
       "0  F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\rec\\Rat...   \n",
       "1  F:\\Hearts Melly\\Rat66\\2214_7.5um_Al1mm\\rec\\Rat...   \n",
       "6  F:\\Hearts Melly\\Rat81\\2214_7.5um_Al1mm_wetfoam...   \n",
       "3  F:\\Hearts Melly\\Rat70\\2214_7.5um_Al1mm_wetfoam...   \n",
       "\n",
       "                                              Folder  \n",
       "5  F:\\Hearts Melly\\Rat80\\2214_7.5um_Al1mm_wetfoam...  \n",
       "0         F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\rec  \n",
       "1         F:\\Hearts Melly\\Rat66\\2214_7.5um_Al1mm\\rec  \n",
       "6  F:\\Hearts Melly\\Rat81\\2214_7.5um_Al1mm_wetfoam...  \n",
       "3  F:\\Hearts Melly\\Rat70\\2214_7.5um_Al1mm_wetfoam...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate us some meaningful colums\n",
    "Data['Animal'] = [l[len(Root)+1:].split(os.sep)[0] for l in Data['LogFile']]\n",
    "Data['Scan'] = ['_'.join(l[len(Root)+1:].split(os.sep)[1:-1]) for l in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We habe 8 scans to work with\n"
     ]
    }
   ],
   "source": [
    "print('We habe %s scans/samples to work with' % len(Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in animals list from Ludovic\n",
    "AnimalTable = pandas.read_excel('Animals.xlsx',\n",
    "                                engine='openpyxl',\n",
    "                                header=None,\n",
    "                                names=['Animal', 'Gender', '', 'Experiment', 'Timepoint'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in data from animals table\n",
    "for c, rowdata in Data.iterrows():\n",
    "    for d, rowanimals in AnimalTable.iterrows():\n",
    "        if str(rowanimals.Animal) in rowdata.Animal:\n",
    "            Data.at[c, 'Experiment'] = rowanimals.Experiment\n",
    "            Data.at[c, 'Timepoint'] = rowanimals.Timepoint\n",
    "            Data.at[c, 'Gender'] = rowanimals.Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we merged the data we can rename the column to a more reusable name\n",
    "Data.columns = Data.columns.str.replace('Animal', 'Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LogFile</th>\n",
       "      <th>Folder</th>\n",
       "      <th>Sample</th>\n",
       "      <th>Scan</th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Timepoint</th>\n",
       "      <th>Gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F:\\Hearts Melly\\Rat70\\2214_7.5um_Al1mm_wetfoam...</td>\n",
       "      <td>F:\\Hearts Melly\\Rat70\\2214_7.5um_Al1mm_wetfoam...</td>\n",
       "      <td>Rat70</td>\n",
       "      <td>2214_7.5um_Al1mm_wetfoam_rec</td>\n",
       "      <td>tachosil only</td>\n",
       "      <td>28.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\rec\\Rat...</td>\n",
       "      <td>F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\rec</td>\n",
       "      <td>Rat63</td>\n",
       "      <td>2214_7.5um_Al1mm_rec</td>\n",
       "      <td>F</td>\n",
       "      <td>7.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F:\\Hearts Melly\\Rat71\\2214_7.5um_Al1mm_wetfoam...</td>\n",
       "      <td>F:\\Hearts Melly\\Rat71\\2214_7.5um_Al1mm_wetfoam...</td>\n",
       "      <td>Rat71</td>\n",
       "      <td>2214_7.5um_Al1mm_wetfoam_rec</td>\n",
       "      <td>tachosil only</td>\n",
       "      <td>28.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F:\\Hearts Melly\\Rat81\\2214_7.5um_Al1mm_wetfoam...</td>\n",
       "      <td>F:\\Hearts Melly\\Rat81\\2214_7.5um_Al1mm_wetfoam...</td>\n",
       "      <td>Rat81</td>\n",
       "      <td>2214_7.5um_Al1mm_wetfoam_rec</td>\n",
       "      <td>V+P</td>\n",
       "      <td>28.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F:\\Hearts Melly\\Rat66\\2214_7.5um_Al1mm\\rec\\Rat...</td>\n",
       "      <td>F:\\Hearts Melly\\Rat66\\2214_7.5um_Al1mm\\rec</td>\n",
       "      <td>Rat66</td>\n",
       "      <td>2214_7.5um_Al1mm_rec</td>\n",
       "      <td>V+P</td>\n",
       "      <td>28.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             LogFile  \\\n",
       "3  F:\\Hearts Melly\\Rat70\\2214_7.5um_Al1mm_wetfoam...   \n",
       "0  F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\rec\\Rat...   \n",
       "4  F:\\Hearts Melly\\Rat71\\2214_7.5um_Al1mm_wetfoam...   \n",
       "6  F:\\Hearts Melly\\Rat81\\2214_7.5um_Al1mm_wetfoam...   \n",
       "1  F:\\Hearts Melly\\Rat66\\2214_7.5um_Al1mm\\rec\\Rat...   \n",
       "\n",
       "                                              Folder Sample  \\\n",
       "3  F:\\Hearts Melly\\Rat70\\2214_7.5um_Al1mm_wetfoam...  Rat70   \n",
       "0         F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\rec  Rat63   \n",
       "4  F:\\Hearts Melly\\Rat71\\2214_7.5um_Al1mm_wetfoam...  Rat71   \n",
       "6  F:\\Hearts Melly\\Rat81\\2214_7.5um_Al1mm_wetfoam...  Rat81   \n",
       "1         F:\\Hearts Melly\\Rat66\\2214_7.5um_Al1mm\\rec  Rat66   \n",
       "\n",
       "                           Scan     Experiment  Timepoint Gender  \n",
       "3  2214_7.5um_Al1mm_wetfoam_rec  tachosil only       28.0      M  \n",
       "0          2214_7.5um_Al1mm_rec              F        7.0      F  \n",
       "4  2214_7.5um_Al1mm_wetfoam_rec  tachosil only       28.0      M  \n",
       "6  2214_7.5um_Al1mm_wetfoam_rec            V+P       28.0      M  \n",
       "1          2214_7.5um_Al1mm_rec            V+P       28.0      M  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclusion from Tims visual inspection\n",
    "# R63\n",
    "# R65\n",
    "# R66\n",
    "# R70\n",
    "#exclude = [63, 65, 66, 70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop samples which should be excluded\n",
    "# Based on https://stackoverflow.com/a/13851602\n",
    "#for c,row in Data.iterrows():\n",
    "#    for ex in exclude:\n",
    "#        if str(ex) in row.Sample:\n",
    "#            Data.drop(c, inplace=True)\n",
    "#Data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # \"Filter\" to subset that we want\n",
    "# for c,row in Data.iterrows():\n",
    "#     if 'cu_10um' not in row.Scan:\n",
    "#         Data.drop(c, inplace=True)\n",
    "# Data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LogFile</th>\n",
       "      <th>Folder</th>\n",
       "      <th>Sample</th>\n",
       "      <th>Scan</th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Timepoint</th>\n",
       "      <th>Gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\rec\\Rat...</td>\n",
       "      <td>F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\rec</td>\n",
       "      <td>Rat63</td>\n",
       "      <td>2214_7.5um_Al1mm_rec</td>\n",
       "      <td>F</td>\n",
       "      <td>7.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F:\\Hearts Melly\\Rat66\\2214_7.5um_Al1mm\\rec\\Rat...</td>\n",
       "      <td>F:\\Hearts Melly\\Rat66\\2214_7.5um_Al1mm\\rec</td>\n",
       "      <td>Rat66</td>\n",
       "      <td>2214_7.5um_Al1mm_rec</td>\n",
       "      <td>V+P</td>\n",
       "      <td>28.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F:\\Hearts Melly\\Rat67\\2214_7.5um_Al1mm\\rec\\Rat...</td>\n",
       "      <td>F:\\Hearts Melly\\Rat67\\2214_7.5um_Al1mm\\rec</td>\n",
       "      <td>Rat67</td>\n",
       "      <td>2214_7.5um_Al1mm_rec</td>\n",
       "      <td>F</td>\n",
       "      <td>28.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F:\\Hearts Melly\\Rat70\\2214_7.5um_Al1mm_wetfoam...</td>\n",
       "      <td>F:\\Hearts Melly\\Rat70\\2214_7.5um_Al1mm_wetfoam...</td>\n",
       "      <td>Rat70</td>\n",
       "      <td>2214_7.5um_Al1mm_wetfoam_rec</td>\n",
       "      <td>tachosil only</td>\n",
       "      <td>28.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F:\\Hearts Melly\\Rat71\\2214_7.5um_Al1mm_wetfoam...</td>\n",
       "      <td>F:\\Hearts Melly\\Rat71\\2214_7.5um_Al1mm_wetfoam...</td>\n",
       "      <td>Rat71</td>\n",
       "      <td>2214_7.5um_Al1mm_wetfoam_rec</td>\n",
       "      <td>tachosil only</td>\n",
       "      <td>28.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             LogFile  \\\n",
       "0  F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\rec\\Rat...   \n",
       "1  F:\\Hearts Melly\\Rat66\\2214_7.5um_Al1mm\\rec\\Rat...   \n",
       "2  F:\\Hearts Melly\\Rat67\\2214_7.5um_Al1mm\\rec\\Rat...   \n",
       "3  F:\\Hearts Melly\\Rat70\\2214_7.5um_Al1mm_wetfoam...   \n",
       "4  F:\\Hearts Melly\\Rat71\\2214_7.5um_Al1mm_wetfoam...   \n",
       "\n",
       "                                              Folder Sample  \\\n",
       "0         F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\rec  Rat63   \n",
       "1         F:\\Hearts Melly\\Rat66\\2214_7.5um_Al1mm\\rec  Rat66   \n",
       "2         F:\\Hearts Melly\\Rat67\\2214_7.5um_Al1mm\\rec  Rat67   \n",
       "3  F:\\Hearts Melly\\Rat70\\2214_7.5um_Al1mm_wetfoam...  Rat70   \n",
       "4  F:\\Hearts Melly\\Rat71\\2214_7.5um_Al1mm_wetfoam...  Rat71   \n",
       "\n",
       "                           Scan     Experiment  Timepoint Gender  \n",
       "0          2214_7.5um_Al1mm_rec              F        7.0      F  \n",
       "1          2214_7.5um_Al1mm_rec            V+P       28.0      M  \n",
       "2          2214_7.5um_Al1mm_rec              F       28.0      M  \n",
       "3  2214_7.5um_Al1mm_wetfoam_rec  tachosil only       28.0      M  \n",
       "4  2214_7.5um_Al1mm_wetfoam_rec  tachosil only       28.0      M  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tim delineated both the patch and myocard region\n",
    "# We thus need to duplicate the dataframe for loading them correctly\n",
    "Data = pandas.concat([Data] *2, ignore_index=True)\n",
    "Data.sort_values('Sample', inplace=True)\n",
    "Data.reset_index(drop=True, inplace=True)\n",
    "# Fill actual VOI column with alternating values\n",
    "Data['VOI'] = ['myocard', 'patch'] * (len(Data)//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['VOIFolder'] = [os.path.join(os.path.dirname(f),\n",
    "                                  'voi_' + v) for f,v in zip(Data['Folder'], Data['VOI'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LogFile</th>\n",
       "      <th>Folder</th>\n",
       "      <th>Sample</th>\n",
       "      <th>Scan</th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Timepoint</th>\n",
       "      <th>Gender</th>\n",
       "      <th>VOI</th>\n",
       "      <th>VOIFolder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>F:\\Hearts Melly\\Rat71\\2214_7.5um_Al1mm_wetfoam...</td>\n",
       "      <td>F:\\Hearts Melly\\Rat71\\2214_7.5um_Al1mm_wetfoam...</td>\n",
       "      <td>Rat71</td>\n",
       "      <td>2214_7.5um_Al1mm_wetfoam_rec</td>\n",
       "      <td>tachosil only</td>\n",
       "      <td>28.0</td>\n",
       "      <td>M</td>\n",
       "      <td>patch</td>\n",
       "      <td>F:\\Hearts Melly\\Rat71\\2214_7.5um_Al1mm_wetfoam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>F:\\Hearts Melly\\Rat82\\2214_7.5um_Al1mm\\rec\\Rat...</td>\n",
       "      <td>F:\\Hearts Melly\\Rat82\\2214_7.5um_Al1mm\\rec</td>\n",
       "      <td>Rat82</td>\n",
       "      <td>2214_7.5um_Al1mm_rec</td>\n",
       "      <td>V</td>\n",
       "      <td>28.0</td>\n",
       "      <td>M</td>\n",
       "      <td>myocard</td>\n",
       "      <td>F:\\Hearts Melly\\Rat82\\2214_7.5um_Al1mm\\voi_myo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\rec\\Rat...</td>\n",
       "      <td>F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\rec</td>\n",
       "      <td>Rat63</td>\n",
       "      <td>2214_7.5um_Al1mm_rec</td>\n",
       "      <td>F</td>\n",
       "      <td>7.0</td>\n",
       "      <td>F</td>\n",
       "      <td>patch</td>\n",
       "      <td>F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\voi_patch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F:\\Hearts Melly\\Rat66\\2214_7.5um_Al1mm\\rec\\Rat...</td>\n",
       "      <td>F:\\Hearts Melly\\Rat66\\2214_7.5um_Al1mm\\rec</td>\n",
       "      <td>Rat66</td>\n",
       "      <td>2214_7.5um_Al1mm_rec</td>\n",
       "      <td>V+P</td>\n",
       "      <td>28.0</td>\n",
       "      <td>M</td>\n",
       "      <td>patch</td>\n",
       "      <td>F:\\Hearts Melly\\Rat66\\2214_7.5um_Al1mm\\voi_patch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F:\\Hearts Melly\\Rat67\\2214_7.5um_Al1mm\\rec\\Rat...</td>\n",
       "      <td>F:\\Hearts Melly\\Rat67\\2214_7.5um_Al1mm\\rec</td>\n",
       "      <td>Rat67</td>\n",
       "      <td>2214_7.5um_Al1mm_rec</td>\n",
       "      <td>F</td>\n",
       "      <td>28.0</td>\n",
       "      <td>M</td>\n",
       "      <td>myocard</td>\n",
       "      <td>F:\\Hearts Melly\\Rat67\\2214_7.5um_Al1mm\\voi_myo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              LogFile  \\\n",
       "17  F:\\Hearts Melly\\Rat71\\2214_7.5um_Al1mm_wetfoam...   \n",
       "28  F:\\Hearts Melly\\Rat82\\2214_7.5um_Al1mm\\rec\\Rat...   \n",
       "1   F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\rec\\Rat...   \n",
       "7   F:\\Hearts Melly\\Rat66\\2214_7.5um_Al1mm\\rec\\Rat...   \n",
       "10  F:\\Hearts Melly\\Rat67\\2214_7.5um_Al1mm\\rec\\Rat...   \n",
       "\n",
       "                                               Folder Sample  \\\n",
       "17  F:\\Hearts Melly\\Rat71\\2214_7.5um_Al1mm_wetfoam...  Rat71   \n",
       "28         F:\\Hearts Melly\\Rat82\\2214_7.5um_Al1mm\\rec  Rat82   \n",
       "1          F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\rec  Rat63   \n",
       "7          F:\\Hearts Melly\\Rat66\\2214_7.5um_Al1mm\\rec  Rat66   \n",
       "10         F:\\Hearts Melly\\Rat67\\2214_7.5um_Al1mm\\rec  Rat67   \n",
       "\n",
       "                            Scan     Experiment  Timepoint Gender      VOI  \\\n",
       "17  2214_7.5um_Al1mm_wetfoam_rec  tachosil only       28.0      M    patch   \n",
       "28          2214_7.5um_Al1mm_rec              V       28.0      M  myocard   \n",
       "1           2214_7.5um_Al1mm_rec              F        7.0      F    patch   \n",
       "7           2214_7.5um_Al1mm_rec            V+P       28.0      M    patch   \n",
       "10          2214_7.5um_Al1mm_rec              F       28.0      M  myocard   \n",
       "\n",
       "                                            VOIFolder  \n",
       "17  F:\\Hearts Melly\\Rat71\\2214_7.5um_Al1mm_wetfoam...  \n",
       "28  F:\\Hearts Melly\\Rat82\\2214_7.5um_Al1mm\\voi_myo...  \n",
       "1    F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\voi_patch  \n",
       "7    F:\\Hearts Melly\\Rat66\\2214_7.5um_Al1mm\\voi_patch  \n",
       "10  F:\\Hearts Melly\\Rat67\\2214_7.5um_Al1mm\\voi_myo...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VOI images\n",
    "Data['VOIFiles'] = [sorted(glob.glob(os.path.join(f, '*.png'))) for f in Data['VOIFolder']]\n",
    "Data['Number of VOI slices'] = [len(vs) for vs in Data['VOIFiles']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what's in the VOI folders\n",
    "# And drop those that are empty\n",
    "for c, row in Data.iterrows():\n",
    "    if not len(row['VOIFiles']):\n",
    "        print('%s contains *no* PNG files' % row['VOIFolder'])\n",
    "        Data.drop(c, inplace=True)\n",
    "Data.reset_index(drop=True, inplace=True)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We habe 12 folders to work with\n"
     ]
    }
   ],
   "source": [
    "print('We habe %s folders to work with' % len(Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get voxelsize from logfiles\n",
    "Data['Voxelsize'] = [get_pixelsize(log) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5abeca0a4bb74b8aba0a03ee319abc8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading VOIs:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert all VOI slices into a DASK array and save them to disk\n",
    "# Partially based on http://stackoverflow.com/a/39195332/323100\n",
    "# and on /LungMetastasis/HighResolutionScanAnalysis.ipynb\n",
    "Data['OutputNameVOI'] = [os.path.join(os.path.dirname(f),\n",
    "                                      '%s_%s_voi_%s.zarr' % (sample,\n",
    "                                                             scan,\n",
    "                                                             voi)) for f, sample, scan, voi in zip(Data.Folder,\n",
    "                                                                                                   Data.Sample,\n",
    "                                                                                                   Data.Scan,\n",
    "                                                                                                   Data.VOI)]\n",
    "# Read VOI files and save them to rechunked .zarr files for faster further access\n",
    "for c, row in notebook.tqdm(Data.iterrows(), desc='Reading VOIs', total=len(Data)):\n",
    "    if not os.path.exists(row['OutputNameVOI']):\n",
    "        print('%2s/%s: Reading %s VOI slices from %s and saving to %s' % (c + 1,\n",
    "                                                                          len(Data),\n",
    "                                                                          row['Number of VOI slices'],\n",
    "                                                                          row['VOIFolder'][len(Root)+1:],\n",
    "                                                                          row['OutputNameVOI'][len(Root)+1:]))\n",
    "        VOI = dask_image.imread.imread(os.path.join(row['VOIFolder'], '*.png'))\n",
    "        # Rechunking (to 'auto' size) is slow, but we only need to do it once and\n",
    "        # further reads of the data are much faster.\n",
    "        VOI.rechunk(chunks='auto').to_zarr(row['OutputNameVOI'],\n",
    "                                 overwrite=True,\n",
    "                                 compressor=Blosc(cname='zstd',\n",
    "                                                  clevel=9,\n",
    "                                                  shuffle=Blosc.BITSHUFFLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the reconstructions a zarr arrays\n",
    "VOIs = [dask.array.from_zarr(file) for file in Data['OutputNameVOI']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print chunk size\n",
    "# for i in VOIs:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How big are the datasets?\n",
    "Data['Size'] = [rec.shape for rec in VOIs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The three cardinal directions\n",
    "directions = ['Axial', 'Sagittal', 'Coronal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LogFile</th>\n",
       "      <th>Folder</th>\n",
       "      <th>Sample</th>\n",
       "      <th>Scan</th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Timepoint</th>\n",
       "      <th>Gender</th>\n",
       "      <th>VOI</th>\n",
       "      <th>VOIFolder</th>\n",
       "      <th>VOIFiles</th>\n",
       "      <th>Number of VOI slices</th>\n",
       "      <th>Voxelsize</th>\n",
       "      <th>OutputNameVOI</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\rec\\Rat...</td>\n",
       "      <td>F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\rec</td>\n",
       "      <td>Rat63</td>\n",
       "      <td>2214_7.5um_Al1mm_rec</td>\n",
       "      <td>F</td>\n",
       "      <td>7.0</td>\n",
       "      <td>F</td>\n",
       "      <td>myocard</td>\n",
       "      <td>F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\voi_myo...</td>\n",
       "      <td>[F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\voi_my...</td>\n",
       "      <td>1045</td>\n",
       "      <td>7.500482</td>\n",
       "      <td>F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\Rat63_2...</td>\n",
       "      <td>(1045, 3072, 3072)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\rec\\Rat...</td>\n",
       "      <td>F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\rec</td>\n",
       "      <td>Rat63</td>\n",
       "      <td>2214_7.5um_Al1mm_rec</td>\n",
       "      <td>F</td>\n",
       "      <td>7.0</td>\n",
       "      <td>F</td>\n",
       "      <td>patch</td>\n",
       "      <td>F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\voi_patch</td>\n",
       "      <td>[F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\voi_pa...</td>\n",
       "      <td>1045</td>\n",
       "      <td>7.500482</td>\n",
       "      <td>F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\Rat63_2...</td>\n",
       "      <td>(1045, 3072, 3072)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\rec\\Rat...</td>\n",
       "      <td>F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\rec</td>\n",
       "      <td>Rat63</td>\n",
       "      <td>2214_7.5um_Al1mm_rec</td>\n",
       "      <td>F</td>\n",
       "      <td>7.0</td>\n",
       "      <td>F</td>\n",
       "      <td>myocard</td>\n",
       "      <td>F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\voi_myo...</td>\n",
       "      <td>[F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\voi_my...</td>\n",
       "      <td>1045</td>\n",
       "      <td>7.500482</td>\n",
       "      <td>F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\Rat63_2...</td>\n",
       "      <td>(1045, 3072, 3072)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\rec\\Rat...</td>\n",
       "      <td>F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\rec</td>\n",
       "      <td>Rat63</td>\n",
       "      <td>2214_7.5um_Al1mm_rec</td>\n",
       "      <td>F</td>\n",
       "      <td>7.0</td>\n",
       "      <td>F</td>\n",
       "      <td>patch</td>\n",
       "      <td>F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\voi_patch</td>\n",
       "      <td>[F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\voi_pa...</td>\n",
       "      <td>1045</td>\n",
       "      <td>7.500482</td>\n",
       "      <td>F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\Rat63_2...</td>\n",
       "      <td>(1045, 3072, 3072)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F:\\Hearts Melly\\Rat66\\2214_7.5um_Al1mm\\rec\\Rat...</td>\n",
       "      <td>F:\\Hearts Melly\\Rat66\\2214_7.5um_Al1mm\\rec</td>\n",
       "      <td>Rat66</td>\n",
       "      <td>2214_7.5um_Al1mm_rec</td>\n",
       "      <td>V+P</td>\n",
       "      <td>28.0</td>\n",
       "      <td>M</td>\n",
       "      <td>myocard</td>\n",
       "      <td>F:\\Hearts Melly\\Rat66\\2214_7.5um_Al1mm\\voi_myo...</td>\n",
       "      <td>[F:\\Hearts Melly\\Rat66\\2214_7.5um_Al1mm\\voi_my...</td>\n",
       "      <td>1041</td>\n",
       "      <td>7.500482</td>\n",
       "      <td>F:\\Hearts Melly\\Rat66\\2214_7.5um_Al1mm\\Rat66_2...</td>\n",
       "      <td>(1041, 3072, 3072)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             LogFile  \\\n",
       "0  F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\rec\\Rat...   \n",
       "1  F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\rec\\Rat...   \n",
       "2  F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\rec\\Rat...   \n",
       "3  F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\rec\\Rat...   \n",
       "4  F:\\Hearts Melly\\Rat66\\2214_7.5um_Al1mm\\rec\\Rat...   \n",
       "\n",
       "                                       Folder Sample                  Scan  \\\n",
       "0  F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\rec  Rat63  2214_7.5um_Al1mm_rec   \n",
       "1  F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\rec  Rat63  2214_7.5um_Al1mm_rec   \n",
       "2  F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\rec  Rat63  2214_7.5um_Al1mm_rec   \n",
       "3  F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\rec  Rat63  2214_7.5um_Al1mm_rec   \n",
       "4  F:\\Hearts Melly\\Rat66\\2214_7.5um_Al1mm\\rec  Rat66  2214_7.5um_Al1mm_rec   \n",
       "\n",
       "  Experiment  Timepoint Gender      VOI  \\\n",
       "0          F        7.0      F  myocard   \n",
       "1          F        7.0      F    patch   \n",
       "2          F        7.0      F  myocard   \n",
       "3          F        7.0      F    patch   \n",
       "4        V+P       28.0      M  myocard   \n",
       "\n",
       "                                           VOIFolder  \\\n",
       "0  F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\voi_myo...   \n",
       "1   F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\voi_patch   \n",
       "2  F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\voi_myo...   \n",
       "3   F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\voi_patch   \n",
       "4  F:\\Hearts Melly\\Rat66\\2214_7.5um_Al1mm\\voi_myo...   \n",
       "\n",
       "                                            VOIFiles  Number of VOI slices  \\\n",
       "0  [F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\voi_my...                  1045   \n",
       "1  [F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\voi_pa...                  1045   \n",
       "2  [F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\voi_my...                  1045   \n",
       "3  [F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\voi_pa...                  1045   \n",
       "4  [F:\\Hearts Melly\\Rat66\\2214_7.5um_Al1mm\\voi_my...                  1041   \n",
       "\n",
       "   Voxelsize                                      OutputNameVOI  \\\n",
       "0   7.500482  F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\Rat63_2...   \n",
       "1   7.500482  F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\Rat63_2...   \n",
       "2   7.500482  F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\Rat63_2...   \n",
       "3   7.500482  F:\\Hearts Melly\\Rat63\\2214_7.5um_Al1mm\\Rat63_2...   \n",
       "4   7.500482  F:\\Hearts Melly\\Rat66\\2214_7.5um_Al1mm\\Rat66_2...   \n",
       "\n",
       "                 Size  \n",
       "0  (1045, 3072, 3072)  \n",
       "1  (1045, 3072, 3072)  \n",
       "2  (1045, 3072, 3072)  \n",
       "3  (1045, 3072, 3072)  \n",
       "4  (1041, 3072, 3072)  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828a28f98488449bb622cd7e4539d5b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MIPs:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rat63\\myocard:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rat63\\patch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rat63\\myocard:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rat63\\patch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rat66\\myocard:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rat66\\patch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rat66\\myocard:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rat66\\patch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rat82\\myocard:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rat82\\patch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rat82\\myocard:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rat82\\patch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read or calculate the directional MIPs, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['MIP_' + direction] = [None] * len(VOIs)\n",
    "for c, row in notebook.tqdm(Data.iterrows(), desc='MIPs', total=len(Data)):\n",
    "    for d, direction in notebook.tqdm(enumerate(directions),\n",
    "                                      desc=os.path.join(row['Sample'], row['VOI']),\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                                   '%s.%s.%s.MIP.%s.png' % (row['Sample'],\n",
    "                                                            row['Scan'],\n",
    "                                                            row['VOI'],\n",
    "                                                            direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c,'MIP_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate MIP\n",
    "            Data.at[c,'MIP_' + direction] = VOIs[c].max(axis=d).compute()\n",
    "            # Save it out\n",
    "            imageio.imwrite(outfilepath, Data.at[c,'MIP_' + direction].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show MIP slices\n",
    "for c, row in Data.iterrows():\n",
    "    outpath = os.path.join(os.path.dirname(row['Folder']), '%s.%s.MIPs.png' % (row['Sample'], row['VOI']))\n",
    "    if not os.path.exists(outpath):    \n",
    "        for d, direction in enumerate(directions):\n",
    "            plt.subplot(1, 3, d + 1)\n",
    "            plt.imshow(row['MIP_' + direction])\n",
    "            plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "            plt.title('%s, %s' % (os.path.join(row['Sample'], row['VOI']),\n",
    "                                  direction + ' MIP'))\n",
    "            plt.axis('off')\n",
    "        plt.savefig(outpath,\n",
    "                    bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mean` gray value needs to be calculated and 'calibrated' to the total volume of the ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask the outside of the ROI that Tim drew\n",
    "Masked = [dask.array.ma.masked_equal(v, 0) for v in VOIs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How large are the VOIs from Tim?\n",
    "# We select/mask everything non-zero and fill this whith one.\n",
    "VOIRegion = [dask.array.ma.filled(dask.array.ma.masked_not_equal(v, 0), 1) for v in VOIs]\n",
    "# By summing it, we get the volume\n",
    "# This takes a *long* time, let's think about it how we can improve the speed.\n",
    "Data['VOIVolume'] = [vr.sum().compute() for vr in VOIRegion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whichone = 3\n",
    "whichslice = 111\n",
    "plt.subplot(131)\n",
    "plt.imshow(Masked[whichone][whichslice])\n",
    "plt.title('Mask %s/%s' % (Data.Sample[whichone], Data.VOI[whichone]))\n",
    "plt.subplot(132)\n",
    "plt.imshow(VOIRegion[whichone][whichslice])\n",
    "plt.title('VOI %s/%s' % (Data.Sample[whichone], Data.VOI[whichone]))\n",
    "plt.subplot(133)\n",
    "plt.imshow(VOIs[whichone][whichslice])\n",
    "plt.title('Original %s/%s' % (Data.Sample[whichone], Data.VOI[whichone]))\n",
    "plt.savefig(os.path.join(OutputDir, 'VOI.%s.%s.png' % (Data.Sample[whichone], Data.VOI[whichone])),\n",
    "            transparent=False,\n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[Data.VOI == 'patch'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[Data.VOI == 'myocard'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, exp in enumerate(Data.Experiment.unique()):\n",
    "    print(c, exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot volume of VOIs for comparison\n",
    "for c, which in enumerate(['patch', 'myocard']):\n",
    "    plt.subplot(1,2,c+1)\n",
    "    seaborn.swarmplot(data=Data[Data.VOI == which],\n",
    "                  x='Experiment',\n",
    "                  y='VOIVolume',\n",
    "                  dodge=True,\n",
    "                  hue='Timepoint',\n",
    "                  s=10,\n",
    "                  linewidth=1.5)\n",
    "    for c, row in Data[Data.VOI == which].iterrows():      \n",
    "        if 'V+P' in row.Experiment:\n",
    "            plt.annotate(row.Sample, (0, row.VOIVolume))\n",
    "        elif 'F' in row.Experiment:\n",
    "            plt.annotate(row.Sample, (1, row.VOIVolume))\n",
    "        elif 'tacho' in row.Experiment:\n",
    "            plt.annotate(row.Sample, (2, row.VOIVolume))\n",
    "        elif row.Experiment == 'V':\n",
    "            plt.annotate(row.Sample, (3, row.VOIVolume))\n",
    "    plt.ylim([0, 4e8])\n",
    "    plt.title('Volume of the individual %s VOIs' % which)\n",
    "plt.savefig(os.path.join(OutputDir, 'Volume.VOIs.png'),\n",
    "            transparent=False,\n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot volume of VOIs for comparison\n",
    "for c,which in enumerate(['patch', 'myocard']):\n",
    "    seaborn.swarmplot(data=Data[Data.VOI == which],\n",
    "                  x='Experiment',\n",
    "                  y='VOIVolume',\n",
    "                  dodge=True,\n",
    "                  hue='Timepoint',\n",
    "                  s=10,\n",
    "                  linewidth=1.5)\n",
    "    for c,row in Data[Data.VOI == which].iterrows():\n",
    "        if 'V+P' in row.Experiment:\n",
    "            plt.annotate(row.Sample, (0, row.VOIVolume))\n",
    "        elif 'F' in row.Experiment:\n",
    "            plt.annotate(row.Sample, (1, row.VOIVolume))\n",
    "        elif 'tacho' in row.Experiment:\n",
    "            plt.annotate(row.Sample, (2, row.VOIVolume))\n",
    "        elif row.Experiment == 'V':\n",
    "            plt.annotate(row.Sample, (3, row.VOIVolume))\n",
    "    plt.ylim([0, 4e8])\n",
    "    plt.title('Volume of the %s VOIs' % which)\n",
    "    plt.savefig(os.path.join(OutputDir, 'Volume.VOIs.%s.png' % which),\n",
    "                transparent=False,\n",
    "                bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Sample',\n",
    "      'VOI',\n",
    "      'VOIVolume']].to_excel(os.path.join(OutputDir, 'Volume.VOIs.xls'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data.groupby(by=[['Experiment', 'VOI']])['VOIVolume'].describe()[['count',\n",
    "#                                                                   'mean',\n",
    "#                                                                   'std',\n",
    "#                                                                   'min',\n",
    "#                                                                   'max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save mean of reconstruction gray values,\n",
    "# We can use this for getting an overview of the image data\n",
    "Data['GrayValueMean'] = [m.mean().compute() for m  in Masked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean gray value VOIs for comparison\n",
    "for c, which in enumerate(['patch', 'myocard']):\n",
    "    plt.subplot(1,2,c+1)\n",
    "    seaborn.swarmplot(data=Data[Data.VOI == which],\n",
    "                      x='Experiment',\n",
    "                      y='GrayValueMean',\n",
    "                      hue='Timepoint',\n",
    "                      s=10,\n",
    "                      linewidth=1.5,\n",
    "                      dodge=True)\n",
    "    for c,row in Data[Data.VOI == which].iterrows():\n",
    "        if 'V+P' in row.Experiment:\n",
    "            plt.annotate(row.Sample, (0, row.GrayValueMean))\n",
    "        elif 'F' in row.Experiment:\n",
    "            plt.annotate(row.Sample, (1, row.GrayValueMean))\n",
    "        elif 'tacho' in row.Experiment:\n",
    "            plt.annotate(row.Sample, (2, row.GrayValueMean))\n",
    "        elif row.Experiment == 'V':\n",
    "            plt.annotate(row.Sample, (3, row.GrayValueMean))\n",
    "    plt.title('Average grayvalue in the %s VOIs' % which)\n",
    "plt.savefig(os.path.join(OutputDir, 'Grayvalues.Mean.VOIs.png'),\n",
    "            transparent=False,           \n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Sample', 'GrayValueMean']].to_excel(os.path.join(OutputDir, 'Grayvalues.Mean.VOIs.xls'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data.groupby(by=['Experiment'])['GrayValueMean'].describe()[['count',\n",
    "#                                                              'mean',\n",
    "#                                                              'std',\n",
    "#                                                              'min',\n",
    "#                                                              'max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdfasdfasdf=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['GrayValueMeanNormalizedToVOIVolume'] = [numpy.divide(gvm,\n",
    "                                                           vv) for gvm, vv in zip(Data['GrayValueMean'],\n",
    "                                                                                  Data['VOIVolume'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot volume-normalized mean of datasets for comparison\n",
    "#seaborn.boxplot(data=Data, x='Experiment', y='GrayValueMeanNormalizedToVOIVolume', hue='Timepoint')\n",
    "seaborn.swarmplot(data=Data, x='Experiment', y='GrayValueMeanNormalizedToVOIVolume', hue='Timepoint', dodge=True, linewidth=1.5, s=10)\n",
    "for c,row in Data.iterrows():\n",
    "    if 'VP' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (0, row.GrayValueMeanNormalizedToVOIVolume))\n",
    "    elif 'F' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (1, row.GrayValueMeanNormalizedToVOIVolume))\n",
    "    elif 'Tacho' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (2, row.GrayValueMeanNormalizedToVOIVolume))\n",
    "plt.ylim(ymin=0, ymax=1.1*Data.GrayValueMeanNormalizedToVOIVolume.max())\n",
    "plt.title('Averaged VOI grayvalue normalized to VOI volume')\n",
    "plt.savefig(os.path.join(OutputDir, 'Grayvalues.Mean.NormalizedVOI.png'),\n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutputDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save STD of reconstruction gray values, which we can use for getting an overview of the image data\n",
    "# Data['GrayValueSTD'] = [m.std().compute() for m in Masked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Plot STD of datasets for comparison\n",
    "# seaborn.catplot(data=Data, kind='box', x='Experiment', y='GrayValueSTD')\n",
    "# # seaborn.swarmplot(data=Data, x='Experiment', y='GrayValueSTD', linewidth=1.5, s=10, color='gray')\n",
    "# # plt.ylim(ymin=0)\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_threshold(img, verbose=False):\n",
    "#     '''\n",
    "#     Calculate (Otsu) threshold of a stack, with some nice output if desired\n",
    "#     '''\n",
    "#     if len(numpy.shape(img)) != 3:\n",
    "#         print('Only works with a 3D stack')\n",
    "#         return()\n",
    "#     if verbose:\n",
    "#         print('The stack we use has a size of %s x %s x %s px' % numpy.shape(img))\n",
    "#     threshold = skimage.filters.threshold_otsu(dask.array.ravel(img.compute()))\n",
    "#     if verbose:\n",
    "#         seaborn.distplot(img.ravel())\n",
    "#         plt.axvline(threshold, label='Otsu@%s' % threshold, c=seaborn.color_palette()[1])\n",
    "#         plt.axvline(numpy.mean(img), label='Image mean@%0.2f' % img.mean(), c=seaborn.color_palette()[2])\n",
    "#         plt.legend()\n",
    "#         plt.semilogy()\n",
    "#         plt.xlim([0,255])\n",
    "#         plt.show()\n",
    "#     return(threshold.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://stackoverflow.com/a/38086839\n",
    "# h,bins=dask.array.histogram(VOIs[0], bins=range(0,255,4))\n",
    "# plt.semilogy(h)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thresholds\n",
    "preset = False\n",
    "if preset:\n",
    "    # Set them (from previous calculations)\n",
    "    Data['Threshold'] = [44, 46, 41, 16, 12, 15, 63, 16, 15, 18, 13, 13]\n",
    "#     Data['Threshold'] = [41, 45, 41, 12, 13, 19, 13]\n",
    "else:\n",
    "    # Calculate Threshold\n",
    "    Data['Threshold'] = [skimage.filters.threshold_otsu(\n",
    "        dask.array.ravel(\n",
    "            dask.array.ma.masked_less(\n",
    "                rec, 1).compute())) for rec in VOIs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(Data.Threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean threshold of all samples\n",
    "Data['ThresholdMean'] = int(Data['Threshold'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the thresholds\n",
    "seaborn.swarmplot(data=Data, x='Experiment', y='Threshold', hue='Timepoint', dodge=True, linewidth=1.5, s=10)\n",
    "for c,row in Data.iterrows():\n",
    "    if 'VP' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (0, row.Threshold))\n",
    "    elif 'F' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (1, row.Threshold))\n",
    "    elif 'Tacho' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (2, row.Threshold))\n",
    "plt.axhline(Data['ThresholdMean'].mean(), label='Mean threshold @ %s' % Data['ThresholdMean'].mean())\n",
    "plt.ylim(ymin=0)\n",
    "plt.legend()\n",
    "plt.title('Otsu thresholds of individual VOIs')\n",
    "plt.savefig(os.path.join(OutputDir, 'Thresholds.png'),\n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Sample', 'Threshold']].to_excel(os.path.join(OutputDir, 'Thresholds.xls'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.groupby(by=['Experiment'])['Threshold'].describe()[['count',\n",
    "                                                         'mean',\n",
    "                                                         'std',\n",
    "                                                         'min',\n",
    "                                                         'max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold the reconstructions individually                                                                                                                           Data.Sample)\n",
    "Data['OutputNameThresholded'] = [f.replace('.zarr',\n",
    "                                           '_thresholded_%s.zarr' % str(t).zfill(3)) for f, t in zip(Data['OutputNameVOI'],\n",
    "                                                                                                     Data['Threshold'])]\n",
    "for c, row in Data.iterrows():\n",
    "    if os.path.exists(row['OutputNameThresholded']):  \n",
    "        print('%2s/%s: Already saved to %s' % (c + 1,\n",
    "                                               len(Data),\n",
    "                                               row['OutputNameThresholded'][len(Root):]))\n",
    "    else:\n",
    "        print('%2s/%s: Thresholding and saving to %s' % (c + 1,\n",
    "                                                         len(Data),\n",
    "                                                         row['OutputNameThresholded'][len(Root):]))\n",
    "        Thresholded = VOIs[c] > row['Threshold']\n",
    "        Thresholded.to_zarr(row['OutputNameThresholded'],\n",
    "                           overwrite=True,\n",
    "                           compressor=Blosc(cname='zstd', clevel=3, shuffle=Blosc.BITSHUFFLE))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold the reconstructions with the mean threshold                                                                                                                           Data.Sample)\n",
    "Data['OutputNameThresholdedMean'] = [f.replace('.zarr',\n",
    "                                           '_thresholded_%s.zarr' % str(t).zfill(3)) for f, t in zip(Data['OutputNameVOI'],\n",
    "                                                                                                     Data['ThresholdMean'])]\n",
    "for c, row in Data.iterrows():\n",
    "    if os.path.exists(row['OutputNameThresholdedMean']):  \n",
    "        print('%2s/%s: Already saved to %s' % (c + 1,\n",
    "                                               len(Data),\n",
    "                                               row['OutputNameThresholdedMean'][len(Root):]))\n",
    "    else:\n",
    "        print('%2s/%s: Thresholding and saving to %s' % (c + 1,\n",
    "                                                         len(Data),\n",
    "                                                         row['OutputNameThresholded'][len(Root):]))\n",
    "        Thresholded = VOIs[c] > row['ThresholdMean']\n",
    "        Thresholded.to_zarr(row['OutputNameThresholdedMean'],\n",
    "                           overwrite=True,\n",
    "                           compressor=Blosc(cname='zstd', clevel=3, shuffle=Blosc.BITSHUFFLE))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DASK arrays of the thresholded samples\n",
    "individualThreshold = True\n",
    "if individualThreshold:\n",
    "    Thresholded = [dask.array.from_zarr(file) for file in Data['OutputNameThresholded']]\n",
    "    print('Loading individually thresholded stacks')\n",
    "else:\n",
    "    Thresholded = [dask.array.from_zarr(file) for file in Data['OutputNameThresholdedMean']]\n",
    "    print('Loading all stacks with a threshold of %s' % Data.ThresholdMean.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DASK\n",
    "# Read or calculate the middle slices of the thresholded images,\n",
    "# put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Thresholded_Mid_' + direction] = [None] * len(VOIs)\n",
    "for c, row in tqdm.notebook.tqdm(Data.iterrows(),\n",
    "                                 desc='Middle thresholded images',\n",
    "                                 total=len(Data)):\n",
    "    for d, direction in tqdm.notebook.tqdm(enumerate(directions),\n",
    "                                           desc=row['Sample'],\n",
    "                                           leave=False,\n",
    "                                           total=len(directions)):\n",
    "        outfilepath = os.path.join(row['Folder'],\n",
    "                                   '%s.Thresholded%03d.Middle.%s.png' % (row['Sample'],\n",
    "                                                                         row['Threshold'],\n",
    "                                                                         direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c,'Thresholded_Mid_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate requested axial view\n",
    "            if 'Axial' in direction:\n",
    "                Data.at[c,'Thresholded_Mid_' + direction] = Thresholded[c][Data['Size'][c][0]//2]\n",
    "            if 'Sagittal' in direction:\n",
    "                Data.at[c,'Thresholded_Mid_' + direction] = Thresholded[c][:,Data['Size'][c][1]//2,:]\n",
    "            if 'Coronal' in direction:\n",
    "                Data.at[c,'Thresholded_Mid_' + direction] = Thresholded[c][:,:,Data['Size'][c][2]//2]\n",
    "            # Save the calculated 'direction' view out\n",
    "            # Dask only calculates/reads the images here at this point...\n",
    "            imageio.imwrite(outfilepath, (Data.at[c,'Thresholded_Mid_' + direction].astype('uint8')*255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for d, direction in enumerate(directions):\n",
    "#     for c,row in Data.iterrows():\n",
    "#         plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)\n",
    "#         plt.imshow(row['Thresholded_Mid_' + direction])\n",
    "#         plt.title('%s\\nMid-%s\\nthresholded slice' % (row['Sample'], direction))\n",
    "#         plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))    \n",
    "#         plt.axis('off')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read or calculate the directional MIPs of the thresholded datasets,\n",
    "# put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Thresholded_MIP_' + direction] = [None] * len(VOIs)\n",
    "for c, row in notebook.tqdm(Data.iterrows(), desc='MIPs', total=len(Data)):\n",
    "    for d, direction in notebook.tqdm(enumerate(directions),\n",
    "                                      desc=row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        outfilepath = os.path.join(row['Folder'], '%s.Thresholded.MIP.%s.png' % (row['Sample'], direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c,'Thresholded_MIP_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate MIP\n",
    "            Data.at[c,'Thresholded_MIP_' + direction] = Thresholded[c].max(axis=d).compute()\n",
    "            # Save it out\n",
    "            imageio.imwrite(outfilepath, Data.at[c,'Thresholded_MIP_' + direction].astype('uint8'))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show thresholded MIP slices\n",
    "for c, row in Data.iterrows():\n",
    "    for d, direction in enumerate(directions):\n",
    "        plt.subplot(1, 3, d + 1)\n",
    "        plt.imshow(row['Thresholded_MIP_' + direction])\n",
    "        plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "#         plt.title('%s/%s: %s, %s' % (c + 1,\n",
    "#                                      len(Data),\n",
    "#                                      row['Sample'],\n",
    "#                                      direction + ' MIP'))\n",
    "        plt.title('%s, %s' % (row['Sample'], direction + ' MIP'))\n",
    "        plt.axis('off')\n",
    "    plt.savefig(os.path.join(row['Folder'], row['Sample'] + '.Thresholded.MIPs.png'),\n",
    "                bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the images, so we can see if they contain approximately the same *thresholded* volume\n",
    "Data['ThresholdedVolume'] = [th.sum().compute() for th in Thresholded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data[['Sample', 'ThresholdedVolume']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the thresholded volumes\n",
    "seaborn.swarmplot(data=Data, x='Experiment', y='ThresholdedVolume', hue='Timepoint', dodge=True, linewidth=1.5, s=10)\n",
    "for c,row in Data.iterrows():\n",
    "    if 'VP' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (0, row.ThresholdedVolume))\n",
    "    elif 'F' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (1, row.ThresholdedVolume))\n",
    "    elif 'Tacho' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (2, row.ThresholdedVolume))\n",
    "plt.ylim(ymin=0)\n",
    "if individualThreshold:\n",
    "    plt.title('Volume of the individually thresholded images, corresponding to the total thresholded volume')\n",
    "    plt.savefig(os.path.join(OutputDir, 'Volumes.Thresholded.ThresholdedIndividually.png'),\n",
    "                bbox_inches='tight')    \n",
    "else:\n",
    "    plt.title('Volume of the images thresholded all equally, corresponding to the total thresholded volume')    \n",
    "    plt.savefig(os.path.join(OutputDir, 'Volumes.Thresholded.ThresholdedEqually.png'),\n",
    "                bbox_inches='tight')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutputDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data[['Sample', 'Experiment', 'Threshold', 'ThresholdedVolume', 'GrayValueMean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['GrayValueMeanNormalizedToThresholdedVolume'] = [numpy.divide(gvm,\n",
    "                                                                   tv) for gvm, tv in zip(Data['GrayValueMean'],\n",
    "                                                                                          Data['ThresholdedVolume'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view(Thresholded[3].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot volume-normalized mean of datasets for comparison\n",
    "seaborn.swarmplot(data=Data, x='Experiment', y='GrayValueMeanNormalizedToThresholdedVolume', hue='Timepoint', dodge=True, linewidth=1.5, s=10)\n",
    "for c,row in Data.iterrows():\n",
    "    if 'VP' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (0, row.GrayValueMeanNormalizedToThresholdedVolume))\n",
    "    elif 'F' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (1, row.GrayValueMeanNormalizedToThresholdedVolume))\n",
    "    elif 'Tacho' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (2, row.GrayValueMeanNormalizedToThresholdedVolume))\n",
    "plt.ylim(ymin=0, ymax=Data.GrayValueMeanNormalizedToThresholdedVolume.max())\n",
    "if individualThreshold:\n",
    "    plt.title('Average grayvalue normalized to the thresholded volume')\n",
    "    plt.savefig(os.path.join(OutputDir, 'Grayvalues.Mean.NormalizedToThresholded.ThresholdedIndividually.png'),\n",
    "            bbox_inches='tight')   \n",
    "else:\n",
    "    plt.title('Average grayvalue normalized to the volume thresholded all with the same threshold')        \n",
    "    plt.savefig(os.path.join(OutputDir, 'Grayvalues.Mean.NormalizedToThresholded.ThresholdedEqually.png'),\n",
    "                bbox_inches='tight')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot volume-normalized mean of datasets for comparison\n",
    "# seaborn.swarmplot(data=Data, x='Experiment', y='GrayValueMeanNormalizedToThresholdedVolume', hue='Timepoint', dodge=True, linewidth=1.5, s=10)\n",
    "# for c,row in Data.iterrows():\n",
    "#     if 'VP' in row.Experiment:\n",
    "#         plt.annotate(row.Sample, (0, row.GrayValueMeanNormalizedToThresholdedVolume))\n",
    "#     elif 'F' in row.Experiment:\n",
    "#         plt.annotate(row.Sample, (1, row.GrayValueMeanNormalizedToThresholdedVolume))\n",
    "#     elif 'Tacho' in row.Experiment:\n",
    "#         plt.annotate(row.Sample, (2, row.GrayValueMeanNormalizedToThresholdedVolume))\n",
    "# plt.ylim(ymin=0, ymax=0.000003)\n",
    "# plt.savefig(os.path.join(OutputDir, 'Grayvalues.Mean.NormalizedToThresholded.Without66.png'),\n",
    "#             bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view(Thresholded[0].compute().astype('uint8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characterization of vessel diameter\n",
    "- Fill the vessels/ventricle (with something like `skimage.fill.small.holes`)\n",
    "    This doesn't seem to be working in the 3D case (maybe because of small holes) but we just loop through every slice and do it for each and everyone of it. This is bad code, but works :)\n",
    "- Remove all the big stuff with `tophat`\n",
    "- Calculate the distance-transformation or skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ourfiller(image, verbose=False):\n",
    "    '''\n",
    "    Since we cannot seem to get remove_small_holes to work in 3D, we simply brute-force it on every slice.\n",
    "    Thanks to `tqdm_notebook` we also get a progress bar...\n",
    "    And afterwards generate an output array.\n",
    "    '''\n",
    "    filled = [skimage.morphology.remove_small_holes(s, area_threshold=1e4) for\n",
    "              s in tqdm.notebook.tqdm(image, leave=False)]\n",
    "    if verbose:\n",
    "        plt.subplot(131)\n",
    "        plt.imshow(image[len(filled)//2,:,:])\n",
    "        plt.title('Original')\n",
    "        plt.subplot(132)\n",
    "        plt.imshow(filled[len(filled)//2,:,:])       \n",
    "        plt.title('Filled (output)')\n",
    "        plt.subplot(133)        \n",
    "        plt.imshow(image[len(filled)//2,:,:], alpha=0.5)       \n",
    "        plt.imshow(filled[len(filled)//2,:,:], cmap='viridis', alpha=0.5)       \n",
    "        plt.title('Overlay')        \n",
    "        plt.show()    \n",
    "    return(filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pad_edges(image, howmanypixels=25):\n",
    "#     '''There *has* to be a better way to set the edges to one, but I havent found one'''\n",
    "#     closed = image.copy()\n",
    "#     closed[:howmanypixels,:,:] = True\n",
    "#     closed[:,:howmanypixels,:] = True\n",
    "#     closed[:,:,:howmanypixels] = True\n",
    "#     closed[-howmanypixels:,:,:] = True\n",
    "#     closed[:,-howmanypixels:,:] = True\n",
    "#     closed[:,:,-howmanypixels:] = True\n",
    "#     return(closed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fill_hollow_bones(image, verbose=False):\n",
    "#     '''\n",
    "#     We flood-fill the image from one edge.\n",
    "#     Then we add the inversion of this to the original image and thus filled all the long bones.\n",
    "#     '''\n",
    "#     dilated = skimage.morphology.binary_dilation(\n",
    "#         skimage.morphology.binary_dilation(\n",
    "#             skimage.morphology.binary_dilation(image)))\n",
    "#     closed = pad_edges(dilated)\n",
    "#     # Flood fill from one edge and invert the (boolean) result\n",
    "#     flooded = ~skimage.morphology.flood_fill(closed.astype('int'),\n",
    "#                                              seed_point=(30,30,30),\n",
    "#                                              new_value=1).astype('bool')\n",
    "#     # Add the inverted result to the original image, filling the long bones\n",
    "#     filled = numpy.add(image, skimage.morphology.binary_dilation(\n",
    "#         skimage.morphology.binary_dilation(\n",
    "#             skimage.morphology.binary_dilation(flooded))))\n",
    "#     if verbose:\n",
    "#         plt.subplot(131)\n",
    "#         plt.imshow(image[len(filled)//2,:,:])\n",
    "#         plt.title('original')\n",
    "#         plt.subplot(132)\n",
    "#         plt.imshow(flooded[len(filled)//2,:,:])       \n",
    "#         plt.title('flooded')\n",
    "#         plt.subplot(133)        \n",
    "#         plt.imshow(image[len(filled)//2,:,:], alpha=0.5)       \n",
    "#         plt.imshow(filled[len(filled)//2,:,:], cmap='viridis', alpha=0.5)       \n",
    "#         plt.title('filled (output)')        \n",
    "#         plt.show()\n",
    "#     return(filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the flood-filled image\n",
    "# Since this takes a while, we don't do it in a Pythonic way\n",
    "# e.g. (Flooded = [ourfiller(t, verbose=True) for t in Tresholded])\n",
    "# but in a loop with saving in between.\n",
    "Data['OutputNameFlooded'] = [f.replace('.zarr', '_flooded.zarr') for f in Data['OutputNameThresholded']]\n",
    "for c, row in Data.iterrows():\n",
    "    if os.path.exists(row['OutputNameFlooded']):  \n",
    "        print('%2s/%s: Already saved to %s' % (c + 1,\n",
    "                                               len(Data),\n",
    "                                               row['OutputNameFlooded'][len(Root):]))\n",
    "    else:\n",
    "        print('%2s/%s: %s: Filling holes' % (c + 1,\n",
    "                                             len(Data),\n",
    "                                             row['Sample'].rjust(Data['SampleNameLength'].max())))\n",
    "        Flooded = ourfiller(Thresholded[c].compute())\n",
    "        Flooded = da.stack(Flooded[:])\n",
    "        print('%11s: Saving to %s' % (row['Sample'].rjust(Data['SampleNameLength'].max()),\n",
    "                                     row['OutputNameFlooded'][len(Root):]))\n",
    "        Flooded.to_zarr(row['OutputNameFlooded'],\n",
    "                        overwrite=True,\n",
    "                        compressor=Blosc(cname='zstd', clevel=3, shuffle=Blosc.BITSHUFFLE))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DASK arrays with the filled samples\n",
    "Flooded = [dask.array.from_zarr(file) for file in Data['OutputNameFlooded']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DASK\n",
    "# Read or calculate the middle slices of the flooded images,\n",
    "# put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Flooded_Mid_' + direction] = [None] * len(VOIs)\n",
    "for c, row in tqdm.notebook.tqdm(Data.iterrows(),\n",
    "                                 desc='Middle flooded images',\n",
    "                                 total=len(Data)):\n",
    "    for d, direction in tqdm.notebook.tqdm(enumerate(directions),\n",
    "                                           desc=row['Sample'],\n",
    "                                           leave=False,\n",
    "                                           total=len(directions)):\n",
    "        outfilepath = os.path.join(row['Folder'],\n",
    "                                   '%s.Thresholded%03d.Flooded.Middle.%s.png' % (row['Sample'],\n",
    "                                                                                 row['Threshold'],\n",
    "                                                                                 direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c,'Flooded_Mid_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate requested axial view\n",
    "            if 'Axial' in direction:\n",
    "                Data.at[c,'Flooded_Mid_' + direction] = Flooded[c][Data['Size'][c][0]//2]\n",
    "            if 'Sagittal' in direction:\n",
    "                Data.at[c,'Flooded_Mid_' + direction] = Flooded[c][:,Data['Size'][c][1]//2,:]\n",
    "            if 'Coronal' in direction:\n",
    "                Data.at[c,'Flooded_Mid_' + direction] = Flooded[c][:,:,Data['Size'][c][2]//2]\n",
    "            # Save the calculated 'direction' view out\n",
    "            # Dask only calculates/reads the images here at this point...\n",
    "            imageio.imwrite(outfilepath,\n",
    "                            (Data.at[c,'Flooded_Mid_' + direction].astype('uint8')*255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show middle flood-filled images\n",
    "for c, row in Data.iterrows():\n",
    "    for d, direction in enumerate(directions):\n",
    "        plt.subplot(1, 3, d + 1)\n",
    "        plt.imshow(row['Flooded_Mid_' + direction])\n",
    "        plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "#         plt.title('%s/%s: %s, %s' % (c + 1,\n",
    "#                                      len(Data),\n",
    "#                                      row['Sample'],\n",
    "#                                      direction + ' MIP'))\n",
    "        plt.title('%s: %s' % (row['Sample'],\n",
    "                              direction + ' MIP'))\n",
    "        \n",
    "        plt.axis('off')\n",
    "    plt.savefig(os.path.join(row['Folder'], row['Sample'] + '.Flooded.MiddleSlices.png'),\n",
    "                bbox_inches='tight')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show middle flood-filled images with overlay\n",
    "for c, row in Data.iterrows():\n",
    "    for d, direction in enumerate(directions):\n",
    "        plt.subplot(1, 3, d + 1)\n",
    "        plt.imshow(row['Flooded_Mid_' + direction])\n",
    "        plt.imshow(dask.array.ma.masked_less(row['Thresholded_Mid_' + direction], 1), alpha=0.5, cmap='viridis')        \n",
    "        plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "#         plt.title('%s/%s: %s, %s' % (c + 1,\n",
    "#                                      len(Data),\n",
    "#                                      row['Sample'],\n",
    "#                                      direction + ' MIP'))\n",
    "        plt.title('%s: %s' % (row['Sample'],\n",
    "                              direction + ' MIP'))\n",
    "        \n",
    "        plt.axis('off')\n",
    "    plt.savefig(os.path.join(row['Folder'], row['Sample'] + '.Flooded.Overlay.MiddleSlices.png'),\n",
    "                bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(Data['Thresholded_Mid_Axial'][0])\n",
    "# plt.imshow(dask.array.ma.masked_less(Data['Flooded_Mid_Axial'][0], 1), alpha=0.5, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show middle axis flooded images\n",
    "for d,direction in enumerate(directions):\n",
    "    for c,row in Data.iterrows():\n",
    "        plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)\n",
    "        plt.imshow(row['Flooded_Mid_' + direction])\n",
    "        plt.imshow(dask.array.ma.masked_less(row['Thresholded_Mid_' + direction], 1), alpha=0.5, cmap='viridis')\n",
    "        plt.title('Middle %s slice of tophat of\\n%s together with original' % (direction, row['Sample']))\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][c], 'um'))    \n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.subplot(121)\n",
    "# plt.imshow(Data['Thresholded_Mid_' + direction][1])\n",
    "# plt.subplot(122)\n",
    "# plt.imshow(Data['Flooded_Mid_' + direction][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flooded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reconstructions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the white tophat\n",
    "# https://scikit-image.org/docs/dev/api/skimage.morphology.html#skimage.morphology.white_tophat\n",
    "# e.g. the bright spots of the image that are smaller than the structuring element.\n",
    "# We use a ball-shaped (sphere) structuring element\n",
    "# Again, since this takes a *long* while, we don't do it nice and pythonic,\n",
    "# but in a loop with intermediate saving\n",
    "# e.g. not (Tophat = [skimage.morphology.white_tophat(f, selem=skimage.morphology.ball(7)) for f in Flooded])\n",
    "# but\n",
    "tophatselem = 5\n",
    "Data['OutputNameTophat'] = [f.replace('.zarr', '_tophat_%s.zarr' % tophatselem) for f in Data['OutputNameFlooded']]\n",
    "Tophat = [numpy.nan for file in Data['OutputNameTophat']]\n",
    "for c, row in Data.iterrows():\n",
    "    if os.path.exists(row['OutputNameTophat']):\n",
    "        print('%2s/%s: Already saved to %s' % (c + 1,\n",
    "                                               len(Data),\n",
    "                                               row['OutputNameTophat'][len(Root):]))\n",
    "    else:\n",
    "        print('%2s/%s: %s: Calculating white thophat with a \"selem\" of %s' % (c + 1,\n",
    "                                                                              len(Data),\n",
    "                                                                              row['Sample'].rjust(Data['SampleNameLength'].max()),\n",
    "                                                                              tophatselem))\n",
    "        Tophat = skimage.morphology.white_tophat(Flooded[c].compute(),\n",
    "                                                 selem=skimage.morphology.ball(tophatselem))\n",
    "        Tophat = da.stack(Tophat[:])        \n",
    "        print('%11s: Saving to %s' % (row['Sample'].rjust(Data['SampleNameLength'].max()),\n",
    "                                      row['OutputNameTophat'][len(Root):]))\n",
    "        Tophat.to_zarr(row['OutputNameTophat'],\n",
    "                       overwrite=True,\n",
    "                       compressor=Blosc(cname='zstd', clevel=3, shuffle=Blosc.BITSHUFFLE))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img = Thresholded[1][1250:-1350,100:-100,100:-100].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D\n",
    "#tophat = skimage.morphology.white_tophat(a, selem=skimage.morphology.disk(33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D\n",
    "#plt.subplot(131)\n",
    "#plt.imshow(a)\n",
    "#plt.subplot(132)\n",
    "#plt.imshow(tophat)\n",
    "#plt.subplot(133)\n",
    "#plt.imshow(numpy.bitwise_xor(a,\n",
    "#                             tophat), alpha=0.5)\n",
    "#plt.imshow(tophat, cmap='viridis', alpha=0.5)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 3D topat\n",
    "#tophat = skimage.morphology.white_tophat(img, selem=skimage.morphology.ball(11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whichslice = numpy.shape(img)[0]//2\n",
    "#plt.subplot(131)\n",
    "#plt.imshow(img[whichslice])\n",
    "#plt.subplot(132)\n",
    "#plt.imshow(tophat[whichslice])\n",
    "#plt.subplot(133)\n",
    "#plt.imshow(numpy.bitwise_xor(img[whichslice],\n",
    "#                             tophat[whichslice]), alpha=0.5)\n",
    "#plt.imshow(tophat[whichslice], cmap='viridis', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DASK arrays with the tophat-filtered samples (e.g. only containing the smaller vessels)\n",
    "Tophat = [dask.array.from_zarr(file) for file in Data['OutputNameTophat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DASK\n",
    "# Read or calculate the middle slices of the Tophat images,\n",
    "# put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Tophat_Mid_' + direction] = [None] * len(VOIs)\n",
    "for c, row in tqdm.notebook.tqdm(Data.iterrows(), desc='Middle tophat images', total=len(Data)):\n",
    "    for d, direction in tqdm.notebook.tqdm(enumerate(directions),\n",
    "                                      desc=row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        outfilepath = os.path.join(row['Folder'],\n",
    "                                   '%s.Thresholded%03d.Tophat.Middle.%s.png' % (row['Sample'],\n",
    "                                                                                row['Threshold'],\n",
    "                                                                                 direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c,'Tophat_Mid_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate requested axial view\n",
    "            if 'Axial' in direction:\n",
    "                Data.at[c,'Tophat_Mid_' + direction] = Tophat[c][Data['Size'][c][0]//2]\n",
    "            if 'Sagittal' in direction:\n",
    "                Data.at[c,'Tophat_Mid_' + direction] = Tophat[c][:,Data['Size'][c][1]//2,:]\n",
    "            if 'Coronal' in direction:\n",
    "                Data.at[c,'Tophat_Mid_' + direction] = Tophat[c][:,:,Data['Size'][c][2]//2]\n",
    "            # Save the calculated 'direction' view out\n",
    "            # Dask only calculates/reads the images here at this point...\n",
    "            imageio.imwrite(outfilepath, (Data.at[c,'Tophat_Mid_' + direction].astype('uint8')*255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show middle flood-filled images with overlay\n",
    "for c, row in Data.iterrows():\n",
    "    for d, direction in enumerate(directions):\n",
    "        plt.subplot(1, 3, d + 1)\n",
    "        plt.imshow(row['Tophat_Mid_' + direction])\n",
    "        plt.imshow(dask.array.ma.masked_less(row['Thresholded_Mid_' + direction], 1), alpha=0.5, cmap='viridis')        \n",
    "        plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "#         plt.title('%s/%s: %s, %s' % (c + 1,\n",
    "#                                      len(Data),\n",
    "#                                      row['Sample'],\n",
    "#                                      direction + ' MIP'))\n",
    "        plt.title('%s: %s' % (row['Sample'],\n",
    "                              direction + ' MIP'))\n",
    "        \n",
    "        plt.axis('off')\n",
    "    plt.savefig(os.path.join(row['Folder'], row['Sample'] + '.Tophat.Overlay.MiddleSlices.png'),\n",
    "                bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show middle slices of tophat data\n",
    "for d,direction in enumerate(directions):\n",
    "    for c,row in Data.iterrows():\n",
    "        plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)\n",
    "        plt.imshow(row['Thresholded_Mid_' + direction])\n",
    "        plt.imshow(dask.array.ma.masked_less(row['Tophat_Mid_' + direction], 1), alpha=0.5, cmap='viridis')\n",
    "        plt.title('Middle %s slice of tophat of\\n%s together with original' % (direction, row['Sample']))\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][c], 'um'))    \n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show overlay\n",
    "# for c, direction in enumerate(directions):\n",
    "#     print(c, direction)    \n",
    "#     if c:\n",
    "#         plt.imshow(numpy.rot90(center, axes=(0,c))[len(flooded)//2,:,:], alpha=0.33)\n",
    "#         plt.imshow(numpy.rot90(flooded, axes=(0,c))[len(flooded)//2,:,:], alpha=0.33, cmap='magma')\n",
    "#         plt.imshow(numpy.rot90(tophat, axes=(0,c))[len(flooded)//2,:,:], alpha=0.33, cmap='viridis')\n",
    "#     else:\n",
    "#         plt.imshow(center[len(flooded)//2,:,:], alpha=0.33)\n",
    "#         plt.imshow(flooded[len(flooded)//2,:,:], alpha=0.33, cmap='magma')\n",
    "#         plt.imshow(tophat[len(flooded)//2,:,:], alpha=0.33, cmap='viridis')\n",
    "#     plt.gca().add_artist(ScaleBar(Data['Voxelsize'][0], 'um'))\n",
    "#     plt.axis('off')\n",
    "#     plt.savefig('Overlay-%s.png' % direction, bbox_inches='tight')  \n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the skeletonization\n",
    "# By multiplying them later on we get a color-coded medial axis transformation\n",
    "# This conforms to what we would expect from 'skimage.morphology.medial_axis(image, return_distance=True)' which does *not* work for 3D images\n",
    "Data['OutputNameSkeleton'] = [f.replace('.zarr', '_skeleton.zarr') for f in Data['OutputNameTophat']]\n",
    "for c, row in Data.iterrows():\n",
    "    if os.path.exists(row['OutputNameSkeleton']):  \n",
    "        print('%2s/%s: Already saved to %s' % (c + 1,\n",
    "                                               len(Data),\n",
    "                                               row['OutputNameSkeleton'][len(Root):]))\n",
    "    else:\n",
    "        print('%2s/%s: %s: Calculating skeletonization' % (c + 1,\n",
    "                                                           len(Data),\n",
    "                                                           row['Sample'].rjust(Data['SampleNameLength'].max())))\n",
    "        Skeleton = skimage.morphology.skeletonize_3d(Tophat[c])\n",
    "        Skeleton = da.stack(Skeleton[:])\n",
    "        print('%11s: Saving to %s' % (row['Sample'].rjust(Data['SampleNameLength'].max()),\n",
    "                                     row['OutputNameSkeleton'][len(Root):]))\n",
    "        Skeleton.to_zarr(row['OutputNameSkeleton'],\n",
    "                         overwrite=True,\n",
    "                         compressor=Blosc(cname='zstd', clevel=3, shuffle=Blosc.BITSHUFFLE))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DASK arrays with the skeletonized images\n",
    "Skeleton = [dask.array.from_zarr(file) for file in Data['OutputNameSkeleton']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read or calculate the middle slices of the Skeletonization images,\n",
    "# put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Skeleton_Mid_' + direction] = [None] * len(VOIs)\n",
    "for c, row in tqdm.notebook.tqdm(Data.iterrows(), desc='Middle skeleton images', total=len(Data)):\n",
    "    for d, direction in tqdm.notebook.tqdm(enumerate(directions),\n",
    "                                      desc=row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        outfilepath = os.path.join(row['Folder'],\n",
    "                                   '%s.Thresholded%03d.Skeleton.Middle.%s.png' % (row['Sample'],\n",
    "                                                                                  row['Threshold'],\n",
    "                                                                                  direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c,'Skeleton_Mid_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate requested axial view\n",
    "            if 'Axial' in direction:\n",
    "                Data.at[c,'Skeleton_Mid_' + direction] = Skeleton[c][Data['Size'][c][0]//2]\n",
    "            if 'Sagittal' in direction:\n",
    "                Data.at[c,'Skeleton_Mid_' + direction] = Skeleton[c][:,Data['Size'][c][1]//2,:]\n",
    "            if 'Coronal' in direction:\n",
    "                Data.at[c,'Skeleton_Mid_' + direction] = Skeleton[c][:,:,Data['Size'][c][2]//2]\n",
    "            # Save the calculated 'direction' view out\n",
    "            # Dask only calculates/reads the images here at this point...\n",
    "            imageio.imwrite(outfilepath, Data.at[c,'Skeleton_Mid_' + direction].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show middle skeletonized images with overlay\n",
    "for c, row in Data.iterrows():\n",
    "    for d, direction in enumerate(directions):\n",
    "        plt.subplot(1, 3, d + 1)\n",
    "        plt.imshow(row['Mid_' + direction])\n",
    "        plt.imshow(dask.array.ma.masked_less(row['Skeleton_Mid_' + direction], 1), alpha=0.5, cmap='viridis')        \n",
    "        plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "#         plt.title('%s/%s: %s, %s' % (c + 1,\n",
    "#                                      len(Data),\n",
    "#                                      row['Sample'],\n",
    "#                                      direction + ' MIP'))\n",
    "        plt.title('%s: %s' % (row['Sample'],\n",
    "                              direction + ' MIP'))\n",
    "        \n",
    "        plt.axis('off')\n",
    "    plt.savefig(os.path.join(row['Folder'], row['Sample'] + '.Skeleton.Overlay.MiddleSlices.png'),\n",
    "                bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d,direction in enumerate(directions):\n",
    "    for c,row in Data.iterrows():\n",
    "        plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)\n",
    "        plt.imshow(row['Tophat_Mid_' + direction])\n",
    "        plt.imshow(row['Skeleton_Mid_' + direction], alpha=0.5, cmap='viridis')\n",
    "        plt.title('Middle %s slice of Skeletonization of\\n%s together with tophat' % (direction, row['Sample']))\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][c], 'um'))    \n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a, b = scipy.ndimage.morphology.distance_transform_edt(Tophat[0][900:1000], sampling=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numpy.shape(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(a[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tophat[0][800:-800,800:-800,800:-800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tophat[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Folder',\n",
    "      'Sample',\n",
    "#       'Scan',\n",
    "      'SampleNameLength',\n",
    "      'ScanNameLength',\n",
    "      'Experiment',\n",
    "      'Timepoint',\n",
    "      'LogFile',\n",
    "#       'VOIFolders',\n",
    "      'VOIFolder',\n",
    "      'Voxelsize',\n",
    "#       'VOISlices',\n",
    "      'Number of VOI slices',\n",
    "      'Size',\n",
    "      'VOIVolume',\n",
    "      'GrayValueMean',\n",
    "      'GrayValueMeanNormalizedToVOIVolume',\n",
    "      'Threshold',\n",
    "      'ThresholdMean',\n",
    "      'ThresholdedVolume',\n",
    "      'GrayValueMeanNormalizedToThresholdedVolume']].to_excel(os.path.join(OutputDir, 'Data_' + get_git_hash() + '.xls'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in Data:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the euclidean distance transformation\n",
    "subsampling = None\n",
    "if subsampling:\n",
    "    Data['OutputNameEDT'] = [f.replace('.zarr', '_edt_sampling%s.zarr' % subsampling) for f in Data['OutputNameTophat']]\n",
    "else:\n",
    "    Data['OutputNameEDT'] = [f.replace('.zarr', '_edt.zarr') for f in Data['OutputNameTophat']]    \n",
    "# Calculate EDT\n",
    "for c, row in Data.iterrows():\n",
    "    if os.path.exists(row['OutputNameEDT']):\n",
    "        print('%2s/%s: Already saved to %s' % (c + 1,\n",
    "                                               len(Data),\n",
    "                                               row['OutputNameEDT'][len(Root):]))\n",
    "    else:\n",
    "        print('%2s/%s: %s: Calculating euclidean distance transformation' % (c + 1,\n",
    "                                                                             len(Data),\n",
    "                                                                             row['Sample'].rjust(Data['SampleNameLength'].max())))\n",
    "        EDT = scipy.ndimage.morphology.distance_transform_edt(Tophat[c].astype('bool'),\n",
    "                                                              sampling=subsampling)\n",
    "        EDT = da.stack(EDT[:])\n",
    "        print('%11s: Saving to %s' % (row['Sample'].rjust(Data['SampleNameLength'].max()),\n",
    "                                      row['OutputNameEDT']))\n",
    "        EDT.to_zarr(row['OutputNameEDT'],\n",
    "                    overwrite=True,\n",
    "                    compressor=Blosc(cname='zstd', clevel=3, shuffle=Blosc.BITSHUFFLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the EDT from the saved zarr files   \n",
    "EDT = [dask.array.from_zarr(file) for file in Data['OutputNameEDT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DASK\n",
    "# Read or calculate the middle slices of the EDT images,\n",
    "# put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['EDT_Mid_' + direction] = [None] * len(VOIs)\n",
    "for c, row in tqdm.notebook.tqdm(Data.iterrows(), desc='Middle EDT images', total=len(Data)):\n",
    "    for d, direction in tqdm.notebook.tqdm(enumerate(directions),\n",
    "                                           desc=row['Sample'],\n",
    "                                           leave=False,\n",
    "                                           total=len(directions)):\n",
    "        outfilepath = os.path.join(row['Folder'],\n",
    "                                   '%s.Thresholded%03d.EDT.Middle.%s.png' % (row['Sample'],\n",
    "                                                                             row['Threshold'],\n",
    "                                                                             direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c,'EDT_Mid_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate requested axial view\n",
    "            if 'Axial' in direction:\n",
    "                Data.at[c,'EDT_Mid_' + direction] = EDT[c][Data['Size'][c][0]//2]\n",
    "            if 'Sagittal' in direction:\n",
    "                Data.at[c,'EDT_Mid_' + direction] = EDT[c][:,Data['Size'][c][1]//2,:]\n",
    "            if 'Coronal' in direction:\n",
    "                Data.at[c,'EDT_Mid_' + direction] = EDT[c][:,:,Data['Size'][c][2]//2]\n",
    "            # Save the calculated 'direction' view out\n",
    "            # Dask only calculates/reads the images here at this point...\n",
    "            imageio.imwrite(outfilepath,Data.at[c,'EDT_Mid_' + direction].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d,direction in enumerate(directions):\n",
    "    for c,row in Data.iterrows():\n",
    "        plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)\n",
    "        plt.imshow(row['Flooded_Mid_' + direction])\n",
    "        plt.imshow(row['EDT_Mid_' + direction], alpha=0.5, cmap='viridis')\n",
    "        plt.title('Middle %s slice of EDT of\\n%s together with original' % (direction, row['Sample']))\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][c], 'um'))    \n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d,direction in enumerate(directions):\n",
    "    for c,row in Data.iterrows():\n",
    "        plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)\n",
    "        plt.imshow(row['EDT_Mid_' + direction], alpha=0.5, cmap='viridis')\n",
    "        plt.title('Middle %s slice of EDT of\\n%s together with original' % (direction, row['Sample']))\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][c], 'um'))    \n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show middle skeletonized images with overlay\n",
    "for c, row in Data.iterrows():\n",
    "    for d, direction in enumerate(directions):\n",
    "        plt.subplot(1, 3, d + 1)\n",
    "        plt.imshow(row['EDT_Mid_' + direction])\n",
    "        plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "#         plt.title('%s/%s: %s, %s' % (c + 1,\n",
    "#                                      len(Data),\n",
    "#                                      row['Sample'],\n",
    "#                                      direction + ' MIP'))\n",
    "        plt.title('%s: %s' % (row['Sample'],\n",
    "                              direction + ' MIP'))\n",
    "        \n",
    "        plt.axis('off')\n",
    "    plt.savefig(os.path.join(row['Folder'], row['Sample'] + '.EDT.MiddleSlices.png'),\n",
    "                bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distance on skeleton\n",
    "Data['OutputNameSkelDist'] = [f.replace('.zarr', '_skeletondistance.zarr') for f in Data['OutputNameTophat']]\n",
    "# Calculate edt\n",
    "for c, row in Data.iterrows():\n",
    "    if os.path.exists(row['OutputNameSkelDist']):\n",
    "        print('%2s/%s: Already saved to %s' % (c + 1,\n",
    "                                               len(Data),\n",
    "                                               row['OutputNameSkelDist'][len(Root):]))\n",
    "    else:\n",
    "        print('%2s/%s: %s: Multiplying skeleton and EDT and saving to %s' % (c + 1,\n",
    "                                                                             len(Data),\n",
    "                                                                             row['Sample'].rjust(Data['SampleNameLength'].max()),\n",
    "                                                                             row['OutputNameSkelDist'][len(Root):]))\n",
    "        SkelDist = numpy.multiply(Skeleton[c], EDT[c])\n",
    "        SkelDist.to_zarr(row['OutputNameSkelDist'],\n",
    "                         overwrite=True,\n",
    "                         compressor=Blosc(cname='zstd', clevel=3, shuffle=Blosc.BITSHUFFLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DASK arrays with the skeleton-distance\n",
    "SkelDist = [dask.array.from_zarr(file) for file in Data['OutputNameSkelDist']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DASK\n",
    "# Read or calculate the middle slices of the SkelDist images,\n",
    "# put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['SkelDist_Mid_' + direction] = [None] * len(VOIs)\n",
    "for c, row in tqdm.notebook.tqdm(Data.iterrows(), desc='Middle SkelDist images', total=len(Data)):\n",
    "    for d, direction in tqdm.notebook.tqdm(enumerate(directions),\n",
    "                                      desc=row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        outfilepath = os.path.join(row['Folder'],\n",
    "                                   '%s.Thresholded%03d.SkelDist.Middle.%s.png' % (row['Sample'],\n",
    "                                                                                  row['Threshold'],\n",
    "                                                                                  direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c,'SkelDist_Mid_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate requested axial view\n",
    "            if 'Axial' in direction:\n",
    "                Data.at[c,'SkelDist_Mid_' + direction] = SkelDist[c][Data['Size'][c][0]//2]\n",
    "            if 'Sagittal' in direction:\n",
    "                Data.at[c,'SkelDist_Mid_' + direction] = SkelDist[c][:,Data['Size'][c][1]//2,:]\n",
    "            if 'Coronal' in direction:\n",
    "                Data.at[c,'SkelDist_Mid_' + direction] = SkelDist[c][:,:,Data['Size'][c][2]//2]\n",
    "            # Save the calculated 'direction' view out\n",
    "            # Dask only calculates/reads the images here at this point...\n",
    "            imageio.imwrite(outfilepath,Data.at[c,'SkelDist_Mid_' + direction].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d,direction in enumerate(directions):\n",
    "    for c,row in Data.iterrows():\n",
    "        plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)\n",
    "#         plt.imshow(row['Thresholded_Mid_' + direction])\n",
    "#         plt.imshow(dask.array.ma.masked_where(0, row['EDT_Mid_' + direction]), alpha=0.5, cmap='viridis')\n",
    "        plt.imshow(row['SkelDist_Mid_' + direction], alpha=0.5, cmap='viridis')\n",
    "        plt.title('Middle %s slice of SkelDist of\\n%s together with original' % (direction, row['Sample']))\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][c], 'um'))    \n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sl = 999\n",
    "# plt.subplot(131)\n",
    "# plt.imshow(numpy.max(Skeleton[0], axis=0), cmap='viridis')\n",
    "# plt.subplot(132)\n",
    "# plt.imshow(numpy.max(EDT[0], axis=0), cmap='viridis')\n",
    "# plt.subplot(133)\n",
    "# plt.imshow(numpy.max(SkelDist[0], axis=0), cmap='viridis')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read or calculate skeletondistance MIPs, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['MIP_SkelDist_' + direction] = [None] * len(VOIs)\n",
    "for c, row in tqdm.notebook.tqdm(Data.iterrows(), desc='MIPs SkelDist', total=len(Data)):\n",
    "    for d, direction in tqdm.notebook.tqdm(enumerate(directions),\n",
    "                                      desc=row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        outfilepath = os.path.join(row['Folder'],\n",
    "                                   '%s.Thresholded%03d.MIP.SkelDist.%s.png' % (row['Sample'],\n",
    "                                                                               row['Threshold'],\n",
    "                                                                               direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c,'MIP_SkelDist_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Keep *this* reconstruction in RAM for a bit\n",
    "            img = SkelDist[c].astype('uint8').persist()\n",
    "            # Generate MIP\n",
    "            Data.at[c,'MIP_SkelDist_' + direction] = img.max(axis=d).compute()\n",
    "            # Save it out\n",
    "            imageio.imwrite(outfilepath,\n",
    "                            Data.at[c,'MIP_SkelDist_' + direction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in Data.iterrows():\n",
    "    for j, direction in enumerate(directions):\n",
    "        plt.subplot(1,3,j+1)\n",
    "#         plt.imshow(row['MIP_' + direction], alpha=0.5)\n",
    "#         plt.imshow(dask.array.ma.masked_less(row['MIP_SkelDist_' + direction],1), cmap='viridis')        \n",
    "        plt.imshow(row['MIP_SkelDist_' + direction], cmap='viridis')            \n",
    "        plt.title('%s view' % direction)\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][c], 'um'))                \n",
    "        plt.axis('off')        \n",
    "    plt.suptitle('%02d/%02d: MIP with Skeleton overlay %s' % (i+1, len(Data), row['Sample']))\n",
    "    plt.savefig(os.path.join(row['Folder'], row['Sample'] + '.SkelDist.MiddleSlices.png'),\n",
    "                bbox_inches='tight')    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SkelDist[0].max().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['SkelDistMean'] = [dask.array.mean(skldst).compute() for skldst in SkelDist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['SkelDistMeanNormalized'] = [dask.array.mean(skldst).compute()/tv for skldst, tv in zip(SkelDist, Data['ThresholdedVolume'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['SkelDistSTD'] = [dask.array.std(skldst).compute() for skldst in SkelDist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean of datasets for comparison\n",
    "seaborn.catplot(data=Data, kind='box', x='Sample', y='SkelDistMean')\n",
    "seaborn.swarmplot(data=Data, x='Sample', y='SkelDistMean', linewidth=1.5, s=10, color='gray');\n",
    "plt.ylabel('Mean Skeleton distance value')\n",
    "plt.ylim(ymin=0)\n",
    "plt.savefig(os.path.join(OutputDir,\n",
    "                         'Skeleton_Average_Distance.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean of datasets for comparison\n",
    "seaborn.catplot(data=Data, kind='box', x='Sample', y='SkelDistMeanNormalized')\n",
    "seaborn.swarmplot(data=Data, x='Sample', y='SkelDistMeanNormalized', linewidth=1.5, s=10, color='gray');\n",
    "plt.ylabel('Mean Skeleton distance value, normalized to thresholded volume')\n",
    "plt.ylim(ymin=0)\n",
    "plt.savefig(os.path.join(OutputDir,\n",
    "                         'Skeleton_Average_Distance_Normalized.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot STD of datasets for comparison\n",
    "seaborn.catplot(data=Data, kind='box', x='Sample', y='SkelDistSTD')\n",
    "seaborn.swarmplot(data=Data, x='Sample', y='SkelDistSTD', linewidth=1.5, s=10, color='gray');\n",
    "plt.ylabel('Skeleton distance STD')\n",
    "plt.ylim(ymin=0)\n",
    "plt.savefig(os.path.join(OutputDir,\n",
    "                         'Skeleton_Average_Distance_STD.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
