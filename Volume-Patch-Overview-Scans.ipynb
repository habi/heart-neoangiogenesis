{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch volume\n",
    "Simple detection of path volume, which Tim delineated in CTAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import os\n",
    "import glob\n",
    "import pandas\n",
    "import numpy\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import seaborn\n",
    "import dask\n",
    "import dask_image.imread\n",
    "from dask.distributed import Client\n",
    "from numcodecs import Blosc\n",
    "from tqdm import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dask temporary folder\n",
    "# Do this before creating a client: https://stackoverflow.com/a/62804525/323100\n",
    "if 'Linux' in platform.system():\n",
    "    tmp = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "elif 'Darwin' in platform.system():\n",
    "    import tempfile\n",
    "    tmp = tempfile.gettempdir()\n",
    "else:\n",
    "    if 'anaklin' in platform.node():\n",
    "        tmp = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        tmp = os.path.join('D:\\\\')\n",
    "dask.config.set({'temporary_directory': os.path.join(tmp, 'tmp')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start dask client and tell where we can see what it does\n",
    "client = Client()\n",
    "print('You can seee what DASK is doing at \"http://localhost:%s/status\"' % client.scheduler_info()['services']['dashboard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings in the notebook\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure defaults\n",
    "plt.rc('image', cmap='gray', interpolation='nearest')  # Display all images in b&w and with 'nearest' interpolation\n",
    "plt.rcParams['figure.figsize'] = (16, 9)  # Size up figures a bit\n",
    "plt.rcParams['figure.dpi'] = 200  # Increase dpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup scale bar defaults\n",
    "plt.rcParams['scalebar.location'] = 'lower right'\n",
    "plt.rcParams['scalebar.frameon'] = False\n",
    "plt.rcParams['scalebar.color'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all plots identically\n",
    "lines = 3\n",
    "# And then do something like\n",
    "# plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_git_hash():\n",
    "    \"\"\"\n",
    "    Get the current git hash from the repository.\n",
    "    Based on http://stackoverflow.com/a/949391/323100 and\n",
    "    http://stackoverflow.com/a/18283905/323100\n",
    "    \"\"\"\n",
    "    from subprocess import Popen, PIPE\n",
    "    import os\n",
    "    gitprocess = Popen(['git', '--git-dir', os.path.join(os.getcwd(), '.git'),\n",
    "                        'rev-parse', '--short', '--verify', 'HEAD'],\n",
    "                       stdout=PIPE)\n",
    "    (output, _) = gitprocess.communicate()\n",
    "    return output.strip().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are we working with?\n",
    "the_current_git_hash = get_git_hash()\n",
    "print('We are working with version %s of the analyis notebook.'\n",
    "      % the_current_git_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the output folder\n",
    "# Including the git hash, so we (potentially) have different versions of all the images we generate\n",
    "OutputDir = os.path.join('Output', the_current_git_hash)\n",
    "os.makedirs(OutputDir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different locations if running either on Linux or Windows\n",
    "if 'anaklin25' in platform.node():\n",
    "    FastSSD = True\n",
    "else:\n",
    "    FastSSD = False\n",
    "# to speed things up significantly\n",
    "if 'Linux' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "    else:\n",
    "        BasePath = os.path.join(os.sep, 'home', 'habi', '1272')\n",
    "elif 'Darwin' in platform.system():\n",
    "    BasePath = os.path.join('/Volumes/2TBSSD/')\n",
    "else:\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        if 'anaklin' in platform.node():\n",
    "            BasePath = os.path.join('S:\\\\')\n",
    "        else:\n",
    "            BasePath = os.path.join('D:\\\\Results')\n",
    "Root = os.path.join(BasePath, 'Hearts Melly')\n",
    "print('We are loading all the data from %s' % Root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixelsize(logfile):\n",
    "    \"\"\"Get the pixel size from the scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Image Pixel' in line and 'Scaled' not in line:\n",
    "                pixelsize = float(line.split('=')[1])\n",
    "    return(pixelsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make us a dataframe for saving all that we need\n",
    "Data = pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get *all* log files\n",
    "Data['LogFile'] = [f for f in sorted(glob.glob(os.path.join(Root, '**', '*rec.log'), recursive=True))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all folders\n",
    "Data['Folder'] = [os.path.dirname(f) for f in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of all non-rec logfiles\n",
    "for c, row in Data.iterrows():\n",
    "    if 'rec' not in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "    if 'tweak' in row.Folder:\n",
    "        Data.drop([c], inplace=True)        \n",
    "# Reset dataframe to something that we would get if we only would have loaded the 'rec' files\n",
    "Data = Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all folders we don't need\n",
    "for c, row in Data.iterrows():\n",
    "    if 'Rat' not in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'Rat4' in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'Rat5' in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'Test' in row.Folder:\n",
    "        Data.drop([c], inplace=True)        \n",
    "# Reset dataframe to something that we would get if we only would have loaded the 'rec' files\n",
    "Data = Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some data from folders\n",
    "Data['Sample'] = [l[len(Root)+1:].split(os.sep)[0] for l in Data['LogFile']]\n",
    "Data['Animal'] = [r.replace('Rat', '') for r in Data['Sample']]\n",
    "Data['Scan'] = [l[len(Root)+1:].split(os.sep)[1] for l in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in animals list from Ludovic\n",
    "AnimalTable = pandas.read_excel('Animals.xlsx',\n",
    "                                engine='openpyxl',\n",
    "                                header=None,\n",
    "                                names=['Animal', 'Gender', '', 'Experiment', 'Timepoint'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AnimalTable.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in data from animals table\n",
    "for c, rowdata in Data.iterrows():\n",
    "    for d, rowanimals in AnimalTable.iterrows():\n",
    "        if str(rowanimals.Animal) in rowdata.Animal:\n",
    "            Data.at[c, 'Experiment'] = rowanimals.Experiment\n",
    "            Data.at[c, 'Timepoint'] = rowanimals.Timepoint\n",
    "            Data.at[c, 'Gender'] = rowanimals.Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclusion from Tims visual inspection\n",
    "# R63\n",
    "# R65\n",
    "# R66\n",
    "# R70\n",
    "#exclude = [63, 65, 66, 70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop samples which should be excluded\n",
    "# Based on https://stackoverflow.com/a/13851602\n",
    "#for c,row in Data.iterrows():\n",
    "#    for ex in exclude:\n",
    "#        if str(ex) in row.Sample:\n",
    "#            Data.drop(c, inplace=True)\n",
    "#Data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Filter\" to subset that we want\n",
    "for c,row in Data.iterrows():\n",
    "    if 'overview' not in row.Scan:\n",
    "        Data.drop(c, inplace=True)\n",
    "Data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tim delineated a *lot* of VOIs for each heart.\n",
    "# He saved them with CTAn, so each get's a successive numbered voi folder (e.g. VOI(X)).\n",
    "# Bruker doesn't use leading zeros. Nonetheless, we can just user a 'sorted'  list.\n",
    "# And just use the last item of the list\n",
    "Data['VOIFolders'] = [sorted(glob.glob(os.path.join(f, 'VOI*')),\n",
    "                             key=os.path.getmtime) for f in Data['Folder']]\n",
    "# If we didn't find a VOI folder, then just return empty\n",
    "Data['VOIFolder'] = [f[-1] if f else [] for f in Data['VOIFolders']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c,row in Data.iterrows():\n",
    "    if not len(row.VOIFolders):\n",
    "        print('The patch of', row.Sample, 'needs to be delineated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop samples which have not been delineated (yet)\n",
    "# Based on https://stackoverflow.com/a/13851602\n",
    "Data = Data[Data['VOIFolder'].map(len) > 0]\n",
    "Data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get voxelsize from logfiles\n",
    "Data['Voxelsize'] = [get_pixelsize(log) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file names of the reconstructions\n",
    "Data['VOISlices'] = [sorted(glob.glob(os.path.join(f, '*.png'))) for f in Data['VOIFolder']]\n",
    "Data['Number of VOI slices'] = [len(r) for r in Data.VOISlices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some info\n",
    "for c, row in Data.iterrows():\n",
    "    print('%02s/%s: %s has %03s slices in its VOI folder in %s' % (c + 1,\n",
    "                                                                   len(Data),\n",
    "                                                                   row['Sample'],\n",
    "                                                                   row['Number of VOI slices'],\n",
    "                                                                   row['VOIFolder'][len(Root):]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all VOI slices into a DASK array and save them to disk\n",
    "# Partially based on http://stackoverflow.com/a/39195332/323100\n",
    "# and on /LungMetastasis/HighResolutionScanAnalysis.ipynb\n",
    "Data['OutputNameVOI'] = [os.path.join(f, sample + '_patch.zarr') for f, sample in zip(Data.Folder, Data.Sample)]\n",
    "for c, row in notebook.tqdm(Data.iterrows(),\n",
    "                            desc='Converting to .zarr',\n",
    "                            total=len(Data)):\n",
    "    if not os.path.exists(row['OutputNameVOI']):\n",
    "        print('%2s/%s: Reading %s VOI slices from %s and saving to %s' % (c + 1,\n",
    "                                                                          len(Data),\n",
    "                                                                          row['Number of VOI slices'],\n",
    "                                                                          row['VOIFolder'][len(Root):],\n",
    "                                                                          row['OutputNameVOI'][len(Root):]))\n",
    "        Reconstructions = dask_image.imread.imread(os.path.join(row['VOIFolder'], '*.png'))\n",
    "        Reconstructions.rechunk(100).to_zarr(row['OutputNameVOI'],\n",
    "                                             overwrite=True,\n",
    "                                             compressor=Blosc(cname='zstd',\n",
    "                                                              clevel=3,\n",
    "                                                              shuffle=Blosc.BITSHUFFLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the reconstructions a zarr arrays\n",
    "VOIs = [dask.array.from_zarr(file) for file in Data['OutputNameVOI']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How big are the datasets?\n",
    "Data['Size'] = [rec.shape for rec in VOIs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The three cardinal directions\n",
    "directions = ['Axial', 'Sagittal', 'Coronal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read or calculate the middle slices, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Mid_' + direction] = [None] * len(VOIs)\n",
    "for c, row in notebook.tqdm(Data.iterrows(), desc='Middle images', total=len(Data)):\n",
    "    for d, direction in notebook.tqdm(enumerate(directions),\n",
    "                                      desc=row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        outfilepath = os.path.join(os.path.dirname(row['Folder']), '%s.Patch.Middle.%s.png' % (row['Sample'], direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c,'Mid_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate requested axial view\n",
    "            if 'Axial' in direction:\n",
    "                Data.at[c,'Mid_' + direction] = VOIs[c][Data['Size'][c][0]//2].compute()\n",
    "            if 'Sagittal' in direction:\n",
    "                Data.at[c,'Mid_' + direction] = VOIs[c][:,Data['Size'][c][1]//2,:].compute()\n",
    "            if 'Coronal' in direction:\n",
    "                Data.at[c,'Mid_' + direction] = VOIs[c][:,:,Data['Size'][c][2]//2].compute()\n",
    "            # Save the calculated 'direction' view to disk\n",
    "            imageio.imwrite(outfilepath, (Data.at[c,'Mid_' + direction]).astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show middle slices\n",
    "# for c, row in Data.iterrows():\n",
    "#     for d, direction in enumerate(directions):\n",
    "#         plt.subplot(1, 3, d + 1)\n",
    "#         plt.imshow(row['Mid_' + direction])\n",
    "#         plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "#         plt.title('%s, %s' % (row['Sample'],\n",
    "#                               direction + ' Middle slice'))\n",
    "#         plt.axis('off')\n",
    "#     plt.savefig(os.path.join(row['Folder'], row['Sample'] + '.MiddleSlices.png'),\n",
    "#                 bbox_inches='tight')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read or calculate the directional MIPs, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['MIP_' + direction] = [None] * len(VOIs)\n",
    "for c, row in notebook.tqdm(Data.iterrows(), desc='MIPs', total=len(Data)):\n",
    "    for d, direction in notebook.tqdm(enumerate(directions),\n",
    "                                      desc=row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        outfilepath = os.path.join(os.path.dirname(row['Folder']), '%s.Patch.MIP.%s.png' % (row['Sample'], direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c,'MIP_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate MIP\n",
    "            Data.at[c,'MIP_' + direction] = VOIs[c].max(axis=d).compute()\n",
    "            # Save it out\n",
    "            imageio.imwrite(outfilepath, Data.at[c,'MIP_' + direction].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show MIP slices\n",
    "# for c, row in Data.iterrows():\n",
    "#     for d, direction in enumerate(directions):\n",
    "#         plt.subplot(1, 3, d + 1)\n",
    "#         plt.imshow(row['MIP_' + direction])\n",
    "#         plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "#         plt.title('%s: %s' % (row['Sample'],\n",
    "#                               direction + ' MIP'))\n",
    "        \n",
    "#         plt.axis('off')\n",
    "#     plt.savefig(os.path.join(row['Folder'], row['Sample'] + '.MIPs.png'),\n",
    "#                 bbox_inches='tight')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mean` gray value needs to be calculated and 'calibrated' to the total volume of the ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask the outside of the ROI that Tim drew\n",
    "Masked = [dask.array.ma.masked_equal(v, 0) for v in VOIs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How large are the VOIs from Tim?\n",
    "# We select/mask everything non-zero and fill this whith one.\n",
    "VOIRegion = [dask.array.ma.filled(dask.array.ma.masked_not_equal(v, 0), 1) for v in VOIs]\n",
    "# By summing it, we get the volume\n",
    "Data['VOIVolume'] = [vr.sum().compute() for vr in VOIRegion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(131)\n",
    "plt.imshow(Masked[0][100][950:1700,700:1800])\n",
    "plt.title('Mask')\n",
    "plt.subplot(132)\n",
    "plt.imshow(VOIRegion[0][100][950:1700,700:1800])\n",
    "plt.title('VOI')\n",
    "plt.subplot(133)\n",
    "plt.imshow(VOIs[0][100][950:1700,700:1800])\n",
    "plt.title('Original')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot volume of (selected) VOIs for comparison\n",
    "seaborn.swarmplot(data=Data,\n",
    "                  x='Experiment',\n",
    "                  y='VOIVolume',\n",
    "                  dodge=True,\n",
    "                  hue='Timepoint',\n",
    "                  s=10,\n",
    "                  linewidth=1.5)\n",
    "for c,row in Data.iterrows():\n",
    "    if 'VP' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (0, row.VOIVolume))\n",
    "    elif 'F' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (1, row.VOIVolume))\n",
    "    elif 'Tacho' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (2, row.VOIVolume))\n",
    "plt.ylim(ymin=0)\n",
    "plt.title('Volume of the individual VOIs')\n",
    "plt.savefig(os.path.join(OutputDir, 'Volume.VOIs.png'),\n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdfasdfasdfasdf=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.Sample.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutputDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Sample',\n",
    "      'VOIVolume']].to_excel(os.path.join(OutputDir, 'Volume.VOIs.xls'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data.groupby(by=[['Experiment', 'VOI']])['VOIVolume'].describe()[['count',\n",
    "#                                                                   'mean',\n",
    "#                                                                   'std',\n",
    "#                                                                   'min',\n",
    "#                                                                   'max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save mean of reconstruction gray values,\n",
    "# We can use this for getting an overview of the image data\n",
    "Data['GrayValueMean'] = [m.mean().compute() for m  in Masked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot volume-normalized mean of datasets for comparison\n",
    "seaborn.stripplot(data=Data, x='Experiment', y='GrayValueMean', hue='Timepoint', s=10, linewidth=1.5, dodge=True)\n",
    "for c,row in Data.iterrows():\n",
    "    if 'VP' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (0, row.GrayValueMean))\n",
    "    elif 'F' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (1, row.GrayValueMean))\n",
    "    elif 'Tacho' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (2, row.GrayValueMean))\n",
    "plt.ylim(ymin=0)\n",
    "plt.title('Average grayvalue in the volumes of interest')\n",
    "plt.savefig(os.path.join(OutputDir, 'Grayvalues.Mean.VOIs.png'),\n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Sample', 'GrayValueMean']].to_excel(os.path.join(OutputDir, 'Grayvalues.Mean.VOIs.xls'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.groupby(by=['Experiment'])['GrayValueMean'].describe()[['count',\n",
    "                                                             'mean',\n",
    "                                                             'std',\n",
    "                                                             'min',\n",
    "                                                             'max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['GrayValueMeanNormalizedToVOIVolume'] = [numpy.divide(gvm,\n",
    "                                                           vv) for gvm, vv in zip(Data['GrayValueMean'],\n",
    "                                                                                  Data['VOIVolume'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot volume-normalized mean of datasets for comparison\n",
    "#seaborn.boxplot(data=Data, x='Experiment', y='GrayValueMeanNormalizedToVOIVolume', hue='Timepoint')\n",
    "seaborn.swarmplot(data=Data, x='Experiment', y='GrayValueMeanNormalizedToVOIVolume', hue='Timepoint', dodge=True, linewidth=1.5, s=10)\n",
    "for c,row in Data.iterrows():\n",
    "    if 'VP' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (0, row.GrayValueMeanNormalizedToVOIVolume))\n",
    "    elif 'F' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (1, row.GrayValueMeanNormalizedToVOIVolume))\n",
    "    elif 'Tacho' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (2, row.GrayValueMeanNormalizedToVOIVolume))\n",
    "plt.ylim(ymin=0, ymax=1.1*Data.GrayValueMeanNormalizedToVOIVolume.max())\n",
    "plt.title('Averaged VOI grayvalue normalized to VOI volume')\n",
    "plt.savefig(os.path.join(OutputDir, 'Grayvalues.Mean.NormalizedVOI.png'),\n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutputDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thresholds\n",
    "preset = True\n",
    "if preset:\n",
    "    # Set them (from previous calculations)\n",
    "    Data['Threshold'] = [44, 46, 41, 16, 12, 7, 63, 15, 15, 19, 13, 13]\n",
    "#     Data['Threshold'] = [41, 45, 41, 12, 13, 19, 13]\n",
    "else:\n",
    "    # Calculate Threshold\n",
    "    Data['Threshold'] = [skimage.filters.threshold_otsu(\n",
    "        dask.array.ravel(\n",
    "            dask.array.ma.masked_less(\n",
    "                rec, 1).compute())) for rec in VOIs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(Data.Threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean threshold of all samples\n",
    "Data['ThresholdMean'] = int(Data['Threshold'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the thresholds\n",
    "seaborn.swarmplot(data=Data, x='Experiment', y='Threshold', hue='Timepoint', dodge=True, linewidth=1.5, s=10)\n",
    "for c,row in Data.iterrows():\n",
    "    if 'VP' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (0, row.Threshold))\n",
    "    elif 'F' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (1, row.Threshold))\n",
    "    elif 'Tacho' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (2, row.Threshold))\n",
    "plt.axhline(Data['ThresholdMean'].mean(), label='Mean threshold @ %s' % Data['ThresholdMean'].mean())\n",
    "plt.ylim(ymin=0)\n",
    "plt.legend()\n",
    "plt.title('Otsu thresholds of individual VOIs')\n",
    "plt.savefig(os.path.join(OutputDir, 'Thresholds.png'),\n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Sample', 'Threshold']].to_excel(os.path.join(OutputDir, 'Thresholds.xls'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.groupby(by=['Experiment'])['Threshold'].describe()[['count',\n",
    "                                                         'mean',\n",
    "                                                         'std',\n",
    "                                                         'min',\n",
    "                                                         'max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold the reconstructions individually                                                                                                                           Data.Sample)\n",
    "Data['OutputNameThresholded'] = [f.replace('.zarr',\n",
    "                                           '_thresholded_%s.zarr' % str(t).zfill(3)) for f, t in zip(Data['OutputNameVOI'],\n",
    "                                                                                                     Data['Threshold'])]\n",
    "for c, row in Data.iterrows():\n",
    "    if os.path.exists(row['OutputNameThresholded']):  \n",
    "        print('%2s/%s: Already saved to %s' % (c + 1,\n",
    "                                               len(Data),\n",
    "                                               row['OutputNameThresholded'][len(Root):]))\n",
    "    else:\n",
    "        print('%2s/%s: Thresholding and saving to %s' % (c + 1,\n",
    "                                                         len(Data),\n",
    "                                                         row['OutputNameThresholded'][len(Root):]))\n",
    "        Thresholded = VOIs[c] > row['Threshold']\n",
    "        Thresholded.to_zarr(row['OutputNameThresholded'],\n",
    "                           overwrite=True,\n",
    "                           compressor=Blosc(cname='zstd', clevel=3, shuffle=Blosc.BITSHUFFLE))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold the reconstructions with the mean threshold                                                                                                                           Data.Sample)\n",
    "Data['OutputNameThresholdedMean'] = [f.replace('.zarr',\n",
    "                                           '_thresholded_%s.zarr' % str(t).zfill(3)) for f, t in zip(Data['OutputNameVOI'],\n",
    "                                                                                                     Data['ThresholdMean'])]\n",
    "for c, row in Data.iterrows():\n",
    "    if os.path.exists(row['OutputNameThresholdedMean']):  \n",
    "        print('%2s/%s: Already saved to %s' % (c + 1,\n",
    "                                               len(Data),\n",
    "                                               row['OutputNameThresholdedMean'][len(Root):]))\n",
    "    else:\n",
    "        print('%2s/%s: Thresholding and saving to %s' % (c + 1,\n",
    "                                                         len(Data),\n",
    "                                                         row['OutputNameThresholded'][len(Root):]))\n",
    "        Thresholded = VOIs[c] > row['ThresholdMean']\n",
    "        Thresholded.to_zarr(row['OutputNameThresholdedMean'],\n",
    "                           overwrite=True,\n",
    "                           compressor=Blosc(cname='zstd', clevel=3, shuffle=Blosc.BITSHUFFLE))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DASK arrays of the thresholded samples\n",
    "individualThreshold = True\n",
    "if individualThreshold:\n",
    "    Thresholded = [dask.array.from_zarr(file) for file in Data['OutputNameThresholded']]\n",
    "    print('Loading individually thresholded stacks')\n",
    "else:\n",
    "    Thresholded = [dask.array.from_zarr(file) for file in Data['OutputNameThresholdedMean']]\n",
    "    print('Loading all stacks with a threshold of %s' % Data.ThresholdMean.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DASK\n",
    "# Read or calculate the middle slices of the thresholded images,\n",
    "# put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Thresholded_Mid_' + direction] = [None] * len(VOIs)\n",
    "for c, row in tqdm.notebook.tqdm(Data.iterrows(),\n",
    "                                 desc='Middle thresholded images',\n",
    "                                 total=len(Data)):\n",
    "    for d, direction in tqdm.notebook.tqdm(enumerate(directions),\n",
    "                                           desc=row['Sample'],\n",
    "                                           leave=False,\n",
    "                                           total=len(directions)):\n",
    "        outfilepath = os.path.join(row['Folder'],\n",
    "                                   '%s.Thresholded%03d.Middle.%s.png' % (row['Sample'],\n",
    "                                                                         row['Threshold'],\n",
    "                                                                         direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c,'Thresholded_Mid_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate requested axial view\n",
    "            if 'Axial' in direction:\n",
    "                Data.at[c,'Thresholded_Mid_' + direction] = Thresholded[c][Data['Size'][c][0]//2]\n",
    "            if 'Sagittal' in direction:\n",
    "                Data.at[c,'Thresholded_Mid_' + direction] = Thresholded[c][:,Data['Size'][c][1]//2,:]\n",
    "            if 'Coronal' in direction:\n",
    "                Data.at[c,'Thresholded_Mid_' + direction] = Thresholded[c][:,:,Data['Size'][c][2]//2]\n",
    "            # Save the calculated 'direction' view out\n",
    "            # Dask only calculates/reads the images here at this point...\n",
    "            imageio.imwrite(outfilepath, (Data.at[c,'Thresholded_Mid_' + direction].astype('uint8')*255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for d, direction in enumerate(directions):\n",
    "#     for c,row in Data.iterrows():\n",
    "#         plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)\n",
    "#         plt.imshow(row['Thresholded_Mid_' + direction])\n",
    "#         plt.title('%s\\nMid-%s\\nthresholded slice' % (row['Sample'], direction))\n",
    "#         plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))    \n",
    "#         plt.axis('off')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read or calculate the directional MIPs of the thresholded datasets,\n",
    "# put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Thresholded_MIP_' + direction] = [None] * len(VOIs)\n",
    "for c, row in notebook.tqdm(Data.iterrows(), desc='MIPs', total=len(Data)):\n",
    "    for d, direction in notebook.tqdm(enumerate(directions),\n",
    "                                      desc=row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        outfilepath = os.path.join(row['Folder'], '%s.Thresholded.MIP.%s.png' % (row['Sample'], direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c,'Thresholded_MIP_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate MIP\n",
    "            Data.at[c,'Thresholded_MIP_' + direction] = Thresholded[c].max(axis=d).compute()\n",
    "            # Save it out\n",
    "            imageio.imwrite(outfilepath, Data.at[c,'Thresholded_MIP_' + direction].astype('uint8'))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show thresholded MIP slices\n",
    "# for c, row in Data.iterrows():\n",
    "#     for d, direction in enumerate(directions):\n",
    "#         plt.subplot(1, 3, d + 1)\n",
    "#         plt.imshow(row['Thresholded_MIP_' + direction])\n",
    "#         plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "# #         plt.title('%s/%s: %s, %s' % (c + 1,\n",
    "# #                                      len(Data),\n",
    "# #                                      row['Sample'],\n",
    "# #                                      direction + ' MIP'))\n",
    "#         plt.title('%s, %s' % (row['Sample'], direction + ' MIP'))\n",
    "#         plt.axis('off')\n",
    "#     plt.savefig(os.path.join(row['Folder'], row['Sample'] + '.Thresholded.MIPs.png'),\n",
    "#                 bbox_inches='tight')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the images, so we can see if they contain approximately the same *thresholded* volume\n",
    "Data['ThresholdedVolume'] = [th.sum().compute() for th in Thresholded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data[['Sample', 'ThresholdedVolume']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the thresholded volumes\n",
    "seaborn.swarmplot(data=Data, x='Experiment', y='ThresholdedVolume', hue='Timepoint', dodge=True, linewidth=1.5, s=10)\n",
    "for c,row in Data.iterrows():\n",
    "    if 'VP' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (0, row.ThresholdedVolume))\n",
    "    elif 'F' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (1, row.ThresholdedVolume))\n",
    "    elif 'Tacho' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (2, row.ThresholdedVolume))\n",
    "plt.ylim(ymin=0)\n",
    "if individualThreshold:\n",
    "    plt.title('Volume of the individually thresholded images, corresponding to the total thresholded volume')\n",
    "    plt.savefig(os.path.join(OutputDir, 'Volumes.Thresholded.ThresholdedIndividually.png'),\n",
    "                bbox_inches='tight')    \n",
    "else:\n",
    "    plt.title('Volume of the images thresholded all equally, corresponding to the total thresholded volume')    \n",
    "    plt.savefig(os.path.join(OutputDir, 'Volumes.Thresholded.ThresholdedEqually.png'),\n",
    "                bbox_inches='tight')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutputDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data[['Sample', 'Experiment', 'Threshold', 'ThresholdedVolume', 'GrayValueMean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['GrayValueMeanNormalizedToThresholdedVolume'] = [numpy.divide(gvm,\n",
    "                                                                   tv) for gvm, tv in zip(Data['GrayValueMean'],\n",
    "                                                                                          Data['ThresholdedVolume'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view(Thresholded[3].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot volume-normalized mean of datasets for comparison\n",
    "seaborn.swarmplot(data=Data, x='Experiment', y='GrayValueMeanNormalizedToThresholdedVolume', hue='Timepoint', dodge=True, linewidth=1.5, s=10)\n",
    "for c,row in Data.iterrows():\n",
    "    if 'VP' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (0, row.GrayValueMeanNormalizedToThresholdedVolume))\n",
    "    elif 'F' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (1, row.GrayValueMeanNormalizedToThresholdedVolume))\n",
    "    elif 'Tacho' in row.Experiment:\n",
    "        plt.annotate(row.Sample, (2, row.GrayValueMeanNormalizedToThresholdedVolume))\n",
    "plt.ylim(ymin=0, ymax=Data.GrayValueMeanNormalizedToThresholdedVolume.max())\n",
    "if individualThreshold:\n",
    "    plt.title('Average grayvalue normalized to the thresholded volume')\n",
    "    plt.savefig(os.path.join(OutputDir, 'Grayvalues.Mean.NormalizedToThresholded.ThresholdedIndividually.png'),\n",
    "            bbox_inches='tight')   \n",
    "else:\n",
    "    plt.title('Average grayvalue normalized to the volume thresholded all with the same threshold')        \n",
    "    plt.savefig(os.path.join(OutputDir, 'Grayvalues.Mean.NormalizedToThresholded.ThresholdedEqually.png'),\n",
    "                bbox_inches='tight')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot volume-normalized mean of datasets for comparison\n",
    "# seaborn.swarmplot(data=Data, x='Experiment', y='GrayValueMeanNormalizedToThresholdedVolume', hue='Timepoint', dodge=True, linewidth=1.5, s=10)\n",
    "# for c,row in Data.iterrows():\n",
    "#     if 'VP' in row.Experiment:\n",
    "#         plt.annotate(row.Sample, (0, row.GrayValueMeanNormalizedToThresholdedVolume))\n",
    "#     elif 'F' in row.Experiment:\n",
    "#         plt.annotate(row.Sample, (1, row.GrayValueMeanNormalizedToThresholdedVolume))\n",
    "#     elif 'Tacho' in row.Experiment:\n",
    "#         plt.annotate(row.Sample, (2, row.GrayValueMeanNormalizedToThresholdedVolume))\n",
    "# plt.ylim(ymin=0, ymax=0.000003)\n",
    "# plt.savefig(os.path.join(OutputDir, 'Grayvalues.Mean.NormalizedToThresholded.Without66.png'),\n",
    "#             bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view(Thresholded[0].compute().astype('uint8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characterization of vessel diameter\n",
    "- Fill the vessels/ventricle (with something like `skimage.fill.small.holes`)\n",
    "    This doesn't seem to be working in the 3D case (maybe because of small holes) but we just loop through every slice and do it for each and everyone of it. This is bad code, but works :)\n",
    "- Remove all the big stuff with `tophat`\n",
    "- Calculate the distance-transformation or skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ourfiller(image, verbose=False):\n",
    "    '''\n",
    "    Since we cannot seem to get remove_small_holes to work in 3D, we simply brute-force it on every slice.\n",
    "    Thanks to `tqdm_notebook` we also get a progress bar...\n",
    "    And afterwards generate an output array.\n",
    "    '''\n",
    "    filled = [skimage.morphology.remove_small_holes(s, area_threshold=1e4) for\n",
    "              s in tqdm.notebook.tqdm(image, leave=False)]\n",
    "    if verbose:\n",
    "        plt.subplot(131)\n",
    "        plt.imshow(image[len(filled)//2,:,:])\n",
    "        plt.title('Original')\n",
    "        plt.subplot(132)\n",
    "        plt.imshow(filled[len(filled)//2,:,:])       \n",
    "        plt.title('Filled (output)')\n",
    "        plt.subplot(133)        \n",
    "        plt.imshow(image[len(filled)//2,:,:], alpha=0.5)       \n",
    "        plt.imshow(filled[len(filled)//2,:,:], cmap='viridis', alpha=0.5)       \n",
    "        plt.title('Overlay')        \n",
    "        plt.show()    \n",
    "    return(filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pad_edges(image, howmanypixels=25):\n",
    "#     '''There *has* to be a better way to set the edges to one, but I havent found one'''\n",
    "#     closed = image.copy()\n",
    "#     closed[:howmanypixels,:,:] = True\n",
    "#     closed[:,:howmanypixels,:] = True\n",
    "#     closed[:,:,:howmanypixels] = True\n",
    "#     closed[-howmanypixels:,:,:] = True\n",
    "#     closed[:,-howmanypixels:,:] = True\n",
    "#     closed[:,:,-howmanypixels:] = True\n",
    "#     return(closed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fill_hollow_bones(image, verbose=False):\n",
    "#     '''\n",
    "#     We flood-fill the image from one edge.\n",
    "#     Then we add the inversion of this to the original image and thus filled all the long bones.\n",
    "#     '''\n",
    "#     dilated = skimage.morphology.binary_dilation(\n",
    "#         skimage.morphology.binary_dilation(\n",
    "#             skimage.morphology.binary_dilation(image)))\n",
    "#     closed = pad_edges(dilated)\n",
    "#     # Flood fill from one edge and invert the (boolean) result\n",
    "#     flooded = ~skimage.morphology.flood_fill(closed.astype('int'),\n",
    "#                                              seed_point=(30,30,30),\n",
    "#                                              new_value=1).astype('bool')\n",
    "#     # Add the inverted result to the original image, filling the long bones\n",
    "#     filled = numpy.add(image, skimage.morphology.binary_dilation(\n",
    "#         skimage.morphology.binary_dilation(\n",
    "#             skimage.morphology.binary_dilation(flooded))))\n",
    "#     if verbose:\n",
    "#         plt.subplot(131)\n",
    "#         plt.imshow(image[len(filled)//2,:,:])\n",
    "#         plt.title('original')\n",
    "#         plt.subplot(132)\n",
    "#         plt.imshow(flooded[len(filled)//2,:,:])       \n",
    "#         plt.title('flooded')\n",
    "#         plt.subplot(133)        \n",
    "#         plt.imshow(image[len(filled)//2,:,:], alpha=0.5)       \n",
    "#         plt.imshow(filled[len(filled)//2,:,:], cmap='viridis', alpha=0.5)       \n",
    "#         plt.title('filled (output)')        \n",
    "#         plt.show()\n",
    "#     return(filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the flood-filled image\n",
    "# Since this takes a while, we don't do it in a Pythonic way\n",
    "# e.g. (Flooded = [ourfiller(t, verbose=True) for t in Tresholded])\n",
    "# but in a loop with saving in between.\n",
    "Data['OutputNameFlooded'] = [f.replace('.zarr', '_flooded.zarr') for f in Data['OutputNameThresholded']]\n",
    "for c, row in Data.iterrows():\n",
    "    if not os.path.exists(row['OutputNameFlooded']):  \n",
    "        print('%2s/%s: %s: Filling holes' % (c + 1,\n",
    "                                             len(Data),\n",
    "                                             row['Sample']))\n",
    "        Flooded = ourfiller(Thresholded[c].compute())\n",
    "        Flooded = da.stack(Flooded[:])\n",
    "        print('%11s: Saving to %s' % (row['Sample'],\n",
    "                                      row['OutputNameFlooded'][len(Root):]))\n",
    "        Flooded.to_zarr(row['OutputNameFlooded'],\n",
    "                        overwrite=True,\n",
    "                        compressor=Blosc(cname='zstd', clevel=3, shuffle=Blosc.BITSHUFFLE))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DASK arrays with the filled samples\n",
    "Flooded = [dask.array.from_zarr(file) for file in Data['OutputNameFlooded']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DASK\n",
    "# Read or calculate the middle slices of the flooded images,\n",
    "# put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Flooded_Mid_' + direction] = [None] * len(VOIs)\n",
    "for c, row in tqdm.notebook.tqdm(Data.iterrows(),\n",
    "                                 desc='Middle flooded images',\n",
    "                                 total=len(Data)):\n",
    "    for d, direction in tqdm.notebook.tqdm(enumerate(directions),\n",
    "                                           desc=row['Sample'],\n",
    "                                           leave=False,\n",
    "                                           total=len(directions)):\n",
    "        outfilepath = os.path.join(row['Folder'],\n",
    "                                   '%s.Thresholded%03d.Flooded.Middle.%s.png' % (row['Sample'],\n",
    "                                                                                 row['Threshold'],\n",
    "                                                                                 direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c,'Flooded_Mid_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate requested axial view\n",
    "            if 'Axial' in direction:\n",
    "                Data.at[c,'Flooded_Mid_' + direction] = Flooded[c][Data['Size'][c][0]//2]\n",
    "            if 'Sagittal' in direction:\n",
    "                Data.at[c,'Flooded_Mid_' + direction] = Flooded[c][:,Data['Size'][c][1]//2,:]\n",
    "            if 'Coronal' in direction:\n",
    "                Data.at[c,'Flooded_Mid_' + direction] = Flooded[c][:,:,Data['Size'][c][2]//2]\n",
    "            # Save the calculated 'direction' view out\n",
    "            # Dask only calculates/reads the images here at this point...\n",
    "            imageio.imwrite(outfilepath,\n",
    "                            (Data.at[c,'Flooded_Mid_' + direction].astype('uint8')*255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show middle flood-filled images\n",
    "for c, row in Data.iterrows():\n",
    "    for d, direction in enumerate(directions):\n",
    "        plt.subplot(1, 3, d + 1)\n",
    "        plt.imshow(row['Flooded_Mid_' + direction])\n",
    "        plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "#         plt.title('%s/%s: %s, %s' % (c + 1,\n",
    "#                                      len(Data),\n",
    "#                                      row['Sample'],\n",
    "#                                      direction + ' MIP'))\n",
    "        plt.title('%s: %s' % (row['Sample'],\n",
    "                              direction + ' MIP'))\n",
    "        \n",
    "        plt.axis('off')\n",
    "    plt.savefig(os.path.join(row['Folder'], row['Sample'] + '.Flooded.MiddleSlices.png'),\n",
    "                bbox_inches='tight')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show middle flood-filled images with overlay\n",
    "for c, row in Data.iterrows():\n",
    "    for d, direction in enumerate(directions):\n",
    "        plt.subplot(1, 3, d + 1)\n",
    "        plt.imshow(row['Flooded_Mid_' + direction])\n",
    "        plt.imshow(dask.array.ma.masked_less(row['Thresholded_Mid_' + direction], 1), alpha=0.5, cmap='viridis')        \n",
    "        plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "#         plt.title('%s/%s: %s, %s' % (c + 1,\n",
    "#                                      len(Data),\n",
    "#                                      row['Sample'],\n",
    "#                                      direction + ' MIP'))\n",
    "        plt.title('%s: %s' % (row['Sample'],\n",
    "                              direction + ' MIP'))\n",
    "        \n",
    "        plt.axis('off')\n",
    "    plt.savefig(os.path.join(row['Folder'], row['Sample'] + '.Flooded.Overlay.MiddleSlices.png'),\n",
    "                bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(Data['Thresholded_Mid_Axial'][0])\n",
    "# plt.imshow(dask.array.ma.masked_less(Data['Flooded_Mid_Axial'][0], 1), alpha=0.5, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show middle axis flooded images\n",
    "for d,direction in enumerate(directions):\n",
    "    for c,row in Data.iterrows():\n",
    "        plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)\n",
    "        plt.imshow(row['Flooded_Mid_' + direction])\n",
    "        plt.imshow(dask.array.ma.masked_less(row['Thresholded_Mid_' + direction], 1), alpha=0.5, cmap='viridis')\n",
    "        plt.title('Middle %s slice of tophat of\\n%s together with original' % (direction, row['Sample']))\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][c], 'um'))    \n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.subplot(121)\n",
    "# plt.imshow(Data['Thresholded_Mid_' + direction][1])\n",
    "# plt.subplot(122)\n",
    "# plt.imshow(Data['Flooded_Mid_' + direction][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flooded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reconstructions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the white tophat\n",
    "# https://scikit-image.org/docs/dev/api/skimage.morphology.html#skimage.morphology.white_tophat\n",
    "# e.g. the bright spots of the image that are smaller than the structuring element.\n",
    "# We use a ball-shaped (sphere) structuring element\n",
    "# Again, since this takes a *long* while, we don't do it nice and pythonic,\n",
    "# but in a loop with intermediate saving\n",
    "# e.g. not (Tophat = [skimage.morphology.white_tophat(f, selem=skimage.morphology.ball(7)) for f in Flooded])\n",
    "# but\n",
    "tophatselem = 5\n",
    "Data['OutputNameTophat'] = [f.replace('.zarr', '_tophat_%s.zarr' % tophatselem) for f in Data['OutputNameFlooded']]\n",
    "Tophat = [numpy.nan for file in Data['OutputNameTophat']]\n",
    "for c, row in Data.iterrows():\n",
    "    if os.path.exists(row['OutputNameTophat']):\n",
    "        print('%2s/%s: Already saved to %s' % (c + 1,\n",
    "                                               len(Data),\n",
    "                                               row['OutputNameTophat'][len(Root):]))\n",
    "    else:\n",
    "        print('%2s/%s: %s: Calculating white thophat with a \"selem\" of %s' % (c + 1,\n",
    "                                                                              len(Data),\n",
    "                                                                              row['Sample'].rjust(Data['SampleNameLength'].max()),\n",
    "                                                                              tophatselem))\n",
    "        Tophat = skimage.morphology.white_tophat(Flooded[c].compute(),\n",
    "                                                 selem=skimage.morphology.ball(tophatselem))\n",
    "        Tophat = da.stack(Tophat[:])        \n",
    "        print('%11s: Saving to %s' % (row['Sample'].rjust(Data['SampleNameLength'].max()),\n",
    "                                      row['OutputNameTophat'][len(Root):]))\n",
    "        Tophat.to_zarr(row['OutputNameTophat'],\n",
    "                       overwrite=True,\n",
    "                       compressor=Blosc(cname='zstd', clevel=3, shuffle=Blosc.BITSHUFFLE))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img = Thresholded[1][1250:-1350,100:-100,100:-100].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D\n",
    "#tophat = skimage.morphology.white_tophat(a, selem=skimage.morphology.disk(33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D\n",
    "#plt.subplot(131)\n",
    "#plt.imshow(a)\n",
    "#plt.subplot(132)\n",
    "#plt.imshow(tophat)\n",
    "#plt.subplot(133)\n",
    "#plt.imshow(numpy.bitwise_xor(a,\n",
    "#                             tophat), alpha=0.5)\n",
    "#plt.imshow(tophat, cmap='viridis', alpha=0.5)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 3D topat\n",
    "#tophat = skimage.morphology.white_tophat(img, selem=skimage.morphology.ball(11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whichslice = numpy.shape(img)[0]//2\n",
    "#plt.subplot(131)\n",
    "#plt.imshow(img[whichslice])\n",
    "#plt.subplot(132)\n",
    "#plt.imshow(tophat[whichslice])\n",
    "#plt.subplot(133)\n",
    "#plt.imshow(numpy.bitwise_xor(img[whichslice],\n",
    "#                             tophat[whichslice]), alpha=0.5)\n",
    "#plt.imshow(tophat[whichslice], cmap='viridis', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DASK arrays with the tophat-filtered samples (e.g. only containing the smaller vessels)\n",
    "Tophat = [dask.array.from_zarr(file) for file in Data['OutputNameTophat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DASK\n",
    "# Read or calculate the middle slices of the Tophat images,\n",
    "# put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Tophat_Mid_' + direction] = [None] * len(VOIs)\n",
    "for c, row in tqdm.notebook.tqdm(Data.iterrows(), desc='Middle tophat images', total=len(Data)):\n",
    "    for d, direction in tqdm.notebook.tqdm(enumerate(directions),\n",
    "                                      desc=row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        outfilepath = os.path.join(row['Folder'],\n",
    "                                   '%s.Thresholded%03d.Tophat.Middle.%s.png' % (row['Sample'],\n",
    "                                                                                row['Threshold'],\n",
    "                                                                                 direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c,'Tophat_Mid_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate requested axial view\n",
    "            if 'Axial' in direction:\n",
    "                Data.at[c,'Tophat_Mid_' + direction] = Tophat[c][Data['Size'][c][0]//2]\n",
    "            if 'Sagittal' in direction:\n",
    "                Data.at[c,'Tophat_Mid_' + direction] = Tophat[c][:,Data['Size'][c][1]//2,:]\n",
    "            if 'Coronal' in direction:\n",
    "                Data.at[c,'Tophat_Mid_' + direction] = Tophat[c][:,:,Data['Size'][c][2]//2]\n",
    "            # Save the calculated 'direction' view out\n",
    "            # Dask only calculates/reads the images here at this point...\n",
    "            imageio.imwrite(outfilepath, (Data.at[c,'Tophat_Mid_' + direction].astype('uint8')*255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show middle flood-filled images with overlay\n",
    "for c, row in Data.iterrows():\n",
    "    for d, direction in enumerate(directions):\n",
    "        plt.subplot(1, 3, d + 1)\n",
    "        plt.imshow(row['Tophat_Mid_' + direction])\n",
    "        plt.imshow(dask.array.ma.masked_less(row['Thresholded_Mid_' + direction], 1), alpha=0.5, cmap='viridis')        \n",
    "        plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "#         plt.title('%s/%s: %s, %s' % (c + 1,\n",
    "#                                      len(Data),\n",
    "#                                      row['Sample'],\n",
    "#                                      direction + ' MIP'))\n",
    "        plt.title('%s: %s' % (row['Sample'],\n",
    "                              direction + ' MIP'))\n",
    "        \n",
    "        plt.axis('off')\n",
    "    plt.savefig(os.path.join(row['Folder'], row['Sample'] + '.Tophat.Overlay.MiddleSlices.png'),\n",
    "                bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show middle slices of tophat data\n",
    "for d,direction in enumerate(directions):\n",
    "    for c,row in Data.iterrows():\n",
    "        plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)\n",
    "        plt.imshow(row['Thresholded_Mid_' + direction])\n",
    "        plt.imshow(dask.array.ma.masked_less(row['Tophat_Mid_' + direction], 1), alpha=0.5, cmap='viridis')\n",
    "        plt.title('Middle %s slice of tophat of\\n%s together with original' % (direction, row['Sample']))\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][c], 'um'))    \n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show overlay\n",
    "# for c, direction in enumerate(directions):\n",
    "#     print(c, direction)    \n",
    "#     if c:\n",
    "#         plt.imshow(numpy.rot90(center, axes=(0,c))[len(flooded)//2,:,:], alpha=0.33)\n",
    "#         plt.imshow(numpy.rot90(flooded, axes=(0,c))[len(flooded)//2,:,:], alpha=0.33, cmap='magma')\n",
    "#         plt.imshow(numpy.rot90(tophat, axes=(0,c))[len(flooded)//2,:,:], alpha=0.33, cmap='viridis')\n",
    "#     else:\n",
    "#         plt.imshow(center[len(flooded)//2,:,:], alpha=0.33)\n",
    "#         plt.imshow(flooded[len(flooded)//2,:,:], alpha=0.33, cmap='magma')\n",
    "#         plt.imshow(tophat[len(flooded)//2,:,:], alpha=0.33, cmap='viridis')\n",
    "#     plt.gca().add_artist(ScaleBar(Data['Voxelsize'][0], 'um'))\n",
    "#     plt.axis('off')\n",
    "#     plt.savefig('Overlay-%s.png' % direction, bbox_inches='tight')  \n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the skeletonization\n",
    "# By multiplying them later on we get a color-coded medial axis transformation\n",
    "# This conforms to what we would expect from 'skimage.morphology.medial_axis(image, return_distance=True)' which does *not* work for 3D images\n",
    "Data['OutputNameSkeleton'] = [f.replace('.zarr', '_skeleton.zarr') for f in Data['OutputNameTophat']]\n",
    "for c, row in Data.iterrows():\n",
    "    if os.path.exists(row['OutputNameSkeleton']):  \n",
    "        print('%2s/%s: Already saved to %s' % (c + 1,\n",
    "                                               len(Data),\n",
    "                                               row['OutputNameSkeleton'][len(Root):]))\n",
    "    else:\n",
    "        print('%2s/%s: %s: Calculating skeletonization' % (c + 1,\n",
    "                                                           len(Data),\n",
    "                                                           row['Sample'].rjust(Data['SampleNameLength'].max())))\n",
    "        Skeleton = skimage.morphology.skeletonize_3d(Tophat[c])\n",
    "        Skeleton = da.stack(Skeleton[:])\n",
    "        print('%11s: Saving to %s' % (row['Sample'].rjust(Data['SampleNameLength'].max()),\n",
    "                                     row['OutputNameSkeleton'][len(Root):]))\n",
    "        Skeleton.to_zarr(row['OutputNameSkeleton'],\n",
    "                         overwrite=True,\n",
    "                         compressor=Blosc(cname='zstd', clevel=3, shuffle=Blosc.BITSHUFFLE))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DASK arrays with the skeletonized images\n",
    "Skeleton = [dask.array.from_zarr(file) for file in Data['OutputNameSkeleton']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read or calculate the middle slices of the Skeletonization images,\n",
    "# put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Skeleton_Mid_' + direction] = [None] * len(VOIs)\n",
    "for c, row in tqdm.notebook.tqdm(Data.iterrows(), desc='Middle skeleton images', total=len(Data)):\n",
    "    for d, direction in tqdm.notebook.tqdm(enumerate(directions),\n",
    "                                      desc=row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        outfilepath = os.path.join(row['Folder'],\n",
    "                                   '%s.Thresholded%03d.Skeleton.Middle.%s.png' % (row['Sample'],\n",
    "                                                                                  row['Threshold'],\n",
    "                                                                                  direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c,'Skeleton_Mid_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate requested axial view\n",
    "            if 'Axial' in direction:\n",
    "                Data.at[c,'Skeleton_Mid_' + direction] = Skeleton[c][Data['Size'][c][0]//2]\n",
    "            if 'Sagittal' in direction:\n",
    "                Data.at[c,'Skeleton_Mid_' + direction] = Skeleton[c][:,Data['Size'][c][1]//2,:]\n",
    "            if 'Coronal' in direction:\n",
    "                Data.at[c,'Skeleton_Mid_' + direction] = Skeleton[c][:,:,Data['Size'][c][2]//2]\n",
    "            # Save the calculated 'direction' view out\n",
    "            # Dask only calculates/reads the images here at this point...\n",
    "            imageio.imwrite(outfilepath, Data.at[c,'Skeleton_Mid_' + direction].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show middle skeletonized images with overlay\n",
    "for c, row in Data.iterrows():\n",
    "    for d, direction in enumerate(directions):\n",
    "        plt.subplot(1, 3, d + 1)\n",
    "        plt.imshow(row['Mid_' + direction])\n",
    "        plt.imshow(dask.array.ma.masked_less(row['Skeleton_Mid_' + direction], 1), alpha=0.5, cmap='viridis')        \n",
    "        plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "#         plt.title('%s/%s: %s, %s' % (c + 1,\n",
    "#                                      len(Data),\n",
    "#                                      row['Sample'],\n",
    "#                                      direction + ' MIP'))\n",
    "        plt.title('%s: %s' % (row['Sample'],\n",
    "                              direction + ' MIP'))\n",
    "        \n",
    "        plt.axis('off')\n",
    "    plt.savefig(os.path.join(row['Folder'], row['Sample'] + '.Skeleton.Overlay.MiddleSlices.png'),\n",
    "                bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d,direction in enumerate(directions):\n",
    "    for c,row in Data.iterrows():\n",
    "        plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)\n",
    "        plt.imshow(row['Tophat_Mid_' + direction])\n",
    "        plt.imshow(row['Skeleton_Mid_' + direction], alpha=0.5, cmap='viridis')\n",
    "        plt.title('Middle %s slice of Skeletonization of\\n%s together with tophat' % (direction, row['Sample']))\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][c], 'um'))    \n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a, b = scipy.ndimage.morphology.distance_transform_edt(Tophat[0][900:1000], sampling=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numpy.shape(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(a[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tophat[0][800:-800,800:-800,800:-800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tophat[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Folder',\n",
    "      'Sample',\n",
    "#       'Scan',\n",
    "      'SampleNameLength',\n",
    "      'ScanNameLength',\n",
    "      'Experiment',\n",
    "      'Timepoint',\n",
    "      'LogFile',\n",
    "#       'VOIFolders',\n",
    "      'VOIFolder',\n",
    "      'Voxelsize',\n",
    "#       'VOISlices',\n",
    "      'Number of VOI slices',\n",
    "      'Size',\n",
    "      'VOIVolume',\n",
    "      'GrayValueMean',\n",
    "      'GrayValueMeanNormalizedToVOIVolume',\n",
    "      'Threshold',\n",
    "      'ThresholdMean',\n",
    "      'ThresholdedVolume',\n",
    "      'GrayValueMeanNormalizedToThresholdedVolume']].to_excel(os.path.join(OutputDir, 'Data_' + get_git_hash() + '.xls'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in Data:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the euclidean distance transformation\n",
    "subsampling = None\n",
    "if subsampling:\n",
    "    Data['OutputNameEDT'] = [f.replace('.zarr', '_edt_sampling%s.zarr' % subsampling) for f in Data['OutputNameTophat']]\n",
    "else:\n",
    "    Data['OutputNameEDT'] = [f.replace('.zarr', '_edt.zarr') for f in Data['OutputNameTophat']]    \n",
    "# Calculate EDT\n",
    "for c, row in Data.iterrows():\n",
    "    if os.path.exists(row['OutputNameEDT']):\n",
    "        print('%2s/%s: Already saved to %s' % (c + 1,\n",
    "                                               len(Data),\n",
    "                                               row['OutputNameEDT'][len(Root):]))\n",
    "    else:\n",
    "        print('%2s/%s: %s: Calculating euclidean distance transformation' % (c + 1,\n",
    "                                                                             len(Data),\n",
    "                                                                             row['Sample'].rjust(Data['SampleNameLength'].max())))\n",
    "        EDT = scipy.ndimage.morphology.distance_transform_edt(Tophat[c].astype('bool'),\n",
    "                                                              sampling=subsampling)\n",
    "        EDT = da.stack(EDT[:])\n",
    "        print('%11s: Saving to %s' % (row['Sample'].rjust(Data['SampleNameLength'].max()),\n",
    "                                      row['OutputNameEDT']))\n",
    "        EDT.to_zarr(row['OutputNameEDT'],\n",
    "                    overwrite=True,\n",
    "                    compressor=Blosc(cname='zstd', clevel=3, shuffle=Blosc.BITSHUFFLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the EDT from the saved zarr files   \n",
    "EDT = [dask.array.from_zarr(file) for file in Data['OutputNameEDT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DASK\n",
    "# Read or calculate the middle slices of the EDT images,\n",
    "# put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['EDT_Mid_' + direction] = [None] * len(VOIs)\n",
    "for c, row in tqdm.notebook.tqdm(Data.iterrows(), desc='Middle EDT images', total=len(Data)):\n",
    "    for d, direction in tqdm.notebook.tqdm(enumerate(directions),\n",
    "                                           desc=row['Sample'],\n",
    "                                           leave=False,\n",
    "                                           total=len(directions)):\n",
    "        outfilepath = os.path.join(row['Folder'],\n",
    "                                   '%s.Thresholded%03d.EDT.Middle.%s.png' % (row['Sample'],\n",
    "                                                                             row['Threshold'],\n",
    "                                                                             direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c,'EDT_Mid_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate requested axial view\n",
    "            if 'Axial' in direction:\n",
    "                Data.at[c,'EDT_Mid_' + direction] = EDT[c][Data['Size'][c][0]//2]\n",
    "            if 'Sagittal' in direction:\n",
    "                Data.at[c,'EDT_Mid_' + direction] = EDT[c][:,Data['Size'][c][1]//2,:]\n",
    "            if 'Coronal' in direction:\n",
    "                Data.at[c,'EDT_Mid_' + direction] = EDT[c][:,:,Data['Size'][c][2]//2]\n",
    "            # Save the calculated 'direction' view out\n",
    "            # Dask only calculates/reads the images here at this point...\n",
    "            imageio.imwrite(outfilepath,Data.at[c,'EDT_Mid_' + direction].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d,direction in enumerate(directions):\n",
    "    for c,row in Data.iterrows():\n",
    "        plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)\n",
    "        plt.imshow(row['Flooded_Mid_' + direction])\n",
    "        plt.imshow(row['EDT_Mid_' + direction], alpha=0.5, cmap='viridis')\n",
    "        plt.title('Middle %s slice of EDT of\\n%s together with original' % (direction, row['Sample']))\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][c], 'um'))    \n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d,direction in enumerate(directions):\n",
    "    for c,row in Data.iterrows():\n",
    "        plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)\n",
    "        plt.imshow(row['EDT_Mid_' + direction], alpha=0.5, cmap='viridis')\n",
    "        plt.title('Middle %s slice of EDT of\\n%s together with original' % (direction, row['Sample']))\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][c], 'um'))    \n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show middle skeletonized images with overlay\n",
    "for c, row in Data.iterrows():\n",
    "    for d, direction in enumerate(directions):\n",
    "        plt.subplot(1, 3, d + 1)\n",
    "        plt.imshow(row['EDT_Mid_' + direction])\n",
    "        plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "#         plt.title('%s/%s: %s, %s' % (c + 1,\n",
    "#                                      len(Data),\n",
    "#                                      row['Sample'],\n",
    "#                                      direction + ' MIP'))\n",
    "        plt.title('%s: %s' % (row['Sample'],\n",
    "                              direction + ' MIP'))\n",
    "        \n",
    "        plt.axis('off')\n",
    "    plt.savefig(os.path.join(row['Folder'], row['Sample'] + '.EDT.MiddleSlices.png'),\n",
    "                bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distance on skeleton\n",
    "Data['OutputNameSkelDist'] = [f.replace('.zarr', '_skeletondistance.zarr') for f in Data['OutputNameTophat']]\n",
    "# Calculate edt\n",
    "for c, row in Data.iterrows():\n",
    "    if os.path.exists(row['OutputNameSkelDist']):\n",
    "        print('%2s/%s: Already saved to %s' % (c + 1,\n",
    "                                               len(Data),\n",
    "                                               row['OutputNameSkelDist'][len(Root):]))\n",
    "    else:\n",
    "        print('%2s/%s: %s: Multiplying skeleton and EDT and saving to %s' % (c + 1,\n",
    "                                                                             len(Data),\n",
    "                                                                             row['Sample'].rjust(Data['SampleNameLength'].max()),\n",
    "                                                                             row['OutputNameSkelDist'][len(Root):]))\n",
    "        SkelDist = numpy.multiply(Skeleton[c], EDT[c])\n",
    "        SkelDist.to_zarr(row['OutputNameSkelDist'],\n",
    "                         overwrite=True,\n",
    "                         compressor=Blosc(cname='zstd', clevel=3, shuffle=Blosc.BITSHUFFLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DASK arrays with the skeleton-distance\n",
    "SkelDist = [dask.array.from_zarr(file) for file in Data['OutputNameSkelDist']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DASK\n",
    "# Read or calculate the middle slices of the SkelDist images,\n",
    "# put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['SkelDist_Mid_' + direction] = [None] * len(VOIs)\n",
    "for c, row in tqdm.notebook.tqdm(Data.iterrows(), desc='Middle SkelDist images', total=len(Data)):\n",
    "    for d, direction in tqdm.notebook.tqdm(enumerate(directions),\n",
    "                                      desc=row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        outfilepath = os.path.join(row['Folder'],\n",
    "                                   '%s.Thresholded%03d.SkelDist.Middle.%s.png' % (row['Sample'],\n",
    "                                                                                  row['Threshold'],\n",
    "                                                                                  direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c,'SkelDist_Mid_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate requested axial view\n",
    "            if 'Axial' in direction:\n",
    "                Data.at[c,'SkelDist_Mid_' + direction] = SkelDist[c][Data['Size'][c][0]//2]\n",
    "            if 'Sagittal' in direction:\n",
    "                Data.at[c,'SkelDist_Mid_' + direction] = SkelDist[c][:,Data['Size'][c][1]//2,:]\n",
    "            if 'Coronal' in direction:\n",
    "                Data.at[c,'SkelDist_Mid_' + direction] = SkelDist[c][:,:,Data['Size'][c][2]//2]\n",
    "            # Save the calculated 'direction' view out\n",
    "            # Dask only calculates/reads the images here at this point...\n",
    "            imageio.imwrite(outfilepath,Data.at[c,'SkelDist_Mid_' + direction].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d,direction in enumerate(directions):\n",
    "    for c,row in Data.iterrows():\n",
    "        plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)\n",
    "#         plt.imshow(row['Thresholded_Mid_' + direction])\n",
    "#         plt.imshow(dask.array.ma.masked_where(0, row['EDT_Mid_' + direction]), alpha=0.5, cmap='viridis')\n",
    "        plt.imshow(row['SkelDist_Mid_' + direction], alpha=0.5, cmap='viridis')\n",
    "        plt.title('Middle %s slice of SkelDist of\\n%s together with original' % (direction, row['Sample']))\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][c], 'um'))    \n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sl = 999\n",
    "# plt.subplot(131)\n",
    "# plt.imshow(numpy.max(Skeleton[0], axis=0), cmap='viridis')\n",
    "# plt.subplot(132)\n",
    "# plt.imshow(numpy.max(EDT[0], axis=0), cmap='viridis')\n",
    "# plt.subplot(133)\n",
    "# plt.imshow(numpy.max(SkelDist[0], axis=0), cmap='viridis')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read or calculate skeletondistance MIPs, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['MIP_SkelDist_' + direction] = [None] * len(VOIs)\n",
    "for c, row in tqdm.notebook.tqdm(Data.iterrows(), desc='MIPs SkelDist', total=len(Data)):\n",
    "    for d, direction in tqdm.notebook.tqdm(enumerate(directions),\n",
    "                                      desc=row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        outfilepath = os.path.join(row['Folder'],\n",
    "                                   '%s.Thresholded%03d.MIP.SkelDist.%s.png' % (row['Sample'],\n",
    "                                                                               row['Threshold'],\n",
    "                                                                               direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c,'MIP_SkelDist_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Keep *this* reconstruction in RAM for a bit\n",
    "            img = SkelDist[c].astype('uint8').persist()\n",
    "            # Generate MIP\n",
    "            Data.at[c,'MIP_SkelDist_' + direction] = img.max(axis=d).compute()\n",
    "            # Save it out\n",
    "            imageio.imwrite(outfilepath,\n",
    "                            Data.at[c,'MIP_SkelDist_' + direction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in Data.iterrows():\n",
    "    for j, direction in enumerate(directions):\n",
    "        plt.subplot(1,3,j+1)\n",
    "#         plt.imshow(row['MIP_' + direction], alpha=0.5)\n",
    "#         plt.imshow(dask.array.ma.masked_less(row['MIP_SkelDist_' + direction],1), cmap='viridis')        \n",
    "        plt.imshow(row['MIP_SkelDist_' + direction], cmap='viridis')            \n",
    "        plt.title('%s view' % direction)\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][c], 'um'))                \n",
    "        plt.axis('off')        \n",
    "    plt.suptitle('%02d/%02d: MIP with Skeleton overlay %s' % (i+1, len(Data), row['Sample']))\n",
    "    plt.savefig(os.path.join(row['Folder'], row['Sample'] + '.SkelDist.MiddleSlices.png'),\n",
    "                bbox_inches='tight')    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SkelDist[0].max().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['SkelDistMean'] = [dask.array.mean(skldst).compute() for skldst in SkelDist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['SkelDistMeanNormalized'] = [dask.array.mean(skldst).compute()/tv for skldst, tv in zip(SkelDist, Data['ThresholdedVolume'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['SkelDistSTD'] = [dask.array.std(skldst).compute() for skldst in SkelDist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean of datasets for comparison\n",
    "seaborn.catplot(data=Data, kind='box', x='Sample', y='SkelDistMean')\n",
    "seaborn.swarmplot(data=Data, x='Sample', y='SkelDistMean', linewidth=1.5, s=10, color='gray');\n",
    "plt.ylabel('Mean Skeleton distance value')\n",
    "plt.ylim(ymin=0)\n",
    "plt.savefig(os.path.join(OutputDir,\n",
    "                         'Skeleton_Average_Distance.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean of datasets for comparison\n",
    "seaborn.catplot(data=Data, kind='box', x='Sample', y='SkelDistMeanNormalized')\n",
    "seaborn.swarmplot(data=Data, x='Sample', y='SkelDistMeanNormalized', linewidth=1.5, s=10, color='gray');\n",
    "plt.ylabel('Mean Skeleton distance value, normalized to thresholded volume')\n",
    "plt.ylim(ymin=0)\n",
    "plt.savefig(os.path.join(OutputDir,\n",
    "                         'Skeleton_Average_Distance_Normalized.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot STD of datasets for comparison\n",
    "seaborn.catplot(data=Data, kind='box', x='Sample', y='SkelDistSTD')\n",
    "seaborn.swarmplot(data=Data, x='Sample', y='SkelDistSTD', linewidth=1.5, s=10, color='gray');\n",
    "plt.ylabel('Skeleton distance STD')\n",
    "plt.ylim(ymin=0)\n",
    "plt.savefig(os.path.join(OutputDir,\n",
    "                         'Skeleton_Average_Distance_STD.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
