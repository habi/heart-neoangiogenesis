{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview all the scans we did\n",
    "Generate MIPs and get a quick look on the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import os\n",
    "import glob\n",
    "import pandas\n",
    "import imageio\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import seaborn\n",
    "import dask\n",
    "import dask_image.imread\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from numcodecs import Blosc\n",
    "import skimage\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BrukerSkyScanLogfileRuminator.parsing_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dask temporary folder\n",
    "# Do this before creating a client: https://stackoverflow.com/a/62804525/323100\n",
    "import tempfile\n",
    "if 'Linux' in platform.system():\n",
    "    tmp = os.path.join(os.sep, 'media', 'habi', 'FastSSD')\n",
    "elif 'Darwin' in platform.system():\n",
    "    tmp = tempfile.gettempdir()\n",
    "else:\n",
    "    if 'anaklin' in platform.node():\n",
    "        tmp = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        tmp = os.path.join('D:\\\\')\n",
    "dask.config.set({'temporary_directory': os.path.join(tmp, 'tmp')})\n",
    "print('Dask temporary files go to %s' % dask.config.get('temporary_directory'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start cluster and client now, after setting tempdir\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('You can seee what DASK is doing at \"http://localhost:%s/status\"' % client.scheduler_info()['services']['dashboard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure defaults\n",
    "plt.rc('image', cmap='gray', interpolation='nearest')  # Display all images in b&w and with 'nearest' interpolation\n",
    "plt.rcParams['figure.figsize'] = (16, 9)  # Size up figures a bit\n",
    "plt.rcParams['figure.dpi'] = 200  # Increase dpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup scale bar defaults\n",
    "plt.rcParams['scalebar.location'] = 'lower right'\n",
    "plt.rcParams['scalebar.frameon'] = False\n",
    "plt.rcParams['scalebar.color'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all plots identically\n",
    "lines = 2\n",
    "# And then do something like\n",
    "# plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different locations if running either on Linux or Windows\n",
    "if 'anaklin' in platform.node():\n",
    "    FastSSD = True\n",
    "else:\n",
    "    FastSSD = False\n",
    "# to speed things up significantly\n",
    "if 'Linux' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join(os.sep, 'media', 'habi', 'FastSSD')\n",
    "    else:\n",
    "        BasePath = os.path.join(os.sep, 'home', 'habi', '1272')\n",
    "elif 'Darwin' in platform.system():\n",
    "    BasePath = os.path.join('/Volumes/2TBSSD/')\n",
    "else:\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        if 'anaklin' in platform.node():\n",
    "            BasePath = os.path.join('S:\\\\')\n",
    "        else:\n",
    "            BasePath = os.path.join('D:\\\\', 'Results')\n",
    "Root = os.path.join(BasePath, 'Hearts Melly')\n",
    "print('We are loading all the data from %s' % Root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make us a dataframe for saving all that we need\n",
    "Data = pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get *all* log files, unsorted but fast\n",
    "Data['LogFile'] = [os.path.join(root, name)\n",
    "                   for root, dirs, files in os.walk(Root)\n",
    "                   for name in files\n",
    "                   if name.endswith((\".log\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all folders\n",
    "Data['Folder'] = [os.path.dirname(f) for f in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for samples which are not yet reconstructed\n",
    "for c, row in Data.iterrows():\n",
    "    # Iterate over every 'proj' folder\n",
    "    if 'proj' in row.Folder:\n",
    "        if not 'TScopy' in row.Folder and not 'PR' in row.Folder:\n",
    "            # If there's nothing with 'rec*' on the same level, then tell us        \n",
    "            if not glob.glob(row.Folder.replace('proj', '*rec*')):\n",
    "                print('- %s is missing matching reconstructions' % row.LogFile[len(Root)+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for any .csv files in each folder\n",
    "Data['XYAlignment'] = [glob.glob(os.path.join(f, '*T*.csv')) for f in Data['Folder']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display samples which are missing the .csv-files for the XY-alignment\n",
    "for c, row in Data.iterrows():\n",
    "    # Iterate over every 'proj' folder\n",
    "    if 'proj' in row['Folder']:\n",
    "        if not row['XYAlignment']:\n",
    "            if not any(x in row.LogFile for x in ['rectmp.log',  # because we only exclude temporary logfiles in a later step\n",
    "                                                  #'proj_nofilter',  # since these two scans of single teeth don't contain a reference scan\n",
    "                                                  #'TScopy',  # discard *t*hermal *s*hift data\n",
    "                                                  ]):\n",
    "                print('- %s has *not* been X/Y aligned' % row.LogFile[len(Root) + 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of all non-rec logfiles\n",
    "for c, row in Data.iterrows():\n",
    "    if 'rec' not in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'ctan.log' in row.LogFile:\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'rectmp.log' in row.LogFile:\n",
    "        Data.drop([c], inplace=True)\n",
    "# Reset dataframe to something that we would get if we only would have loaded the 'rec' files\n",
    "Data = Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all folders we don't need\n",
    "for c, row in Data.iterrows():\n",
    "    if 'Rat' not in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "#     elif 'overview' not in row.Folder:\n",
    "#         Data.drop([c], inplace=True)\n",
    "    elif 'Test' in row.Folder:\n",
    "        Data.drop([c], inplace=True)\n",
    "# Reset dataframe to something that we would get if we only would have loaded the 'rec' files\n",
    "Data = Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate us some meaningful colums\n",
    "Data['Animal'] = [l[len(Root)+1:].split(os.sep)[0] for l in Data['LogFile']]\n",
    "Data['Scan'] = ['.'.join(l[len(Root)+1:].split(os.sep)[1:-1]) for l in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We habe %s scans to work with' % len(Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in animals list from Ludovic\n",
    "AnimalTable = pandas.read_excel('Animals.xlsx',\n",
    "                                engine='openpyxl',\n",
    "                                header=None,\n",
    "                                names=['Animal', 'Gender', '', 'Experiment', 'Timepoint'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in data from animals table\n",
    "for c, rowdata in Data.iterrows():\n",
    "    for d, rowanimals in AnimalTable.iterrows():\n",
    "        if str(rowanimals.Animal) in rowdata.Animal:\n",
    "            Data.at[c, 'Experiment'] = rowanimals.Experiment\n",
    "            Data.at[c, 'Timepoint'] = rowanimals.Timepoint\n",
    "            Data.at[c, 'Gender'] = rowanimals.Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we merged the data we can rename the column to a more reusable name\n",
    "Data.columns = Data.columns.str.replace('Animal', 'Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters to doublecheck from logfiles\n",
    "Data['Voxelsize'] = [pixelsize(log) for log in Data['LogFile']]\n",
    "Data['Filter'] = [whichfilter(log) for log in Data['LogFile']]\n",
    "Data['Exposuretime'] = [exposure(log) for log in Data['LogFile']]\n",
    "Data['Grayvalue'] = [reconstruction_grayvalue(log) for log in Data['LogFile']]\n",
    "Data['RingartefactCorrection'] = [ringremoval(log) for log in Data['LogFile']]\n",
    "Data['BeamHardeningCorrection'] = [beamhardening(log) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Did we reconstruct them equally?\n",
    "Data[['Sample', 'Scan', 'Voxelsize', 'Filter', 'Exposuretime', 'Grayvalue', 'RingartefactCorrection', 'BeamHardeningCorrection']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Data['Voxelsize'].unique())\n",
    "print(Data['Exposuretime'].unique())\n",
    "print(Data['Filter'].unique())\n",
    "print(Data['Grayvalue'].unique())\n",
    "print(Data['RingartefactCorrection'].unique())\n",
    "print(Data['BeamHardeningCorrection'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to XLS sheet for Ludovic\n",
    "Data[['Folder', 'Sample', 'Scan', 'Voxelsize']].to_excel('Voxelsizes.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file names of the reconstructions\n",
    "Data['Reconstructions'] = [sorted(glob.glob(os.path.join(f,\n",
    "                                                         '*.png'))) for f in Data['Folder']]\n",
    "Data['Number of reconstructions'] = [len(r) for r in Data.Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop samples which have not been reconstructed yet\n",
    "# Based on https://stackoverflow.com/a/13851602\n",
    "for c, row in Data[Data['Number of reconstructions'] == 0].iterrows():\n",
    "    print('%s/%s has not been reconstructed yet, we remove it from our data temporarily' % (row.Sample, row.Scan))\n",
    "    Data = Data[Data['Number of reconstructions'] > 0]\n",
    "    Data.reset_index(drop=True, inplace=True)\n",
    "print('We have %s folders with reconstructions' % (len(Data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display some info\n",
    "# for c, row in Data.iterrows():\n",
    "#     print('%02s/%s: %s has %03s reconstructions in %s/%s' % (c + 1,\n",
    "#                                                              len(Data),\n",
    "#                                                              row['Sample'].rjust(Data['SampleNameLength'].max()),\n",
    "#                                                              row['Number of reconstructions'],\n",
    "#                                                              os.path.basename(row['Folder']),\n",
    "#                                                              'rec'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data['PreviewImagePath'] = [sorted(glob.glob(os.path.join(f, '*_spr.bmp')))[0] for f in Data['Folder']]\n",
    "# # Data['PreviewImage'] = [imageio.imread(pip)\n",
    "# #                         if pip\n",
    "# #                         else numpy.random.random((100, 100)) for pip in Data['PreviewImagePath']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all (original, uncropped) reconstructions into ephemereal DASK arrays\n",
    "Reconstructions = [None] * len(Data)\n",
    "for c, row in tqdm(Data.iterrows(), desc='Load reconstructions', total=len(Data)):\n",
    "    Reconstructions[c] = dask_image.imread.imread(os.path.join(row['Folder'], '*rec*.png'))[:,:,:,0].rechunk('auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reconstructions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The three cardinal directions\n",
    "directions = ['Axial',\n",
    "              'Coronal',\n",
    "              'Sagittal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cropper(image,\n",
    "            despeckle=True,\n",
    "            verbose=False,\n",
    "            threshold=50):\n",
    "    import skimage.morphology\n",
    "    import scipy\n",
    "    ''' Function to crop the reconstruction stack to its 'minimal' size, based on the biggest item visible in the MIP '''\n",
    "    dimensions = len(image.shape)\n",
    "    if verbose:\n",
    "        print('Cropping %s-dimensional image' % dimensions)\n",
    "    # Threshold\n",
    "    thresholded = image > threshold\n",
    "    if despeckle:\n",
    "        if verbose:\n",
    "            print('Removing small objects')\n",
    "        despeckled = skimage.util.apply_parallel(skimage.morphology.remove_small_objects,\n",
    "                                                 thresholded,\n",
    "                                                 extra_keywords={'min_size': 10**dimensions},\n",
    "                                                 chunks=[15,15,15])\n",
    "    # Find biggest object\n",
    "    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.find_objects.html\n",
    "    if verbose:\n",
    "        print('Finding objects')\n",
    "    if despeckle:\n",
    "        cropdimensions = scipy.ndimage.find_objects(despeckled)[0]\n",
    "    else:\n",
    "        cropdimensions = scipy.ndimage.find_objects(thresholded)[0]\n",
    "    if verbose:\n",
    "        print('Cutting the image down to')\n",
    "        for c, cd in enumerate(cropdimensions):\n",
    "            print('\\t- %s: %s' % (c, cd))\n",
    "        if dimensions > 2:\n",
    "            # Calculate cardinal direction MIPs\n",
    "            MIPs = [image.max(axis=d) for d, direction in tqdm(enumerate(directions),\n",
    "                                                                                  desc='Calculating MIPs',\n",
    "                                                                                  total=len(directions))]\n",
    "            for d, direction in enumerate(directions):\n",
    "                plt.subplot(1, dimensions, d + 1)\n",
    "                plt.imshow(MIPs[d], alpha=0.618)\n",
    "                plt.imshow(dask.array.ma.masked_less(MIPs[d]>threshold, 1),\n",
    "                           alpha=0.309,\n",
    "                           cmap='viridis_r')\n",
    "                if d == 0:\n",
    "                    plt.axhline(cropdimensions[1].start,\n",
    "                                c=seaborn.color_palette()[0],\n",
    "                                label=cropdimensions[1].start)\n",
    "                    plt.axhline(cropdimensions[1].stop,\n",
    "                                c=seaborn.color_palette()[1],\n",
    "                                label=cropdimensions[1].stop)\n",
    "                    plt.axvline(cropdimensions[2].start,\n",
    "                                c=seaborn.color_palette()[2],\n",
    "                                label=cropdimensions[2].start)\n",
    "                    plt.axvline(cropdimensions[2].stop,\n",
    "                                c=seaborn.color_palette()[3],\n",
    "                                label=cropdimensions[2].stop)\n",
    "                elif d == 1:\n",
    "                    plt.axhline(cropdimensions[0].start,\n",
    "                                c=seaborn.color_palette()[0],\n",
    "                                label=cropdimensions[0].start)\n",
    "                    plt.axhline(cropdimensions[0].stop,\n",
    "                                c=seaborn.color_palette()[1],\n",
    "                                label=cropdimensions[0].stop)\n",
    "                    plt.axvline(cropdimensions[2].start,\n",
    "                                c=seaborn.color_palette()[2],\n",
    "                                label=cropdimensions[2].start)\n",
    "                    plt.axvline(cropdimensions[2].stop,\n",
    "                                c=seaborn.color_palette()[3],\n",
    "                                label=cropdimensions[2].stop)                    \n",
    "                elif d == 2:\n",
    "                    plt.axhline(cropdimensions[0].start,\n",
    "                                c=seaborn.color_palette()[0],\n",
    "                                label=cropdimensions[0].start)\n",
    "                    plt.axhline(cropdimensions[0].stop,\n",
    "                                c=seaborn.color_palette()[1],\n",
    "                                label=cropdimensions[0].stop)\n",
    "                    plt.axvline(cropdimensions[1].start,\n",
    "                                c=seaborn.color_palette()[2],\n",
    "                                label=cropdimensions[1].start)\n",
    "                    plt.axvline(cropdimensions[1].stop,\n",
    "                                c=seaborn.color_palette()[3],\n",
    "                                label=cropdimensions[1].stop)                                        \n",
    "                plt.title('%s MIP' % direction)\n",
    "                plt.legend(loc='lower right')\n",
    "        else:\n",
    "            plt.subplot(121)\n",
    "            plt.imshow(image, alpha=0.618)\n",
    "            plt.imshow(dask.array.ma.masked_less(thresholded, 1), alpha=0.618, cmap='viridis_r')\n",
    "            plt.axhline(cropdimensions[0].start, c=seaborn.color_palette()[0], label=cropdimensions[0].start)\n",
    "            plt.axhline(cropdimensions[0].stop, c=seaborn.color_palette()[1], label=cropdimensions[0].stop)\n",
    "            plt.axvline(cropdimensions[1].start, c=seaborn.color_palette()[2], label=cropdimensions[1].start)\n",
    "            plt.axvline(cropdimensions[1].stop, c=seaborn.color_palette()[3], label=cropdimensions[1].stop)\n",
    "            plt.title('Original image with thresholded overlay')\n",
    "            plt.legend(loc='lower right')\n",
    "            plt.subplot(122)\n",
    "            plt.imshow(image[cropdimensions])\n",
    "            plt.title('Output')\n",
    "        plt.show()\n",
    "    if verbose:\n",
    "        print('Cropped image from %s to %s' % (image.shape, image[cropdimensions].shape))\n",
    "        print(cropdimensions)\n",
    "    return(image[cropdimensions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test cropping function\n",
    "# img = Reconstructions[-1]\n",
    "# Cropped = cropper(img, verbose=True, despeckle=True, threshold=55)\n",
    "# for c, direction in enumerate(directions):\n",
    "#     plt.subplot(1,3,c+1)    \n",
    "#     if 'Axial' in direction:\n",
    "#         plt.imshow(Cropped[Cropped.shape[0] // 2])\n",
    "#     if 'Sagittal' in direction:\n",
    "#         plt.imshow(Cropped[:, Cropped.shape[1] // 2, :])\n",
    "#     if 'Coronal' in direction:\n",
    "#         plt.imshow(Cropped[:, :, Cropped.shape[2] // 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cropper(Reconstructions[3], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop the reconstructions and save them out as .zarr files\n",
    "Data['OutputNameRecCrop'] = [os.path.join(os.path.dirname(f),\n",
    "                                          '%s.%s.crop.zarr' % (sample, scan)) for f, sample, scan in zip(Data.Folder,\n",
    "                                                                                                         Data.Sample,\n",
    "                                                                                                         Data.Scan)]\n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                            desc='Cropping reconstructions and saving to rechunked .zarr files',\n",
    "                            total=len(Data)):\n",
    "    if not os.path.exists(row['OutputNameRecCrop']):\n",
    "        print('%2s/%2s: Saving to %s' % (c + 1,\n",
    "                                         len(Data),\n",
    "                                         row['OutputNameRecCrop'][len(Root):]))\n",
    "        # Calculate crop with our function and directly write to ZARR file\n",
    "        cropper(Reconstructions[c],\n",
    "                verbose=True,\n",
    "                despeckle=True).rechunk(chunks='auto').to_zarr(row['OutputNameRecCrop'],\n",
    "                                                               overwrite=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the *cropped* zarr arrays as reconstructions\n",
    "Reconstructions = [dask.array.from_zarr(file) for file in Data['OutputNameRecCrop']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in Reconstructions:\n",
    "    if len(r.shape) != 3:\n",
    "        print(r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save out cropped rec slices\n",
    "# for c, row in tqdm(Data.iterrows(),\n",
    "#                             desc='Saving out cropped recs',\n",
    "#                             total=len(Data)):\n",
    "#     os.makedirs(os.path.join(row.Folder, 'rec_crop'),\n",
    "#                 exist_ok=True)\n",
    "#     for d, rec in tqdm(enumerate(Reconstructions[c]),\n",
    "#                                 total=len(Reconstructions[c]),\n",
    "#                                 desc=os.path.join(row.Sample, os.path.basename(row.Folder)),\n",
    "#                                 leave=False):\n",
    "#         filename = os.path.join(row.Folder,\n",
    "#                                 'rec_crop', str(row.Sample) + '_rec_crop_%08d.png' % d)\n",
    "#         if not os.path.exists(filename):\n",
    "#             imageio.imsave(filename, rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How big are the datasets?\n",
    "Data['Size'] = [numpy.shape(rec) for rec in Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read or calculate the middle slices, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Mid_' + direction] = [None] * len(Reconstructions)\n",
    "for c, row in tqdm(Data.iterrows(), desc='Middle images', total=len(Data)):\n",
    "    for d, direction in tqdm(enumerate(directions),\n",
    "                                      desc='%s/%s' % (row['Sample'], row['Scan']),\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                                   '%s.%s.Middle.%s.png' % (row['Sample'],\n",
    "                                                            row['Scan'],\n",
    "                                                            direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c, 'Mid_' + direction] = dask_image.imread.imread(outfilepath).squeeze()\n",
    "        else:\n",
    "            # Generate requested axial view\n",
    "            if 'Axial' in direction:\n",
    "                Data.at[c, 'Mid_' + direction] = Reconstructions[c][Data['Size'][c][0] // 2].compute()\n",
    "            if 'Sagittal' in direction:\n",
    "                Data.at[c, 'Mid_' + direction] = Reconstructions[c][:, Data['Size'][c][1] // 2, :].compute()\n",
    "            if 'Coronal' in direction:\n",
    "                Data.at[c, 'Mid_' + direction] = Reconstructions[c][:, :, Data['Size'][c][2] // 2].compute()\n",
    "            # Save the calculated 'direction' view to disk\n",
    "            imageio.imwrite(outfilepath, (Data.at[c, 'Mid_' + direction]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show middle slices\n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                            desc='Saving middle images overview',\n",
    "                            total=len(Data)):\n",
    "    outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                               '%s.%s.MiddleSlices.png' % (row['Sample'], row['Scan']))\n",
    "    if not os.path.exists(outfilepath):    \n",
    "        for d, direction in tqdm(enumerate(directions),\n",
    "                                          desc='%s/%s' % (row['Sample'], row['Scan']),\n",
    "                                          leave=False,\n",
    "                                          total=len(directions)):\n",
    "            plt.subplot(1, 3, d + 1)\n",
    "            plt.imshow(row['Mid_' + direction].squeeze())\n",
    "            if d == 0:\n",
    "                plt.axhline(row.Size[1] // 2, c=seaborn.color_palette()[0])\n",
    "                plt.axvline(row.Size[2] // 2, c=seaborn.color_palette()[1])\n",
    "                plt.gca().add_artist(ScaleBar(row['Voxelsize'],\n",
    "                                              'um',\n",
    "                                              color=seaborn.color_palette()[2]))\n",
    "            elif d == 1:\n",
    "                plt.axhline(row.Size[0] // 2, c=seaborn.color_palette()[2])\n",
    "                plt.axvline(row.Size[d] // 2, c=seaborn.color_palette()[1])\n",
    "                plt.gca().add_artist(ScaleBar(row['Voxelsize'],\n",
    "                                              'um',\n",
    "                                              color=seaborn.color_palette()[0]))\n",
    "            else:\n",
    "                plt.axhline(row.Size[0] // 2, c=seaborn.color_palette()[2])\n",
    "                plt.axvline(row.Size[d] // 2, c=seaborn.color_palette()[0])\n",
    "                plt.gca().add_artist(ScaleBar(row['Voxelsize'],\n",
    "                                              'um',\n",
    "                                              color=seaborn.color_palette()[1]))\n",
    "            plt.title('%s\\n%s' % (os.path.join(row['Sample'], row['Scan']),\n",
    "                                  direction + ' Middle slice'))\n",
    "            plt.axis('off')\n",
    "            plt.savefig(outfilepath,\n",
    "                        transparent=True,\n",
    "                        bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read or calculate the directional MIPs, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['MIP_' + direction] = [None] * len(Reconstructions)\n",
    "for c, row in tqdm(Data.iterrows(), desc='MIPs', total=len(Data)):\n",
    "    for d, direction in tqdm(enumerate(directions),\n",
    "                                      desc='%s/%s' % (row['Sample'], row['Scan']),\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                                   '%s.%s.MIP.%s.png' % (row['Sample'], row['Scan'], direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c, 'MIP_' + direction] = dask_image.imread.imread(outfilepath).squeeze()\n",
    "        else:\n",
    "            # Generate MIP\n",
    "            Data.at[c, 'MIP_' + direction] = Reconstructions[c].max(axis=d).compute()\n",
    "            # Save it out\n",
    "            imageio.imwrite(outfilepath, Data.at[c, 'MIP_' + direction].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show MIP slices\n",
    "for c, row in tqdm(Data.iterrows(), desc='Saving MIP images overview', total=len(Data)):\n",
    "    outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                               '%s.%s.MIPs.png' % (row['Sample'], row['Scan']))\n",
    "    if not os.path.exists(outfilepath):    \n",
    "        for d, direction in tqdm(enumerate(directions),\n",
    "                                          desc='%s/%s' % (row['Sample'], row['Scan']),\n",
    "                                          leave=False,\n",
    "                                          total=len(directions)):\n",
    "            plt.subplot(1, 3, d + 1)\n",
    "            plt.imshow(row['MIP_' + direction].squeeze())\n",
    "            plt.gca().add_artist(ScaleBar(row['Voxelsize'],\n",
    "                                          'um'))\n",
    "            plt.title('%s\\n%s' % (os.path.join(row['Sample'], row['Scan']),\n",
    "                                 direction + ' MIP'))\n",
    "            plt.axis('off')\n",
    "        plt.savefig(outfilepath,\n",
    "                    transparent=True,\n",
    "                    bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show MIPs together\n",
    "lines=5\n",
    "for d, direction in enumerate(directions):\n",
    "    for c, row in Data.iterrows():\n",
    "        plt.subplot(lines, int(numpy.ceil(len(Data) / float(lines))), c + 1)\n",
    "        plt.imshow(row['MIP_' + direction].squeeze())\n",
    "        plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "        plt.title('%s\\n%s' % (row['Sample'], row['Scan']))\n",
    "        plt.axis('off')\n",
    "        plt.suptitle(direction + ' MIPs')\n",
    "    plt.savefig(os.path.join(Root, 'MIPs.' + direction + '.png'),\n",
    "                transparent=True,\n",
    "                bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Data:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the histograms of one of the MIPs\n",
    "# Caveat: dask.da.histogram returns histogram AND bins, making each histogram a 'nested' list of [h, b]\n",
    "Data['Histogram'] = [dask.array.histogram(dask.array.array(mip),\n",
    "                                          bins=2**8,\n",
    "                                          range=[0, 2**8]) for mip in Data['MIP_Sagittal']]\n",
    "# Actually compute the data and put only h into the dataframe, so we can use it below.\n",
    "# Discard the bins\n",
    "Data['Histogram'] = [h for h, b in Data['Histogram']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overeexposecheck(item,\n",
    "                     threshold=250,\n",
    "                     howmanypercent=1,\n",
    "                     whichone='Lateral',\n",
    "                     verbose=False):\n",
    "    '''Function to check if a certain amount of voxels are brighter than a certain value'''\n",
    "    if (Data['MIP_%s' % whichone][item] > threshold).sum() > (Data['MIP_%s' % whichone][item].size * howmanypercent / 100):\n",
    "        if verbose:\n",
    "            plt.subplot(121)\n",
    "            plt.imshow(Data['MIP_%s' % whichone][item])\n",
    "            plt.imshow(numpy.ma.masked_equal(Data['MIP_%s' % whichone][item] > threshold, False),\n",
    "                       cmap='viridis_r',\n",
    "                       alpha=.618)\n",
    "            plt.title('%s/%s\\n%s px of %s Mpixels (>%s%%) are brighter '\n",
    "                      'than %s' % (Data['Sample'][item],\n",
    "                                   Data['Scan'][item],\n",
    "                                   (Data['MIP_%s' % whichone][item] > threshold).compute().sum(),\n",
    "                                   round(1e-6 * Data['MIP_%s' % whichone][item].size, 2),\n",
    "                                   howmanypercent,\n",
    "                                   threshold))\n",
    "            plt.axis('off')\n",
    "            plt.gca().add_artist(ScaleBar(Data['Voxelsize'][item],\n",
    "                                          'um'))\n",
    "            plt.subplot(122)\n",
    "            plt.semilogy(Data['Histogram'][item])\n",
    "            plt.xlim([0, 2**8])\n",
    "            plt.show()\n",
    "        return(True)\n",
    "    else:\n",
    "        return(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'too much' of the MIP is overexposed\n",
    "Data['OverExposed'] = [overeexposecheck(c,\n",
    "                                        whichone='Sagittal',\n",
    "                                        verbose=True) for c, row in Data.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save mean of reconstruction gray values, which we can use for getting an overview of the image data\n",
    "#Data['GrayValueMean'] = [rec.mean().compute() for rec in Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save STD of reconstruction gray values, which we can use for getting an overview of the image data\n",
    "#Data['GrayValueSTD'] = [rec.std().compute() for rec in Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out the dataframe to disk, so we can use it later in another notebook\n",
    "#Data.to_pickle(os.path.join(Root, 'MellyDataframe.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view(Reconstructions[2],\n",
    "#     annotations=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done with previewing %s scans' % len(Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
